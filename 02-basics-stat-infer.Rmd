```{r setup, include=FALSE}
#  bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs/")

# packages
library(bookdown)
library(RColorBrewer)
library(readr)
library(plotly)
library(tidyverse)
library(reshape2)
library(latex2exp)
library(svglite)
library(knitr)
library(kableExtra)
library(formattable)
library(DT)
library(dplyr)
library(datasets)
# data sets
data(iris)
data(trees)

knitr::opts_chunk$set(echo = TRUE, comment = "") #, dev = "svglite", fig.width=8
#plotly::config(plot_ly(), displaylogo = FALSE, mathjax = "cdn")
```

\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\se}{\mathsf{se}}

# Basics of statistical inferences {#statistical-inference}

We discuss the basics of point estimation, confidence intervals, and hypothesis testing for making inferences about a population based on a single sample in Sections \@ref(point-estimation), \@ref(confidence-intervals), and \@ref(hypothesis-testing), respectively.  

## Point estimation {#point-estimation}

A **statistic** is a quantity that can be calculated from sample data. Prior to obtaining data, a statistic is an unknown quantity and is therefore a rv. We refer to the probability distribution for a statistic as a **sampling distribution** to emphasize how the distribution will vary across all possible sample data. 

Statistical inference seeks to draw conclusions about the characteristics of a population from data. For example, suppose we are botanists interested in taxonomic classification of iris flowers. Let $\mu$ denote the true average petal length (in cm) of the [*Iris setosa*](https://www.wikiwand.com/en/Iris_setosa) (AKA the bristle-pointed iris). The parameter $\mu$ is a characteristic of the whole population of the *setosa* species. Before we collect data, the petal lengths of $n$ independent *setosa* flowers are denoted by rvs $X_1, X_2, \dots, X_n$. Any function of the $X_i$'s, such as the sample mean,
\begin{equation}
  \overline{X} = \frac{1}{n} \sum_{i=1}^n X_i\,, (\#eq:sample-mean)
\end{equation}
or the sample variance,
\begin{equation*}
  S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 \,, (\#eq:sample-var)
\end{equation*}
is also a rv. 

Suppose we actually find and measure the petal length of $50$ independent *setosa* flowers resulting in observations $x_1, x_2, \dots, x_{50}$; the distribution (counts) of $50$ such petal length measurements are displayed in Figure \@ref(fig:setosa-petal-lengths). The sample mean $\overline{x}$ for petal length can then be used to draw a conclusion about the (true) value of the population mean $\mu$. Based on the data in Figure \@ref(fig:setosa-petal-lengths) and using \@ref(eq:sample-mean), the value of the sample mean is $\overline{x} = 1.462$. The value $\overline{x}$ provides a "best guess" or point estimate for the true value of $\mu$ based on the $n=50$ samples. 

```{r setosa-petal-lengths, echo = FALSE, fig.cap = "The distribution (counts) of $50$ *setosa* petal length measurments."}
df <-  select(iris, Petal.Length, Species) %>% filter(Species == "setosa")
ggplot(data = df, aes(x = Petal.Length, fill = Species)) + geom_histogram(binwidth = 0.1) 
```

> The botonist Edgar Anderson's **Iris Data** contains 50 obs. of four features (sepal length [cm], sepal width [cm], petal length [cm], and petal width [cm]) for each of three plant species (*setosa*, *virginica*, *versicolor*) for 150 obs. total. This data set can be accessed in `r` by loading `library(datasets)` and then calling `data(iris)`. 

```{definition, point-estimate}
A **point estimate** of a parameter $\theta$ (recall: a fixed, unknown quantity) is a single number that we regard as a sensible value for $\theta$. Let $X_1, X_2, \dots, X_n$ be iid samples from a (general) distribution $F(\theta)$. A **point estimator** $\widehat{\theta}_n$ of a parameter $\theta$ is obtained by selecting a suitable statistic $g$,
\begin{equation*}
  \widehat{\theta}_n = g(X_1, \dots, X_n) \,.
\end{equation*}
A point estimate $\widehat{\theta}_n$ can then be computed from the estimator using sample data.
```

> ⚠️  The symbol $\widehat{\theta}_n$ (or simply $\widehat{\theta}$ when the sample size $n$ is clear from context) is typically used to denote both the estimator and the point estimate resulting from a given sample. Note that writing, e.g., $\widehat{\theta} = 42$ does not indicate how the point estimate was obtained. Therefore, it is essential to report both the estimator and the resulting point estimate. 

Note that Definition \@ref(def:point-estimate) does not say how to select the appropriate statistic. For the *setosa* example, the sample mean $\overline{X}$ is suggested as a good estimator of the population mean $\mu$. That is, $\widehat{\mu} = \overline{X}$ or "the point estimator of $\mu$ is the sample mean $\overline{X}$". Here, while $\mu$ and $\sigma^2$ are fixed quantities representing characteristics of the population, $\overline{X}$ and $S^2$ are rvs with sampling distributions. If the population is *normally distributed* or if the *sample is large* then the sampling distribution for $\overline{X}$ has a known form: $\overline{X}$ is normal with mean $\mu_{\overline{X}} = \mu$ and variance $\sigma_{\overline{X}}^2 = \sigma^{2} / n$, i.e.,
\begin{equation*}
  \overline{X} \sim \mathsf{N}(\mu, \sigma^{2} / n) \,,
\end{equation*}
where $n$ is the sample size and $\mu$ and $\sigma$ are the (typically unknown) population parameters.


```{example, eg-estimators}
Let us consider the heights (measured in inches) of $31$ black cherry trees (sorted, for your enjoyment) in Table \@ref(tab:cherry-data).
```    

```{r cherry-data, echo = FALSE, warning = FALSE, message = FALSE}
dattab <- trees %>%
  arrange(Height) %>%
  summarise(heights = list(Height))
kable(dattab, col.names = c("Height [in]"), caption = "Observations of 31 felled black cherry trees.")
```    

> The **Cherry Tree Data** contains 31 obs. of three features (diameter, height, and volume) and can be accessed in `r` by loading `library(datasets)` and then calling `data(trees)`.  

The quantile-quantile plot^[
How do we tell whether a population is normal? Constructing a normal quantile-quantile plot is one way of assessing whether a normality assumption is reasonable; such a plot compares the the quantiles of the sample data $x_i$ against the (theoretical) standard normal quantiles. If the sample data is consistent with a sample from a normal distribution, then the points will lie on a straight line (more or less). Below we display the QQ plot comparing quantiles of cherry tree heights from \@ref(tab:cherry-data) to normal quantiles.
```{r qq-plot-cherry, echo = FALSE, warning = FALSE, message = FALSE}
ggplot(trees, aes(sample = Height)) + stat_qq() + stat_qq_line()
```   
]
comparing the quantiles of this data to the quantiles of a normal distribution, is fairly straight. Therefore, we assume that the distribution of black cherry tree heights is (at least approximately) normal with a mean value $\mu$; i.e., that the population of heights is distributed $\mathsf{N}(\mu, \sigma^2)$ where $\mu$ is a parameter to be estimated and $\sigma^2$ is unknown. The observations $X_1, \dots, X_{31}$ are then assumed to be a random sample from this normal distribution (iid). Consider the following three different stimators and the resulting point estimates for $\mu$ based on the $31$ samples.

a. Estimator (sample mean) $\overline{X}$ as in \@ref(eq:sample-mean) and estimate $\overline{x} = \sum x_i / n = 2356 / 31 = 76$.

b. Estimator (average of extreme heights) $\widetilde{X} = [\min(X_i) + \max(X_i)]/2$ and estimate $\widetilde{x} = (63 + 87)/2 = 75$. 

c. Estimator ($10\%$ trimmed mean -- i.e., in this instance exclude the smallest and largest three values) $\overline{X}_{\text{tr}(10)}$ and estimate $\overline{x}_{\text{tr}(10)} = (2356 - 63 - 64 - 65 - 87 - 86 - 85) / 25 = 76.24$. 

Each estimator above uses a different notion of center for the sample data. An interesting question to think about is: which estimator will tend to produce estimates closest to the true parameter value? Will the estimators work universally well for all distributions? $\lozenge$
 

In addition to reporting a point estimate (together with its estimator), some indication of its precision should be given. One measure of the precision of an estimate is its standard error.  

```{definition, standard-error}
The **standard error** of an estimator $\widehat{\theta}$ is the standard deviation $\sigma_{\widehat{\theta}} = \sqrt{\Var(\widehat{\theta})}$ (sometimes denoted $\se = \se(\widehat{\theta})$). Often, the standard error depends on unknown parameters and must also be estimated. The **estimated standard error** is denoted by $\widehat{\sigma}_{\widehat{\theta}}$ or $s_{\widehat{\theta}}$ or $\widehat{\se}$. 
```  


## Confidence intervals {#confidence-intervals}

An alternative to reporting a point estimate for a parameter is to report an interval estimate suggesting an entire range of plausible values for the parameter of interest. A confidence interval is an interval estimate that makes a probability statement about the degree of reliability, or the confidence level, of the interval. The first step in computing a confidence interval is to select the confidence level. A popular choice is a $95\%$ confidence interval which corresponds to level $\alpha = 0.05$.  

```{definition, confidence-interval-gen}
A $100(1-\alpha)\%$ **confidence interval** for a parameter $\theta$ is a *random* interval $C_n = (L_n , U_n)$ where $L_n = \ell(X_1, \dots, X_n)$ and $U_n = u(X_1, \dots, X_n)$ are functions of the data such that 
\begin{equation}
P_{\theta}(L_n < \theta < U_n ) = 1 - \alpha\,, 
\end{equation}
for all $\theta \in \Theta$. 
```  

My favorite interpretation of a confidence interval is due to [@Wasserman:2013as, p 92]:  

> On day 1, you collect data and construct a 95 percent confidence interval for a parameter $\theta_1$. On day 2, you collect new data and construct a 95 percent confidence interval for an unrelated parameter $\theta_2$. On day 3, you collect new data and construct a 95 percent confidence interval for an unrelated parameter $\theta_3$. You continue this way constructing confidence intervals for a sequence of unrelated parameters $\theta_1$, $\theta_2$, $\dots$ Then 95 percent of your intervals will trap the true parameter value. There is no need to introduce the idea of repeating the same experiment over and over.  

This interpretation makes clear that a confidence interval is not a probability statement about the parameter $\theta$. In Definition \@ref(def:confidence-interval-gen), note that $\theta$ is fixed ($\theta$ is not a rv) and the interval $C_n$ is random. After data has been collected and a point estimator has been calculated, the resulting CIs either contain the true parameter value or they do not (see). 

```{r exp-many-cis, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Fifty $95\\%$ CIs for a population mean $\\mu$. After a sample is taken, the computed interval estimate either contains $\\mu$ or it does not (asteriks identify intervals that do not include $\\mu$). When drawing such a large number of $95\\%$ CIs, we would anticipate that approximately $5\\%$ (ca. 2.5) would fail to cover the true parameter $\\mu$."}
df <- read_csv("data/exp-many-cis.csv")
ggplot(df, aes(x = X1, y = xbar)) + geom_hline(yintercept = 0, size = 1, alpha = 1) + geom_pointrange(aes(ymin = l, ymax = u, color = star)) + theme(axis.title.y = element_blank(), axis.title.x = element_blank()) + coord_flip()
```

**TODO**: fix above plot EG devore fig 7.3 that illustrates ca. 50 $95\%$ CIs (with asterisks identifying intervals that do not include $\mu$).


## Hypothesis testing {#hypothesis-testing}

In Sections \@ref(point-estimation) and \@ref(confidence-intervals) we reviewed how to estimate a parameter by a single number (point estimate) or range of plausible values (confidence-interval), respectively. Next we discuss methods for determining which of two contradictory claims, or **hypotheses**, about a parameter is correct. 

```{definition, null-alt-hypothesis}
The **null hypothesis**, denoted by $H_0$, is a claim that we intially assume to be true by dafault. The **alternative hypothesis**, denoted by $H_a$, is an assertion that is contradictory to $H_0$. 
```  

For a statistical hypothesis regarding the *equality* of a parameter $\theta$ with a fixed quantity $\theta_0$, the null and alternative hypotheses will take one of the following forms.

| Null hypothesis  | Alternative hypothesis |
|:-----------------|:----------------------|
| $H_0 : \theta \leq \theta_0$ | $H_a : \theta > \theta_0$ |
| $H_0 : \theta \geq \theta_0$ | $H_a : \theta < \theta_0$ |
| $H_0 : \theta = \theta_0$ | $H_a : \theta \neq \theta_0$ |

The value $\theta_0$, called the **null value**, separates the alternative from the null. 

```{definition, hypothesis-test}
A **hypothesis test** asks if the available data provides sufficient evidence to reject $H_0$. If the observations disagree with $H_0$, then we reject the null hypothesis. If the sample evidence does not strongly contradict $H_0$, then we continue to believe $H_0$. The two possible conclustions of a hypothesis test are: *reject $H_0$* or *fail to reject $H_0$*.^[We comment that *fail to reject $H_0$* is sometimes phrased *retain $H_0$* or (perhaps less accurately) *accept $H_0$*.]  
```  

A procedure for carrying out a hypothesis test is based on specifying two additional items: a test statistic and a corresponding rejection region. A **test statistic** is a function of the sample data (like an estimator). The statistical decision to reject or fail to reject the null hypothesis will involve computing the test statistic. The **rejection region** are the values of the test statistic for which the null hypothesis is to be rejected in favor of the alternative. That is, we compute the test statistic based on a given sample; the test statistic either falls in the rejection region---in which case we reject the null $H_0$---or it does not fall in the rejection region---in which case we fail to reject the null $H_0$. 

```{example, eg-hyp-test-def}
**TODO**: example hypothesis test $\lozenge$
```

When carrying out a hypothesis test, two types of errors can be made. The basis for choosing a rejection region typically involves considering these errors. 

```{definition, error-types}
A **type I** error occurs if $H_0$ is rejected when $H_0$ is actually true. A **type II** error is made if we fail to reject $H_0$ when $H_0$ is actually false. 
```

```{example, eg-hyp-test-errors}
**TODO**: example hypothesis error types $\lozenge$
```  

To summarize, the elements of a statistical test are:  

1. Null hypothesis, $H_0$
2. Alternative hypothesis, $H_a$
3. Test statistic
4. Rejection region  

***  