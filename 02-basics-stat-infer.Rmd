```{r setup, include=FALSE}
#  bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs/")

# packages
library(bookdown)
library(RColorBrewer)
library(readr)
library(plotly)
library(tidyverse)
library(reshape2)
library(latex2exp)
library(svglite)
library(knitr)
library(kableExtra)
library(formattable)
library(DT)
library(dplyr)
library(datasets)
# data sets
data(iris)
data(trees)

knitr::opts_chunk$set(echo = TRUE, comment = "") #, dev = "svglite", fig.width=8
#plotly::config(plot_ly(), displaylogo = FALSE, mathjax = "cdn")
```

\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\se}{\mathsf{se}}

# Inferences based on a single sample {#statistical-inference}

We discuss the basics of point estimation, confidence intervals, and hypothesis testing for making inferences about a population based on a single sample in Sections \@ref(point-estimation), \@ref(confidence-intervals), and \@ref(hypothesis-testing), respectively.  In particular, we provide details about estimating population means ($\mu$) in Section \@ref(estimating-means), population proportions ($p$) in Section \@ref(estimating-proportions), and population variances ($\sigma^2$) in Section \@ref(estimating-variances).  

## Point estimation {#point-estimation}

A **statistic** is a quantity that can be calculated from sample data. Prior to obtaining data, a statistic is an unknown quantity and is therefore a rv. We refer to the probability distribution for a statistic as a **sampling distribution** to emphasize how the distribution will vary across all possible sample data. 

Statistical inference seeks to draw conclusions about the characteristics of a population from data. For example, suppose we are botanists interested in taxonomic classification of iris flowers. Let $\mu$ denote the true average petal length (in cm) of the [*Iris setosa*](https://www.wikiwand.com/en/Iris_setosa) (AKA the bristle-pointed iris). The parameter $\mu$ is a characteristic of the whole population of the *setosa* species. Before we collect data, the petal lengths of $n$ independent *setosa* flowers are denoted by rvs $X_1, X_2, \dots, X_n$. Any function of the $X_i$'s, such as the sample mean,
\begin{equation}
  \overline{X} = \frac{1}{n} \sum_{i=1}^n X_i\,, (\#eq:sample-mean)
\end{equation}
or the sample variance,
\begin{equation*}
  S^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 \,, (\#eq:sample-var)
\end{equation*}
is also a rv. 

Suppose we actually find and measure the petal length of $50$ independent *setosa* flowers resulting in observations $x_1, x_2, \dots, x_{50}$; the distribution (counts) of $50$ such petal length measurements are displayed in Figure \@ref(fig:setosa-petal-lengths). The sample mean $\overline{x}$ for petal length can then be used to draw a conclusion about the (true) value of the population mean $\mu$. Based on the data in Figure \@ref(fig:setosa-petal-lengths) and using \@ref(eq:sample-mean), the value of the sample mean is $\overline{x} = 1.462$. The value $\overline{x}$ provides a "best guess" or point estimate for the true value of $\mu$ based on the $n=50$ samples. 

```{r setosa-petal-lengths, echo = FALSE, fig.cap = "The distribution (counts) of $50$ *setosa* petal length measurments."}
df <-  select(iris, Petal.Length, Species) %>% filter(Species == "setosa")
ggplot(data = df, aes(x = Petal.Length, fill = Species)) + geom_histogram(binwidth = 0.1) 
```

> The botonist Edgar Anderson's **Iris Data** contains 50 obs. of four features (sepal length [cm], sepal width [cm], petal length [cm], and petal width [cm]) for each of three plant species (*setosa*, *virginica*, *versicolor*) for 150 obs. total. This data set can be accessed in `r` by loading `library(datasets)` and then calling `data(iris)`. 

```{definition, point-estimate}
A **point estimate** of a parameter $\theta$ (recall: a fixed, unknown quantity) is a single number that we regard as a sensible value for $\theta$. Let $X_1, X_2, \dots, X_n$ be iid samples from a distribution $F(\theta)$. A **point estimator** $\widehat{\theta}_n$ of a parameter $\theta$ is obtained by selecting a suitable statistic $g$,
\begin{equation*}
  \widehat{\theta}_n = g(X_1, \dots, X_n) \,.
\end{equation*}
A point estimate $\widehat{\theta}_n$ can then be computed from the estimator using sample data.
```

> ⚠️  The symbol $\widehat{\theta}_n$ (or simply $\widehat{\theta}$ when the sample size $n$ is clear from context) is typically used to denote both the estimator and the point estimate resulting from a given sample. Note that writing, e.g., $\widehat{\theta} = 42$ does not indicate how the point estimate was obtained. Therefore, it is essential to report both the estimator and the resulting point estimate. 

Note that Definition \@ref(def:point-estimate) does not say how to select the appropriate statistic. For the *setosa* example, the sample mean $\overline{X}$ is suggested as a good estimator of the population mean $\mu$. That is, $\widehat{\mu} = \overline{X}$ or "the point estimator of $\mu$ is the sample mean $\overline{X}$". Here, while $\mu$ and $\sigma^2$ are fixed quantities representing characteristics of the population, $\overline{X}$ and $S^2$ are rvs with sampling distributions. If the population is *normally distributed* or if the *sample is large* then the sampling distribution for $\overline{X}$ has a known form: $\overline{X}$ is normal with mean $\mu_{\overline{X}} = \mu$ and variance $\sigma_{\overline{X}}^2 = \sigma^{2} / n$, i.e.,
\begin{equation*}
  \overline{X} \sim \mathcal{N}(\mu, \sigma^{2} / n) \,,
\end{equation*}
where $n$ is the sample size and $\mu$ and $\sigma$ are the (typically unknown) population parameters.

```{example, eg-estimators}
Let us consider the heights (measured in inches) of $31$ black cherry trees (sorted, for your enjoyment): `63 64 65 66 69 70 71 72 72 74 74 75 75 75 76 76 77 78 79 80 80 80 80 80 81 81 82 83 85 86 87`.

The quantile-quantile plot comparing this data to a normal distribution is fairly straight, so we assume that the distribution of black cherry tree heights is normal with a mean value $\mu$; i.e., that the population of heights is distributed $\mathcal{N}(\mu, \sigma^2)$ where $\mu$ is a parameter to be estimated. The observations $X_1, \dots, X_{31}$ are then assumed to be a random sample from this normal distribution (iid). Consider the following three different stimators and the resulting point estimates for $\mu$ based on the $31$ samples.

a. Estimator (sample mean) $\overline{X}$ as in \@ref(eq:sample-mean) and estimate $\overline{x} = \sum x_i / n = 2356 / 31 = 76$.

b. Estimator (average of extreme heights) $\widetilde{X} = [\min(X_i) + \max(X_i)]/2$ and estimate $\widetilde{x} = (63 + 87)/2 = 75$. 

c. Estimator ($10\%$ trimmed mean -- i.e., in this instance exclude the smallest and largest three values) $\overline{X}_{\text{tr}(10)}$ and estimate $\overline{x}_{\text{tr}(10)} = (2356 - 63 - 64 - 65 - 87 - 86 - 85) / 25 = 76.24$. 

Each estimator above uses a different notion of center for the sample data. An interesting question to think about is: which estimator will tend to produce estimates closest to the true parameter value? Will the estimators work universally well for all distributions?
```


> The **Cherry Tree Data** contains 31 obs. of three features (diameter [in], height [in], and volume [cu ft]) and can be accessed in `r` by loading `library(datasets)` and then calling `data(trees)`. 

In addition to reporting a point estimate (together with its estimator), some indication of its precision should be given. One measure of the precision of an estimate is its standard error. 

```{definition, standard-error}
The **standard error** of an estimator $\widehat{\theta}$ is the standard deviation $\sigma_{\widehat{\theta}} = \sqrt{\Var(\widehat{\theta})}$ (sometimes denoted $\se = \se(\widehat{\theta})$). Often, the standard error depends on unknown parameters and must also be estimated. The **estimated standard error** is denoted by $\widehat{\sigma}_{\widehat{\theta}}$ or $s_{\widehat{\theta}}$ or $\widehat{\se}$. 
```


## Confidence intervals {#confidence-intervals}

An alternative to reporting a point estimate for a parameter is to report an interval estimate suggesting an entire range of plausible values for the parameter of interest. A confidence interval is an interval estimate that makes a probability statement about the degree of reliability, or the confidence level, of the interval. The first step in computing a confidence interval is to select the confidence level. A popular choice is a $95\%$ confidence interval which corresponds to level $\alpha = 0.05$. 

```{definition, confidence-interval-gen}
A $100(1-\alpha)\%$ **confidence interval** for a parameter $\theta$ is a *random* interval $C_n = (L_n , U_n)$ where $L_n = \ell(X_1, \dots, X_n)$ and $U_n = u(X_1, \dots, X_n)$ are functions of the data such that 
\begin{equation}
P_{\theta}(L_n < \theta < U_n ) = 1 - \alpha\,, 
\end{equation}
for all $\theta \in \Theta$. 
```  

My favorite interpretation of a confidence interval is due to [@Wasserman:2013as, p 92]:  

> On day 1, you collect data and construct a 95 percent confidence interval for a parameter $\theta_1$. On day 2, you collect new data and construct a 95 percent confidence interval for an unrelated parameter $\theta_2$. On day 3, you collect new data and construct a 95 percent confidence interval for an unrelated parameter $\theta_3$. You continue this way constructing confidence intervals for a sequence of unrelated parameters $\theta_1$, $\theta_2$, $\dots$ Then 95 percent of your intervals will trap the true parameter value. There is no need to introduce the idea of repeating the same experiment over and over.  

This interpretation makes clear that a confidence interval is not a probability statement about the parameter $\theta$. In Definition \@ref(def:confidence-interval-gen), note that $\theta$ is fixed ($\theta$ is not a rv) and the interval $C_n$ is random. After data has been collected and a point estimator has been calculated, the resulting CIs either contain the true parameter value or they do not (see). 

```{r exp-many-cis, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Fifty $95\\%$ CIs for a population mean $\\mu$. After a sample is taken, the computed interval estimate either contains $\\mu$ or it does not (asteriks identify intervals that do not include $\\mu$). When drawing such a large number of $95\\%$ CIs, we would anticipate that approximately $5\\%$ (ca. 2.5) would fail to cover the true parameter $\\mu$."}
df <- read_csv("data/exp-many-cis.csv")
ggplot(df, aes(x = X1, y = xbar)) + geom_hline(yintercept = 0, size = 1, alpha = 1) + geom_pointrange(aes(ymin = l, ymax = u, color = star)) + theme(axis.title.y = element_blank(), axis.title.x = element_blank()) + coord_flip()
```

**TODO**: fix above plot EG devore fig 7.3 that illustrates ca. 50 $95\%$ CIs (with asterisks identifying intervals that do not include $\mu$).


## Hypothesis testing {#hypothesis-testing}

In Sections \@ref(point-estimation) and \@ref(confidence-intervals) we reviewed how to estimate a parameter by a single number (point estimate) or range of plausible values (confidence-interval), respectively. Next we discuss methods for determining which of two contradictory claims, or **hypotheses**, about a parameter is correct. 

```{definition, null-alt-hypothesis}
The **null hypothesis**, denoted by $H_0$, is a claim that we intially assume to be true by dafault. The **alternative hypothesis**, denoted by $H_a$, is an assertion that is contradictory to $H_0$. 
```  

For a statistical hypothesis regarding the *equality* of a parameter $\theta$ with a fixed quantity $\theta_0$, the null and alternative hypotheses will take one of the following forms.

| Null hypothesis  | Alternative hypothesis |
|:-----------------|:----------------------|
| $H_0 : \theta \leq \theta_0$ | $H_a : \theta > \theta_0$ |
| $H_0 : \theta \geq \theta_0$ | $H_a : \theta < \theta_0$ |
| $H_0 : \theta = \theta_0$ | $H_a : \theta \neq \theta_0$ |

The value $\theta_0$, called the **null value**, separates the alternative from the null. 

```{definition, hypothesis-test}
A **hypothesis test** asks if the available data provides sufficient evidence to reject $H_0$. If the observations disagree with $H_0$, then we reject the null hypothesis. If the sample evidence does not strongly contradict $H_0$, then we continue to believe $H_0$. The two possible conclustions of a hypothesis test are: *reject $H_0$* or *fail to reject $H_0$*.^[We comment that *fail to reject $H_0$* is sometimes phrased *retain $H_0$* or (perhaps less accurately) *accept $H_0$*.]  
```  

A procedure for carrying out a hypothesis test is based on specifying two additional items: a test statistic and a corresponding rejection region. A **test statistic** is a function of the sample data (like an estimator). The statistical decision to reject or fail to reject the null hypothesis will involve computing the test statistic. The **rejection region** are the values of the test statistic for which the null hypothesis is to be rejected in favor of the alternative. That is, we compute the test statistic based on a given sample; the test statistic either falls in the rejection region---in which case we reject the null $H_0$---or it does not fall in the rejection region---in which case we fail to reject the null $H_0$. 

```{example, eg-hyp-test-def}
**TODO**: example hypothesis test
```

When carrying out a hypothesis test, two types of errors can be made. The basis for choosing a rejection region typically involves considering these errors. 

```{definition, error-types}
A **type I** error occurs if $H_0$ is rejected when $H_0$ is actually true. A **type II** error is made if we fail to reject $H_0$ when $H_0$ is actually false. 
```

```{example, eg-hyp-test-errors}
**TODO**: example hypothesis error types
```  

To summarize, the elements of a statistical test are:  

1. Null hypothesis, $H_0$
2. Alternative hypothesis, $H_a$
3. Test statistic
4. Rejection region  

## Estimating means {#estimating-means}

If the parameter of interest is the population mean $\theta = \mu$, then the sample mean estimator $\widehat{\theta} = \overline{X}$ in \@ref(eq:sample-mean) has (at least approximately) a normal distribution for sufficiently large $n$. We will consider three cases where the form of the confidence interval can be derived using the approximate normality of the sample mean:  

1. [CI for $\mu$ of a normal population with known $\sigma^2$](#ci-normal-var-known),
2. [CI for $\mu$ of any population with unknown $\sigma^2$, when the sample size $n$ is large](#ci-large-sample),
3. [CI for $\mu$ of a normal population with unknown $\sigma^2$, when the sample size $n$ is small](#ci-normal-var-unknown).  

In general, the confidence intervals for the mean based on normality theory will have the form:
\begin{equation}
\text{point estimate}\; \mu \pm (\text{critical value of reference dist.}) \cdot (\text{precision of point estimate})\,, (\#eq:ci-gen-form)
\end{equation}
where the reference distribution will be the standard normal (for 1. and 2.) and the Student's $t$ distribution (for 3.). The critical value corresponds to the two-sided (symmetric) tail areas under the reference distribution.  

### CI for mean of a Normal population with known variance {#ci-normal-var-known}

```{definition, ci-norm-known-var}
A **$100(1-\alpha)\%$ confidence interval** for the mean $\mu$ of a normal population when the value of $\sigma^2$ is known is given by 
\begin{equation} 
 \left(\overline{x} - z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}\,, 
        \overline{x} + z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}} \right)\,,  (\#eq:ci-norm-known-var)
\end{equation}
or $\overline{x} \pm z_{\alpha/2} \cdot \sigma / \sqrt{n}$. 
```  

The CI for the mean \@ref(eq:ci-norm-known-var) can be expressed as 
\begin{equation*}
 \text{point estimate}\; \mu \pm 
 (z \;\text{critical value}) \cdot (\text{standard error of mean})\,.
\end{equation*}
The $z$ critical value is related to the tail areas under the standard normal curve; we need to find the $z$-score having a cumulative probability equal to $1-\alpha$ according to Definition \@ref(def:confidence-interval-gen). Below we provide a table containing commonly used normal critical values (note: indexed by $\alpha/2$).  

```{r table-activities, echo=FALSE, warning = FALSE, message = FALSE, fig.cap = "$z$ critical values."}
kable(tibble(read_csv("data/z-crit-val.csv")))
```  

**TODO**: add example 

As noted in \@ref(eq:ci-gen-form) and \@ref(eq:ci-norm-known-var), the width of a CI is related to the estimator's precision. The confidence level (or reliability) is inversely related to this precision. When the population is normal and the variance is known, then an appealing strategy is to determine the sample size necessary to achieve a desired confidence level and precision. A general formula for the sample size $n$ necessary to achieve an interval width $w$ is obtained at confidence level  $\alpha$ is obtained by equating $w$ to $2z_{\alpha/2} \cdot \sigma /\sqrt{n}$ and then solving for $n$. 

```{proposition, ci-select-n-fixed-w-alpha}
The sample size $n$ required to achieve a CI for $\mu$ with width $w$ at level $\alpha$ is given by,
\begin{equation}
n = \left( 2 z_{\alpha/2} \cdot \frac{\sigma}{w} \right)^2 \,.
\end{equation}
```  
From Proposition \@ref(prp:ci-select-n-fixed-w-alpha), we see that the smaller the desired $w$ then the larger $n$ must be (and subsequently, the more effort that must be allocated to data collection).

**TODO**: add example of sample size calculation

### Large-sample CI for mean of a population with unknown variance {#ci-large-sample}

Consider samples $X_1, \dots, X_n$ from a population with mean $\mu$ and variance $\sigma^2$. Provided that $n$ is large enough, the Central Limit Theorem implies that the estimator for the sample mean $\overline{X}$ in \@ref(eq:sample-mean) has *approximately* a normal distribution. Then 
\begin{equation}
P \left( - z_{\alpha/2} < \frac{\overline{X} - \mu}{\sigma/\sqrt{n}} < z_{\alpha/2} \right) \approx 1 - \alpha\,,
\end{equation}
since the transformed variable has approximately a standard normal distribution. Thus, computing a point estimate based on a large $n$ of samples yields a CI for the population parameter $\mu$ at an *approximate* confidence level $\alpha$. However, it is often the case that the variance is unknown. When $n$ is large, replacing the population variance $\sigma^2$ by the sample variance $S^2$ in \@ref(eq:sample-var) will not typically introduce too much additional variability.

```{proposition, ci-large-sample}
For large samples $n$, an approximate $100(1-\alpha)\%$ confidence interval for the mean $\mu$ of any population when the variance is uknown is given by 
\begin{equation} 
 \left(\overline{x} - z_{\alpha/2} \cdot \frac{s}{\sqrt{n}} \,, 
        \overline{x} + z_{\alpha/2} \cdot \frac{s}{\sqrt{n}} \right)\,,  (\#eq:ci-large-sample)
\end{equation}
or $\overline{x} \pm z_{\alpha/2} \cdot s / \sqrt{n}$. 
```

The CI for the mean \@ref(eq:ci-large-sample) applies regardless of the shape of the population distribution so long as the number of samples is large. A rule of thumb is that $n > 40$ is sufficient. In words, the CI \@refeq:ci-large-sample) can be expressed as 
\begin{equation*}
 \text{point estimate}\; \mu \pm 
 (z \;\text{critical value}) \cdot (\text{estimated standard error of mean})\,.
\end{equation*}
Typically, a large-sample CI for a general parameter $\theta$ similar to \@ref(eq:ci-large-sample) holds for any estimator $\widehat{\theta}$ that satisfies: (1) approximately normal in distribution, (2) approximately unbiased, and (3) an expression for the standard error is available.

**TODO**: add example 

### CI for mean of a normal population with unknown variance {#ci-normal-var-unknown}

In Section \@ref(ci-normal-var-known), we considered samples $X_1, \dots, X_n$ from a normal population with a known $\mu$ and $\sigma^2$. In contrast, here we consider samples from a normal population and assume the population parameters $\mu$ and $\sigma^2$ are unknown. If the number of samples is large, the discussion in Section \@ref(ci-large-sample) indicates that the rv $Z = (\overline{X} - \mu) \sqrt{n} / S$ has approximately a standard normal distribution. However, if $n$ is not sufficiently large then the transformed variable will be more spread out than a standard normal distribution. 

```{theorem, sample-mean-t-dist}
For the sample mean $\overline{X}$ based on $n$ samples from a normal distribution with mean $\mu$, the rv
\begin{equation}
 T = \frac{\overline{X} - \mu}{S} \sqrt{n} \quad \sim t_{n-1}\,,
\end{equation}
that is, $T$ has Student's $t$ distribution with $\nu = n-1$ degrees of freedom (df). 
```

This leads us to consider a CI for the population parameter $\mu$ that is based on critical values of the $t$ distribution. 

```{proposition, ci-norm-unknown-var}
A **$100(1-\alpha)\%$ confidence interval** for the mean $\mu$ of a normal population when the value of $\sigma^2$ is unknown is given by 
\begin{equation} 
 \left(\overline{x} - t_{\alpha/2, n-1} \cdot \frac{s}{\sqrt{n}}\,, 
        \overline{x} + t_{\alpha/2, n-1} \cdot \frac{s}{\sqrt{n}} \right)\,,  (\#eq:ci-norm-known-var)
\end{equation}
or $\overline{x} \pm t_{\alpha/2, n-1} \cdot s/ \sqrt{n}$. Here $\overline{x}$ and $s$ are the sample mean and sample standard deviation, respectively.
```  

In contrast to Proposition \@ref(prp:ci-select-n-fixed-w-alpha), it is difficult to select the sample size $n$ to control the width of the $t$-based CI as the width involves the unknown (before the sample is acquired) $s$ and because $n$ also enters through $t_{\alpha/2, n-1}$.  

**TODO**: add example 
 
```{proposition, norm-ci-rule-thumb}
**TODO**: rule of thumb normal CI
```

**TODO**: add example using rule of thumb


## Estimating proportions {#estimating-proportions}

Consider a population of size $N$ in which a proportion $p$ of the population satisfies a given property. The $p \in (0,1)$ is a parameter characterizing the population, with distribution $F(p)$,^[Here we write $F$ for a general distribution, but what special distribution might this be?] that we might be interested in estimating. A sample, $X_1, \dots, X_n \sim F(p)$, of size $n$ from the population contains a proportion,
\begin{equation}
 \widehat{p} = \frac{1}{n} \sum_{i=1}^n X_i\,,
 (\#eq:proportion-estimator)
\end{equation}
satisfying the given property. The estimator $\widehat{p}$ varies with the sample and for large $n$ it's sampling distribution has the following properties:
\begin{equation*}
\mu_{\widehat{p}} = \E[X_i] = p 
 (\#eq:proportion-mean)
\end{equation*}
and 
\begin{equation}
 \sigma_{\widehat{p}}^2 = \frac{\Var[X_i]}{n} = \frac{p(1-p)}{n}\,,
 (\#eq:proportion-var)
\end{equation}
provided that $n$ is small relative to $N$ (a rule of thumb is $n \leq 0.05 N$).^[Note that if $n$ is large relative to $N$ ($n > 0.05 N$) then the variance \@ref(eq:proportion-var) must be adjusted by a factor (related to the hypergeometric distribution):
\begin{equation*}
 \sigma_{\widehat{p}}^2 = \frac{p(1-p)}{n} \frac{N-n}{N-1}\,,
\end{equation*}
where for fixed $n$ the factor converges to $1$ as $N\to \infty$.] Moreover, by invoking the Central Limit Theorem we have the distribution of $\widehat{p}$ is approximately normal for sufficiently large $n$ as \@ref(eq:proportion-estimator) is a sample mean. Indeed, this normal approximation works well for moderately large $n$ as long as $p$ is not too close to zero or one; a rule of thumb is that $np > 5$ and $n(1-p) > 5$.

```{proposition, ci-proportion}
For large samples $n$, a $100(1-\alpha)\%$ confidence interval for the parameter $p$ is given by
\begin{equation}
 \widehat{p} \pm z_{\alpha/2} \sqrt{\frac{\widehat{p} (1-\widehat{p})}{n}}\,.
 (\#eq:proportion-mean-ci)
\end{equation}
```

This follows from Proposition \@ref(prp:ci-large-sample) by observing that \@ref(eq:proportion-estimator) is a sample mean and replacing the standard error $\sigma_{\widehat{p}}$ from \@ref(eq:proportion-var) by the estimated standard error,
\begin{equation*}
 \widehat{\se}(\widehat{p}) = \sqrt{\frac{\widehat{p} (1-\widehat{p})}{n}}\,;
\end{equation*}
recall the $s$ in \@ref(eq:ci-large-sample) is the sample variance for the *population* and $s / \sqrt{n} = \se$ is the standard error of the point estimator.

```{example, eg-est-prop-norm-approx-binom}
**TODO**: Examples of sampling distribution for p
```

```{example, eg-est-prop-ci}
**TODO**: Example confidence interval for p
```

```{example, eg-est-prop-hypothesis-test}
**TODO**: Example hypothesis test 
```

## Estimating variances {#estimating-variances}

Next we consider estimates of the population variance (and standard deviation) when the population is assumed to have a normal distribution. In this case, the sample variance $S^2$ in \@ref(eq:sample-var) provides the basis for inferences. Consider iid samples $X_1, \dots, X_n \sim \mathcal{N}(\mu, \sigma^2)$. We provide the following theorem without proof.

```{theorem, samp-var-chisq}
For the sample variance $S^2$ based on $n$ samples from a normal distribution with variance $\sigma$, the rv
\begin{equation*}
V = \frac{(n-1)S^2}{\sigma^2} = \frac{\sum_i(X_i - \overline{X})^2}{\sigma^2} \qquad \sim \chi^2_{n-1}\,,
\end{equation*}
that is, $V$ has a $\chi^2$ distribution with $\nu = n-1$ df. 
```  

Based on Theorem \@ref(thm:samp-var-chisq), 
\begin{equation*}
P\left(\chi^2_{1-\alpha/2, n-1} < \frac{(n-1)S^2}{\sigma^2} < \chi^2_{\alpha/2, n-1} \right) = 1 - \alpha \,,
\end{equation*}
i.e., the area captured between the right and left tail critical $\chi^2$ values is $1-\alpha$. The expression above can be further manipulated to obtain an interval for the unknown parameter $\sigma^2$:
\begin{equation*}
P\left(\frac{(n-1) s^2}{\chi^2_{\alpha/2, n-1}} < \sigma^2 < \frac{(n-1) s^2}{\chi^2_{1-\alpha/2, n-1}} \right) = 1 - \alpha \,,
\end{equation*}
where we substitute the computed value of the point estimate $s^2$ for the estimator into the limits to give a CI for $\sigma^2$. If we take square roots of the inequality above, we obtain a CI for the population standard deviation $\sigma$. 

```{proposition, ci-variance}
A $100(1-\alpha)\%$ confidence interval for the variance of a normal population is
\begin{equation*}
 \left( (n-1)s^2 / \chi^2_{\alpha/2, n-1} \,,  (n-1)s^2 / \chi^2_{1-\alpha/2, n-1} \right) \,.
 (\#eq:ci-variance)
\end{equation*}
A $100(1-\alpha)\%$ confidence interval for the standard deviation $\sigma$ of a normal population is given by taking the square roots of the lower and upper limits in \@ref(eq:ci-variance).
```

```{example, eg-ci-variance}
**TODO**: Example CI for variance (using the tree data?)
```

***  