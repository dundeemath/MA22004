```{r setup, include=FALSE}
#  bookdown::render_book("index.Rmd", "bookdown::gitbook", output_dir = "docs/")

# packages
library(bookdown)
library(RColorBrewer)
library(readr)
library(plotly)
library(tidyverse)
library(reshape2)
library(latex2exp)
library(svglite)
library(knitr)
library(kableExtra)
library(formattable)
library(DT)
library(dplyr)
library(datasets)
# data sets
data(iris)
data(trees)

knitr::opts_chunk$set(echo = TRUE, comment = "") #, dev = "svglite", fig.width=8
#plotly::config(plot_ly(), displaylogo = FALSE, mathjax = "cdn")

df <- read_csv("data/anova-salaries.csv")
mse <- (5*(2.588)^2 + 3*(2.582)^2 + 4*(1.871)^2 + 4*(1.924)^2)/16
mstr <- (6*(14.5-11.9)^2 + 4*(10.0 - 11.9)^2 + 5*(13.0-11.9)^2 + 5*( 9.2 - 11.9)^2) / 3
```

\newcommand{\Var}{\operatorname{Var}}
\newcommand{\E}{\operatorname{E}}
\newcommand{\se}{\mathsf{se}}

# Analysis of variance (ANOVA) {#anova}

Analysis of variance, shortened as ANOVA, is a collection of statistical models and estimation procedures for analyzing the variation among different groups. In particular, a single-factor ANOVA provides a hypothesis test regarding the equality of two or more population means, thereby generalizing the one-sample and two-sample $\mathsf{t}$ tests considered in Sections \@ref(mean-normal-var-unknown) and \@ref(compare-means-normpops-vars-unknown).

## Single factor ANOVA test {#anova-single-factor-test}

Suppose that we have $k$ normally distributed populations^[In the context of ANOVA, these $k$ populations are often referred to as **treatment distributions**.] with different means $\mu_1, \dots, \mu_k$ and equal variances $\sigma^2$. We denote the rv for the $j$th measurement taken from the $i$th population by $X_{ij}$ and the corresponding sample observation by $x_{ij}$. For samples of size $m_1, \dots, m_k$, we denote the sample means
\begin{equation*}
 \overline{X}_i = \frac{1}{m_i} \sum_{j=1}^{m_i} X_{ij}\,, 
\end{equation*}
and sample variances 
\begin{equation*}
 S_i^2 = \frac{1}{m_i - 1} \sum_{j=1}^{m_i} (X_{ij} - \overline{X}_{i})^2\,,
\end{equation*}
for each $i = 1, \dots, k$; likewise, we denote the associated point estimates for the sample means $\overline{x}_1, \dots, \overline{x}_k$ and the sample variances $s_1^2, \dots, s_k^2$. The average over all observations $m = \sum m_i$, called the **grand mean**, is denoted by 
\begin{equation*}
 \overline{X} = \frac{1}{m} \sum_{i=1}^k \sum_{j=1}^{m_i} X_{ij}\,.
\end{equation*} 
The sample variances $s_i^2$, and hence the sample standard deviations, will generally vary even when the $k$ populations share the same variance; a rule of thumb is that the equality of variances is reasonable if the largest $s_i$ is not much more than two times the smallest. 

We wish to test the equality of the population means, given by the null hypothesis,
\begin{equation*}
 H_0 : \mu_1 = \mu_2 = \cdots = \mu_k\,,
\end{equation*}
versus the alternative hypothesis,
\begin{equation*}
 H_a : \text{at least two}\; \mu_i \; \text{differ}\,.
\end{equation*}
Note that if $k=3$ then $H_0$ is true only if all three means are the same, i.e., $\mu_1 = \mu_1 = \mu_3$, but there are a number of ways which the alternative might hold: $\mu_1 \neq \mu_2 = \mu_3$ or $\mu_1 = \mu_2 \neq \mu_3$ or $\mu_1 = \mu_3 \neq \mu_2$ or $\mu_1 \neq \mu_2 \neq \mu_3$. 

The test procedure is based on comparing a measure of difference in variation among the sample means, i.e., the variation between $x_i$'s, to a measure of variation within each sample.  

```{definition, mstr-mse}
The **mean square for treatments** is 
\begin{equation*}
 \mathsf{MSTr} = \frac{1}{k-1} \sum_{i=1}^k m_i (\overline{X}_i - \overline{X})^2\,,
\end{equation*}
and the **mean square error** is 
\begin{equation*}
 \mathsf{MSE} = \frac{1}{m-k} \sum_{i=1}^k (m_i - 1) S_i^2 \,.
\end{equation*}
The $\mathsf{MSTr}$ and $\mathsf{MSE}$ are statistics that measure, respectively, the variation among sample means and the variation within samples. We will also use $\mathsf{MSTr}$ and $\mathsf{MSE}$ to denote calculated values of these statistics. 
```

```{proposition, htest-anova}
The test statistic 
\begin{equation*}
F = \frac{\mathsf{MSTr}}{\mathsf{MSE}} 
\end{equation*}
is the appropriate test statistic for the single-factor ANOVA problem involving $k$ populations (or treatments) with a random sample of size $m_1, \dots, m_k$ from each. When $H_0$ is true, 
\begin{equation*}
F \sim \mathsf{F}(\nu_1 = k-1, \nu_2 = m - k)\,.
\end{equation*}
In the present context a large test statistic value is more constradictory to $H_0$ than a smaller value, therefore the test is upper-tailed, i.e., consider the area $F_\alpha$ to the right of the critical value $F_{\alpha, \nu_1, \nu_2}$. We reject $H_0$ if the value of the test statistic $F > F_\alpha$.
```   


```{example, anova}
Consider the average salary data from lcocal councils in Table \@ref(tab:anova-samples-stats). Is the expected average salary in each nation the same at the $5\%$ level?
```  

```{r anova-samples-stats, echo = FALSE, warning = FALSE, message = FALSE}
df$nation <- factor(df$nation)
dattab <- df %>%
 group_by(nation) %>%
 summarise(observations = list(mean_salary), n = n(), means = signif(mean(mean_salary), digits = 4), ssds = signif(sd(mean_salary), digits = 4))
kable(dattab, col.names = c('Nation', "Average salaries ('000 Â£)", 'Size $(m_i)$', 'Sample Mean $(\\overline{x}_i)$', 'Sample SD $(s_i)$'), align = "llccc", caption = "Data and summary sample statistics for Figure \\@ref(fig:anova-samples-boxplots).")
```   

> Table \@ref(tab:anova-samples-stats) presents the average salaries (in thousands of pounds) reported from 20 local councils classified by nation (England, N Ireland, Scotland, and Wales). The sample means together with the sample standard deviations are summarized in the table as well as presented using using box plots in Figure \@ref(fig:anova-samples-boxplots).  


```{r anova-samples-boxplots, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Box plots of the average mean salary data in Table \\@ref(tab:anova-samples-stats) indicate five summary statistics: the median, two hinges (first and third quartiles) and two whiskers (extending from the hinge to the most extreme data point within 1.5 * IQR)."}
df$nation <- factor(df$nation)
ggplot(df, aes(x = mean_salary, y = reorder(nation, desc(nation)))) + geom_boxplot() + theme(axis.title.y = element_blank())
```      

For $\alpha = 0.05$, we compute the upper-tail area $F_{0.05}$ i.e. to the right of the critical value $F_{0.05, 3, 16}$ by consulting a statistical table or by using `r`:

```{r eg-code-F-upper-tail-area, echo=TRUE}
qf(1-.05, df1 = 4-1, df2 = 20-4) # alt: qf(.05, df1 = 4-1, df2 = 20-4, lower.tail = FALSE)
```  

to find $F_{0.05} = `r qf(.05, df1 = 4-1, df2 = 20-4, lower.tail = FALSE)`$.  
 
The grand mean is 
\begin{equation*}
 \overline{x} = \frac{17 + 12 + 18 + \cdots + 8 + 7 + 9}{20} = `r mean(df$mean_salary)`\,,
\end{equation*}
and hence the variation among sample means is given by,
\begin{equation*}
\begin{aligned}
 \mathsf{MSTr} &= \frac{1}{4-1} \left(m_1(\overline{x}_1 - \overline{x})^2 + \cdots + m_4 (\overline{x}_4 - \overline{x})^2\right) \\
 &= \left(6 (14.5-11.9)^2 + 4(10.0 - 11.9)^2 + 5(13.0-11.9)^2 + 5 ( 9.2 - 11.9)^2\right) / 3 \\
 &= `r mstr` \,.
\end{aligned}
\end{equation*}
The mean square error is
\begin{equation*}
 \begin{aligned}
 \mathsf{MSE} & = \frac{1}{20-4} \left((m_1 - 1)s_1^2 + \cdots (m_4-1)s_4^2\right)\\ 
  &= \frac{5(2.588)^2 + 3(2.582)^2 + 4(1.871)^2 + 4(1.924)^2}{16} \\
  &= `r mse`
 \end{aligned}
\end{equation*}
yielding the test statistic value
\begin{equation*}
 F = \frac{\mathsf{MSTr}}{\mathsf{MSE}} = \frac{`r mstr`}{`r mse`} 
 = `r mstr / mse` \,.
\end{equation*}
Since $F > F_\alpha$ we reject $H_0$. The data does not support they hypothesis that the mean salaries in each nation are identical at the $5\%$ level. $\lozenge$    

## Confidence intervals {#anova-ci}

In Section \@ref(compare-means) we gave a CI for comparing population means that involved the difference $\mu_X - \mu_Y$. In some settings, we would like to give CIs for more complicated functions of population means $\mu_i$. Let
\begin{equation*}
 \theta = \sum_{i=1}^k c_i \mu_i\,,
\end{equation*}
for constants $c_i$. As we assume the $X_ij$ are normally distributed with $\E[X_{ij}] = \mu_i$ and $\Var[X_{ij}] = \sigma^2$, the estimator
\begin{equation*}
 \widehat{\theta} = \sum_{i=1}^k c_i \overline{X}_{i}\,,
\end{equation*}
is normally distributed with
\begin{equation*}
 \Var[\widehat{\theta}] - \sum_{i=1}^k c_i^2 \Var[\overline{X}_i] = \sigma^2 \sum_{i=1}^{k} \frac{c_i}{m_i}\,.
\end{equation*}
Estimating $\sigma^2$ by the \mathsf{MSE} and forming $\widehat{\sigma}_{\widehat{\theta}}$ results in a $\mathsf{t}$ variable
\begin{equation*}
 \frac{\widehat{\theta} - \theta}{\widehat{\sigma}_{\widehat{\theta}}}\,.
\end{equation*}

```{proposition, ci-anova}
A $100(1-\alpha)\%$ CI for $\sum c_i \mu_i$ is given by 
\begin{equation*}
 \sum_{i=1}^k c_i \overline{x}_i \pm t_{\alpha/2, m-k} \sqrt{\mathsf{MSE} \sum_{i=1}^k \frac{c_i}{m_i}}\,.
\end{equation*}
```

```{example, anova-ci}
Determine a $90\%$ CI for the difference in mean average salary for councils in Scotland and England, based on the data available in Table \@ref(tab:anova-samples-stats)
```

For $\alpha = 0.10$, the critical value $t_{0.05, 16} = `r qt(1-0.1/2, df = 20 - 4)`$ is found by looking in a table of $\mathsf{t}$ critical values or by using `r`:

```{r eg-code-crit-t-val-anova, echo=TRUE}
qt(1-0.1/2, df = 20 - 4) # alt: qt(0.1/2, 16, lower.tail = FALSE)
```

Then for the function $\overline{x}_2 - \overline{x_1}$, 
\begin{equation*}
\begin{aligned}
(\overline{x}_{Eng} - \overline{x}_{Sco}) \pm& t_{0.05, 16} \sqrt{\mathsf{MSE}} \sqrt{\frac{1}{m_{Eng}} + \frac{1}{m_{Sco}}} \\
& = (14.5 - 13.0) \pm `r qt(1-0.1/2, df = 20 - 4)` \sqrt{`r mse`} \sqrt{\frac{1}{6} + \frac{1}{5}} \\
& = 1.5 \pm `r qt(1-0.1/2, df = 20 - 4) * sqrt(mse) * sqrt(1/6 + 1/5)`\,.
\end{aligned}
\end{equation*}
Thus a $90\%$ confidence interval for $\mu_{Eng} - \mu_{Sco}$ is $(`r 1.5 - qt(1-0.1/2, df = 20 - 4) * sqrt(mse) * sqrt(1/6 + 1/5)`\,, `r 1.5 + qt(1-0.1/2, df = 20 - 4) * sqrt(mse) * sqrt(1/6 + 1/5)`)$. $\lozenge$   

> How does the result in Example \@ref(exm:anova-ci) compare to the $\mathsf{t}$ method in Section \@ref(compare-means-normpops-vars-unknown)?