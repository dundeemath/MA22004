<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Inferences based on two samples | MA22004 – Statistics and Probability II</title>
  <meta name="description" content="Course guide and course notes for MA22004 - Statistics and Probability II. Division of Mathematics, University of Dundee." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Inferences based on two samples | MA22004 – Statistics and Probability II" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://dundeemath.github.io/MA22004" />
  
  <meta property="og:description" content="Course guide and course notes for MA22004 - Statistics and Probability II. Division of Mathematics, University of Dundee." />
  <meta name="github-repo" content="dundeemath/MA22004" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Inferences based on two samples | MA22004 – Statistics and Probability II" />
  
  <meta name="twitter:description" content="Course guide and course notes for MA22004 - Statistics and Probability II. Division of Mathematics, University of Dundee." />
  

<meta name="author" content="Dr Eric Hall" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inference-single-sample.html"/>
<link rel="next" href="anova.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MA22004</a></li>

<li class="divider"></li>
<li class="part"><span><b>Course Documents</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html"><i class="fa fa-check"></i>Course Guide</a>
<ul>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-organization"><i class="fa fa-check"></i>Organisation</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-timetable"><i class="fa fa-check"></i>Timetable</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-prerecs"><i class="fa fa-check"></i>Pre-requisites</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-syllabus"><i class="fa fa-check"></i>Syllabus</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-books"><i class="fa fa-check"></i>Recommended Books</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-assessment"><i class="fa fa-check"></i>Assessment</a>
<ul>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-assessment-coursework"><i class="fa fa-check"></i>Coursework</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-assessment-exams"><i class="fa fa-check"></i>Examinations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-commitment"><i class="fa fa-check"></i>Your Commitment</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-calcs"><i class="fa fa-check"></i>Approved Calculators</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-support"><i class="fa fa-check"></i>Study Support</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-disability"><i class="fa fa-check"></i>Disability</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-honesty"><i class="fa fa-check"></i>Academic Honesty</a></li>
<li class="chapter" data-level="" data-path="cg.html"><a href="cg.html#cg-questionaire"><i class="fa fa-check"></i>End of Module Questionaire</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="labs.html"><a href="labs.html"><i class="fa fa-check"></i>Lab Guide</a></li>
<li class="chapter" data-level="" data-path="deadlines.html"><a href="deadlines.html"><i class="fa fa-check"></i>Deadlines</a></li>
<li class="part"><span><b>Course Notes</b></span></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i>Preliminaries</a>
<ul>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#notation"><i class="fa fa-check"></i>Notation</a></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#abbreviations"><i class="fa fa-check"></i>Abbreviations</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="sampling-distributions.html"><a href="sampling-distributions.html"><i class="fa fa-check"></i><b>1</b> Sampling distributions</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>1.1</b> Normal distribution</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#facts-normals"><i class="fa fa-check"></i><b>1.1.1</b> Some useful facts about normal variates</a></li>
<li class="chapter" data-level="1.1.2" data-path="sampling-distributions.html"><a href="sampling-distributions.html#empirical-rule-68-95-99.7-rule"><i class="fa fa-check"></i><b>1.1.2</b> Empirical rule (<span class="math inline">\(68-95-99.7\)</span> rule)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="sampling-distributions.html"><a href="sampling-distributions.html#t-distribution"><i class="fa fa-check"></i><b>1.2</b> <span class="math inline">\(\mathsf{t}\)</span> distribution</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#facts-t"><i class="fa fa-check"></i><b>1.2.1</b> Properties of <span class="math inline">\(\mathsf{t}\)</span> distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="sampling-distributions.html"><a href="sampling-distributions.html#chisq-distribution"><i class="fa fa-check"></i><b>1.3</b> <span class="math inline">\(\chi^2\)</span> distribution</a></li>
<li class="chapter" data-level="1.4" data-path="sampling-distributions.html"><a href="sampling-distributions.html#F-distribution"><i class="fa fa-check"></i><b>1.4</b> <span class="math inline">\(\mathsf{F}\)</span> distribution</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>2</b> Basics of statistical inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-inference.html"><a href="statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>2.1</b> Point estimation</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>2.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-inference.html"><a href="statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>2.3</b> Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference-single-sample.html"><a href="inference-single-sample.html"><i class="fa fa-check"></i><b>3</b> Inferences based on a single sample</a>
<ul>
<li class="chapter" data-level="3.1" data-path="inference-single-sample.html"><a href="inference-single-sample.html#estimating-means"><i class="fa fa-check"></i><b>3.1</b> Estimating means</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="inference-single-sample.html"><a href="inference-single-sample.html#mean-normal-var-known"><i class="fa fa-check"></i><b>3.1.1</b> Mean of a normal population with known variance</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference-single-sample.html"><a href="inference-single-sample.html#mean-large-sample"><i class="fa fa-check"></i><b>3.1.2</b> Mean of a population with unknown variance (large-sample)</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference-single-sample.html"><a href="inference-single-sample.html#mean-normal-var-unknown"><i class="fa fa-check"></i><b>3.1.3</b> Mean of a normal population with unknown variance</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inference-single-sample.html"><a href="inference-single-sample.html#estimating-proportions"><i class="fa fa-check"></i><b>3.2</b> Estimating proportions</a></li>
<li class="chapter" data-level="3.3" data-path="inference-single-sample.html"><a href="inference-single-sample.html#estimating-variances"><i class="fa fa-check"></i><b>3.3</b> Estimating variances</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inference-two-samples.html"><a href="inference-two-samples.html"><i class="fa fa-check"></i><b>4</b> Inferences based on two samples</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-means"><i class="fa fa-check"></i><b>4.1</b> Comparing means</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-means-normpops-vars-known"><i class="fa fa-check"></i><b>4.1.1</b> Comparing means of normal populations when variances are known</a></li>
<li class="chapter" data-level="4.1.2" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-means-large-samples"><i class="fa fa-check"></i><b>4.1.2</b> Comparing means when the sample sizes are large</a></li>
<li class="chapter" data-level="4.1.3" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-means-normpops-vars-unknown"><i class="fa fa-check"></i><b>4.1.3</b> Comparing means of normal populations when variances are unknown and the sample size is small</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-paired-samples"><i class="fa fa-check"></i><b>4.2</b> Comparing paired samples</a></li>
<li class="chapter" data-level="4.3" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-proportions"><i class="fa fa-check"></i><b>4.3</b> Comparing proportions</a></li>
<li class="chapter" data-level="4.4" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-variances"><i class="fa fa-check"></i><b>4.4</b> Comparing variances</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>5</b> Analysis of variance (ANOVA)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="anova.html"><a href="anova.html#anova-single-factor-test"><i class="fa fa-check"></i><b>5.1</b> Single factor ANOVA test</a></li>
<li class="chapter" data-level="5.2" data-path="anova.html"><a href="anova.html#anova-ci"><i class="fa fa-check"></i><b>5.2</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>6</b> Linear regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-models.html"><a href="linear-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.1</b> Simple linear regression models</a></li>
<li class="chapter" data-level="6.2" data-path="linear-models.html"><a href="linear-models.html#ls-estimate-var"><i class="fa fa-check"></i><b>6.2</b> Estimating <span class="math inline">\(\sigma^2\)</span> for linear regressions</a></li>
<li class="chapter" data-level="6.3" data-path="linear-models.html"><a href="linear-models.html#inference-ls"><i class="fa fa-check"></i><b>6.3</b> Inferences for least-squares parameters</a></li>
<li class="chapter" data-level="6.4" data-path="linear-models.html"><a href="linear-models.html#correlation"><i class="fa fa-check"></i><b>6.4</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-data.html"><a href="categorical-data.html"><i class="fa fa-check"></i><b>7</b> Categorical data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="categorical-data.html"><a href="categorical-data.html#multinomial-experiments"><i class="fa fa-check"></i><b>7.1</b> Multinomial experiments</a></li>
<li class="chapter" data-level="7.2" data-path="categorical-data.html"><a href="categorical-data.html#goodness-of-fit-tests"><i class="fa fa-check"></i><b>7.2</b> Goodness-of-fit for a single factor</a></li>
<li class="chapter" data-level="7.3" data-path="categorical-data.html"><a href="categorical-data.html#test-for-independence-of-factors"><i class="fa fa-check"></i><b>7.3</b> Test for independence of factors</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="quality-control.html"><a href="quality-control.html"><i class="fa fa-check"></i><b>8</b> Quality control</a>
<ul>
<li class="chapter" data-level="8.1" data-path="quality-control.html"><a href="quality-control.html#control-charts"><i class="fa fa-check"></i><b>8.1</b> Control charts</a></li>
</ul></li>
<li class="part"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html"><i class="fa fa-check"></i>Curated Content</a>
<ul>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-0"><i class="fa fa-check"></i>Investigation 0</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-1"><i class="fa fa-check"></i>Investigation 1</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-2"><i class="fa fa-check"></i>Investigation 2</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-3"><i class="fa fa-check"></i>Investigation 3</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-4"><i class="fa fa-check"></i>Investigation 4</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-5"><i class="fa fa-check"></i>Investigation 5</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-6"><i class="fa fa-check"></i>Investigation 6</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-7"><i class="fa fa-check"></i>Investigation 7</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-8"><i class="fa fa-check"></i>Investigation 8</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="http://uod.ac.uk/sig-home" rel="nofollow"><img width="73" height="73" src="https://www.dundee.ac.uk/media/dundeewebsite/themes/brandnewhope/img/university-of-dundee-email-favicon.png" alt="University of Dundee shield logo"> </a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MA22004 – Statistics and Probability II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<span class="math inline">\(\DeclareMathOperator{\E}{\mathbf{E}}\)</span>
<span class="math inline">\(\DeclareMathOperator{\Var}{Var}\)</span>
<span class="math inline">\(\DeclareMathOperator{\Cov}{Cov}\)</span>
<span class="math inline">\(\DeclareMathOperator{\corr}{corr}\)</span>
<span class="math inline">\(\newcommand{\se}{\mathsf{se}}\)</span>
<span class="math inline">\(\DeclareMathOperator{\sd}{sd}\)</span>
<div id="inference-two-samples" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Inferences based on two samples</h1>
<p>We consider inferences—estimators, confidence intervals, and hypothesis testing—for comparing means, proportions, and variances based on two independent samples from different populations, respectively, in Sections <a href="inference-two-samples.html#compare-means">4.1</a>, <a href="inference-two-samples.html#compare-proportions">4.3</a>, <a href="inference-two-samples.html#compare-variances">4.4</a>. We also consider inferences when the samples are not independent, so-called paired samples, in Section <a href="inference-two-samples.html#compare-paired-samples">4.2</a>.</p>
<div id="compare-means" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Comparing means</h2>
<p>Let us assume that we have two normal populations with iid samples
<span class="math display">\[\begin{equation*}
 X_1, \dots, X_m \sim \mathsf{N}(\mu_X, \sigma_X^2)
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
 Y_1, \dots, Y_n \sim \mathsf{N}(\mu_Y, \sigma_Y^2)
\end{equation*}\]</span>
and, moreover, that the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> samples are independent of one another. When comparing the means of two populations, the quantity of interest is the difference: <span class="math inline">\(\mu_X - \mu_Y\)</span>.</p>

<div class="proposition">
<span id="prp:qoi-diff-pop-means" class="proposition"><strong>Proposition 4.1  </strong></span>If we consider the sample means <span class="math inline">\(\overline{X}\)</span> and <span class="math inline">\(\overline{Y}\)</span>, then the mean of the variable <span class="math inline">\(\overline{X} - \overline{Y}\)</span> is,
<span class="math display">\[\begin{equation*}
 \mu_{\overline{X} - \overline{Y}} 
= \E \left[ \overline{X} - \overline{Y} \right] = \mu_X - \mu_Y\,,
\end{equation*}\]</span>
and the variance is,
<span class="math display">\[\begin{equation*}
 \sigma_{\overline{X} - \overline{Y}}^2
= \Var \left[ \overline{X} - \overline{Y} \right]
= \frac{\sigma_X^2}{m} + \frac{\sigma_Y^2}{n} \,.
\end{equation*}\]</span>
</div>
<p>Proposition <a href="inference-two-samples.html#prp:qoi-diff-pop-means">4.1</a> follows directly from the definition of the sample mean in <a href="statistical-inference.html#eq:sample-mean">(2.1)</a> and properties of expectation and variance. If our parameter of interest is <span class="math display">\[\begin{equation*}
 \theta = \mu_1 - \mu_2\,,
\end{equation*}\]</span>
then its estimator,
<span class="math display">\[\begin{equation*}
 \widehat{\theta} = \overline{X} - \overline{Y}\,,
\end{equation*}\]</span>
is normally distributed with mean and variance given by Proposition <a href="inference-two-samples.html#prp:qoi-diff-pop-means">4.1</a>. If the samples sizes <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are large, then the estimator is approximately normally distributed by the Central Limit Theorem regardless of the population. We now discuss CIs and hypothesis tests for comparing population means <span class="math inline">\(\theta = \mu_X - \mu_Y\)</span>. We consider three cases when comparing means:</p>
<ol style="list-style-type: decimal">
<li><a href="inference-two-samples.html#compare-means-normpops-vars-known">normal populations when the variances <span class="math inline">\(\sigma_X^2\)</span> and <span class="math inline">\(\sigma_Y^2\)</span> are known</a>,</li>
<li><a href="inference-two-samples.html#compare-means-large-samples">any populations with unknown variances <span class="math inline">\(\sigma_X^2\)</span> and <span class="math inline">\(\sigma_Y^2\)</span>, when the sample sizes <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are large</a>,</li>
<li><a href="inference-two-samples.html#compare-means-normpops-vars-unknown">normal populations when the variances <span class="math inline">\(\sigma_X^2\)</span> and <span class="math inline">\(\sigma_Y^2\)</span> are unknown, when the sample sizes <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are small</a>,</li>
</ol>
<p>noting that the development largely reflects that of Section <a href="inference-single-sample.html#estimating-means">3.1</a>.</p>
<div id="compare-means-normpops-vars-known" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Comparing means of normal populations when variances are known</h3>
<p>When <span class="math inline">\(\sigma_X^2\)</span> and <span class="math inline">\(\sigma_Y^2\)</span> are known, standardizing <span class="math inline">\(\overline{X} - \overline{Y}\)</span> yields the standard normal variable:
<span class="math display" id="eq:compare-means-standard-trans">\[\begin{equation}
 Z = \frac{\overline{X} - \overline{Y} - (\mu_X - \mu_Y)}{\sqrt{\frac{\sigma_X^2}{m} + \frac{\sigma_Y^2}{n}}}\quad \sim \mathsf{N}(0,1)\,.
 \tag{4.1}
\end{equation}\]</span><br />
Inferences proceed by treating the parameter of interest <span class="math inline">\(\theta\)</span> as in the single sample case using the test statistic <a href="inference-two-samples.html#eq:compare-means-standard-trans">(4.1)</a>.</p>

<div class="proposition">
<span id="prp:ci-compare-means-normpops-vars-known" class="proposition"><strong>Proposition 4.2  </strong></span>A <span class="math inline">\(100(1-\alpha)\%\)</span> CI for the parameter <span class="math inline">\(\theta = \mu_X - \mu_Y\)</span> based on samples of size <span class="math inline">\(m\)</span> from a normal population <span class="math inline">\(\mathsf{N}(\mu_X, \sigma_X^2)\)</span> and of size <span class="math inline">\(n\)</span> from <span class="math inline">\(\mathsf{N}(\mu_Y, \sigma_Y^2)\)</span> with known variances, is given by
<span class="math display">\[\begin{equation*}
 (\overline{x} + \overline{y}) \pm z_{\alpha/2} 
 \cdot \sqrt{\frac{\sigma_X^2}{m} + \frac{\sigma_Y^2}{n}} \,.
\end{equation*}\]</span>
</div>

<div class="proposition">
<p><span id="prp:htest-compare-means-normpops-vars-known" class="proposition"><strong>Proposition 4.3  </strong></span>Assume that we sample iid <span class="math inline">\(X_1, \dots, X_m \sim \mathsf{N}(\mu_X, \sigma_X^2)\)</span> and iid <span class="math inline">\(Y_1, \dots, Y_n \sim \mathsf{N}(\mu_Y, \sigma_Y^2)\)</span> and that the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> samples are independent.</p>
<p>Consider <span class="math inline">\(H_0 : \mu_X - \mu_Y = \theta_0\)</span>. The test statistic is
<span class="math display" id="eq:htest-compare-means-normpops-vars-known-statistic">\[\begin{equation}
 Z = \frac{\overline{X} - \overline{Y} - \theta_0}{\sqrt{\frac{\sigma_{X}^2}{m} + \frac{\sigma_{Y}^2}{n}}}\,.
 \tag{4.2}
\end{equation}\]</span></p>
<p>For a hypothesis test at level <span class="math inline">\(\alpha\)</span>, we use the following procedure:</p>
<p>If <span class="math inline">\(H_a : \mu_X - \mu_Y &gt; \theta_0\)</span>, then <span class="math inline">\(P = 1 - \Phi(z)\)</span>, i.e., upper-tail <span class="math inline">\(R = \{z &gt; z_{\alpha}\}\)</span>.</p>
<p>If <span class="math inline">\(H_a : \mu_X - \mu_Y &lt; \theta_0\)</span>, then <span class="math inline">\(P = \Phi(z)\)</span>, i.e., lower-tail <span class="math inline">\(R = \{z &lt; - z_{\alpha}\}\)</span>.</p>
If <span class="math inline">\(H_a : \mu_X - \mu_Y \neq \theta_0\)</span>, then <span class="math inline">\(P = 2(1-\Phi(|z|))\)</span>, i.e., two-tailed <span class="math inline">\(R = \{|z| &gt; z_{\alpha/2}\}\)</span>.
</div>
</div>
<div id="compare-means-large-samples" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Comparing means when the sample sizes are large</h3>
<p>When the samples are large, then the assumptions about normality of the populations and knowledge of the variances <span class="math inline">\(\sigma_X^2\)</span> and <span class="math inline">\(\sigma_Y^2\)</span> can be relaxed. For sufficiently large <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>, the difference of the sample means, <span class="math inline">\(\overline{X} - \overline{Y}\)</span>, has approximately a normal distribution for any underlying population distributions by the Central Limit Theorem. Moreover, if <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are large enough, then replacing the population variances with the sample variances <span class="math inline">\(S_X^2\)</span> and <span class="math inline">\(S_Y^2\)</span> will not increase the variability of the estimator or the test statistic too much.</p>

<div class="proposition">
<span id="prp:ci-compare-means-large-samples" class="proposition"><strong>Proposition 4.4  </strong></span>For <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> sufficiently large, an approximate <span class="math inline">\(100(1-\alpha)\%\)</span> CI for <span class="math inline">\(\mu_X - \mu_Y\)</span> for two samples from populations with any underlying distribution is given by
<span class="math display">\[\begin{equation*}
 (\overline{x} + \overline{y}) \pm z_{\alpha/2} 
 \cdot \sqrt{\frac{s_{X}^2}{m} + \frac{s_{Y}^2}{n}}
\end{equation*}\]</span>
</div>

<div class="proposition">
<span id="prp:htest-compare-means-large-samples" class="proposition"><strong>Proposition 4.5  </strong></span>Under the same assumptions and procedures as in Proposition <a href="inference-two-samples.html#prp:htest-compare-means-normpops-vars-known">4.3</a>, a large-sample, i.e., <span class="math inline">\(m &gt; 40\)</span> and <span class="math inline">\(n &gt; 40\)</span>, test statistic,
<span class="math display">\[\begin{equation*}
 Z = \frac{\overline{X} - \overline{Y} - \theta_0}{\sqrt{\frac{S_{X}^2}{m} + \frac{S_{Y}^2}{n}}}\,,
\end{equation*}\]</span>
can be used in place of <a href="inference-two-samples.html#eq:htest-compare-means-normpops-vars-known-statistic">(4.2)</a> for hypothesis testing.
</div>
</div>
<div id="compare-means-normpops-vars-unknown" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Comparing means of normal populations when variances are unknown and the sample size is small</h3>
<p>If <span class="math inline">\(\sigma_X\)</span> and <span class="math inline">\(\sigma_Y\)</span> are unknown and either sample is small (e.g., <span class="math inline">\(m &lt; 30\)</span> or <span class="math inline">\(n &lt;30\)</span>), but both populations are normally distributed, then we can use Student’s <span class="math inline">\(\mathsf{t}\)</span> distribution to make inferences. We provide the following theorem without proof.</p>

<div class="theorem">
<span id="thm:dist-t-compare-means-normpops" class="theorem"><strong>Theorem 4.1  </strong></span>When both population distributions are normal, the standardized variable
<span class="math display">\[\begin{equation*}
T = \frac{\overline{X}-\overline{Y} - (\mu_X - \mu_Y)}{\sqrt{\frac{S_X^2}{m} + \frac{S_Y^2}{n}}} 
\quad \sim \mathsf{t}(\nu)
\end{equation*}\]</span>
where the df <span class="math inline">\(\nu\)</span> is estimated from the data. Namely, <span class="math inline">\(\nu\)</span> is given by (round <span class="math inline">\(\nu\)</span> down to the nearest integer):
<span class="math display" id="eq:dist-t-compare-means-normpops-nu">\[\begin{equation}
 \nu = \frac{ \left( \frac{s_X^2}{m} + \frac{s_Y^2}{n} \right)^2}{\frac{(s_X^2 / m)^2}{m-1} + \frac{(s_Y^2/n)^2}{n-1}} 
 = \frac{ \left( s_{\overline{X}}^2 + s_{\overline{Y}}^2 \right)^2}{\frac{s_{\overline{X}}^4}{m-1} + \frac{s_{\overline{Y}}^4}{n-1}}
 \tag{4.3}
\end{equation}\]</span>
where <span class="math inline">\(s_X^2\)</span> and <span class="math inline">\(s_Y^2\)</span> are point estimators of the sample variances; alternatively, we see that the formula <a href="inference-two-samples.html#eq:dist-t-compare-means-normpops-nu">(4.3)</a> can also be written in terms of the standard error of the sample means:
<span class="math display">\[\begin{equation*}
 s_{\overline{X}} = \frac{s_X}{\sqrt{m}} 
 \quad \text{and} \quad \qquad 
 s_{\overline{Y}} = \frac{s_Y}{\sqrt{n}} \,.
\end{equation*}\]</span>
</div>
<p>The formula <a href="inference-two-samples.html#eq:dist-t-compare-means-normpops-nu">(4.3)</a> for the data-driven choice of <span class="math inline">\(\nu\)</span> calls for the computation of the standard error of the sample means.</p>

<div class="proposition">
<span id="prp:ci-compare-means-normpops-vars-unknown" class="proposition"><strong>Proposition 4.6  </strong></span>A <span class="math inline">\(100(1-\alpha)\%\)</span> CI for <span class="math inline">\(\mu_X - \mu_Y\)</span> for two samples of size <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> from normal populations where the variances are unknown is given by
<span class="math display">\[\begin{equation*}
 (\overline{x} - \overline{y}) \pm t_{\alpha/2, \nu} \sqrt{ \frac{s_X^2}{m} + \frac{s_Y^2}{n}}\,,
\end{equation*}\]</span>
where we recall that <span class="math inline">\(t_{\alpha/2, \nu}\)</span> is the <span class="math inline">\(\alpha/2\)</span> critical value of <span class="math inline">\(\mathsf{t}(\nu)\)</span> with <span class="math inline">\(\nu\)</span> given by <a href="inference-two-samples.html#eq:dist-t-compare-means-normpops-nu">(4.3)</a>.
</div>

<div class="proposition">
<p><span id="prp:htest-compare-means-normpops-vars-unknown" class="proposition"><strong>Proposition 4.7  </strong></span>Assume that we sample iid <span class="math inline">\(X_1, \dots, X_m\)</span> and iid <span class="math inline">\(Y_1, \dots, Y_n\)</span> from normal populations with with unknown variances and means <span class="math inline">\(\mu_X\)</span> and <span class="math inline">\(\mu_Y\)</span>, respecitvely, and that the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> samples are independent.</p>
<p>Consider <span class="math inline">\(H_0 : \mu_X - \mu_Y = \theta_0\)</span>. The test statistic is
<span class="math display" id="eq:htest-compare-means-normpops-vars-unknown-statistic">\[\begin{equation}
 T = \frac{\overline{X} - \overline{Y} - \theta_0}{\sqrt{\frac{S_{X}^2}{m} + \frac{S_{Y}^2}{n}}}\,.
 \tag{4.4}
\end{equation}\]</span></p>
<p>For a hypothesis test at level <span class="math inline">\(\alpha\)</span>, we use the following procedure:</p>
<p>If <span class="math inline">\(H_a : \mu_X - \mu_Y &gt; \theta_0\)</span>, then <span class="math inline">\(P\)</span>-value is the area under <span class="math inline">\(\mathsf{t}(\nu)\)</span> to the right of <span class="math inline">\(t\)</span>, i.e., upper-tail <span class="math inline">\(R = \{t &gt; t_{\alpha,\nu}\}\)</span>.</p>
<p>If <span class="math inline">\(H_a : \mu_X - \mu_Y &lt; \theta_0\)</span>, then <span class="math inline">\(P\)</span>-value is the area under <span class="math inline">\(\mathsf{t}(\nu)\)</span> to the left of <span class="math inline">\(t\)</span>, i.e., lower-tail <span class="math inline">\(R = \{t &lt; - t_{\alpha,\nu}\}\)</span>.</p>
<p>If <span class="math inline">\(H_a : \mu_X - \mu_Y \neq \theta_0\)</span>, then <span class="math inline">\(P\)</span>-value is twice the area under <span class="math inline">\(\mathsf{t}(\nu)\)</span> to the right of <span class="math inline">\(|t|\)</span>, i.e., two-tailed <span class="math inline">\(R = \{|t| &gt; t_{\alpha/2, \nu}\}\)</span>.</p>
Here <span class="math inline">\(\nu\)</span> is given by <a href="inference-two-samples.html#eq:dist-t-compare-means-normpops-nu">(4.3)</a>.
</div>
<p>If the variances of the normal populations are unknown but are the same, <span class="math inline">\(\sigma_X^2 = \sigma_Y^2\)</span>, then deriving CIs and test statistics for comparing the means can be simplified by considering a combined or pooled estimator for the single parameter <span class="math inline">\(\sigma^2\)</span>. If we have two samples from populations with variance <span class="math inline">\(\sigma^2\)</span>, then each sample provides an estimate for <span class="math inline">\(\sigma^2\)</span>. That is, <span class="math inline">\(S_X^2\)</span>, based on the <span class="math inline">\(m\)</span> observations of the first sample, is one estimator for <span class="math inline">\(\sigma^2\)</span> and another is given by <span class="math inline">\(S_Y^2\)</span>, based on <span class="math inline">\(n\)</span> observations of the second sample. The correct way to combine these two estimators into a single estimator for the sample variance is to consider the <strong>pooled estimator</strong> of <span class="math inline">\(\sigma^2\)</span>,
<span class="math display" id="eq:pooled-sample-var">\[\begin{equation}
 S_{\mathsf{p}}^2 = \frac{m-1}{m+n-2} S_X^2 + \frac{n-1}{m+n-2} S_Y^2 \,.
 \tag{4.5}
\end{equation}\]</span>
The pooled estimator is a weighted average that adjusts for differences between the sample sizes <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>.<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a></p>

<div class="proposition">
<span id="prp:ci-compare-means-normpops-pooled" class="proposition"><strong>Proposition 4.8  </strong></span>A <span class="math inline">\(100(1-\alpha)\%\)</span> CI for <span class="math inline">\(\mu_X - \mu_Y\)</span> for two samples of size <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> from normal populations where the variance <span class="math inline">\(\sigma^2\)</span> is unknown is given by
<span class="math display">\[\begin{equation*}
 (\overline{x} - \overline{y}) \pm t_{\alpha/2, m  + n - 2} \cdot \sqrt{ s_{\mathsf{p}}^2 \left( \frac{1}{m} + \frac{1}{n} \right)} \,,
\end{equation*}\]</span>
where we recall that <span class="math inline">\(t_{\alpha/2, m+n-2}\)</span> is the <span class="math inline">\(\alpha/2\)</span> critical value of the <span class="math inline">\(\mathsf{t}(\nu)\)</span> with <span class="math inline">\(\nu = m + n - 2\)</span> df.
</div>
<p>Similarly, one can consider a pooled <span class="math inline">\(\mathsf{t}\)</span> test, i.e., a hypothesis test based on the pooled estimator for the variance as opposed to the two-sample <span class="math inline">\(\mathsf{t}\)</span> test in Proposition <a href="inference-two-samples.html#prp:htest-compare-means-normpops-vars-unknown">4.7</a>. In the case of a pooled <span class="math inline">\(\mathsf{t}\)</span> test, the test statistic
<span class="math display">\[\begin{equation*}
 T = \frac{\overline{X} - \overline{Y} - \theta_0}{\sqrt{S_{\mathsf{p}}^2 \left(\frac{1}{m} + \frac{1}{n}\right)}}\,,
\end{equation*}\]</span>
with the pooled estimator of the variance, replaces <a href="inference-two-samples.html#eq:htest-compare-means-normpops-vars-unknown-statistic">(4.4)</a> in Proposition <a href="inference-two-samples.html#prp:htest-compare-means-normpops-vars-unknown">4.7</a> and the same procedures are followed for determining the <span class="math inline">\(P\)</span>-value with <span class="math inline">\(\nu = m+n-2\)</span> in place of <a href="inference-two-samples.html#eq:dist-t-compare-means-normpops-nu">(4.3)</a>. If you have reasons to believe that <span class="math inline">\(\sigma_X^2 = \sigma_Y^2\)</span>, these pooled <span class="math inline">\(\mathsf{t}\)</span> procedures are appealing because <span class="math inline">\(\nu\)</span> is very easy to compute.</p>
<div class="warningblock">
<p>Pooled <span class="math inline">\(t\)</span> procedures are not robust if the assumption of equalized variance is violated. Theoretically, you could first carry out a statistical test <span class="math inline">\(H_0 : \sigma_X^2 = \sigma_Y^2\)</span> on the equality of variances and then use a pooled <span class="math inline">\(\mathsf{t}\)</span> procedure if the null hypothesis is not rejected. However, there is no free lunch: the typical <span class="math inline">\(\mathsf{F}\)</span> test for equal variances (see Section <a href="inference-two-samples.html#compare-variances">4.4</a>) is sensitive to normality assumptions. The two sample <span class="math inline">\(\mathsf{t}\)</span> procedures, with the data-driven choice of <span class="math inline">\(\nu\)</span> in <a href="inference-two-samples.html#eq:dist-t-compare-means-normpops-nu">(4.3)</a>, are therefore recommended unless, of course, you have a very compelling reason to believe <span class="math inline">\(\sigma_X^2 = \sigma_Y^2\)</span>.</p>
</div>
</div>
</div>
<div id="compare-paired-samples" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Comparing paired samples</h2>
<p>The preceding analysis for comparing population means was based on the assumption that a random sample <span class="math inline">\(X_1, \dots, X_m\)</span> is drawn from a distribution with mean <span class="math inline">\(\mu_X\)</span> and that a completely independent random sample <span class="math inline">\(Y_1, \dots, Y_n\)</span> is drawn from a distribution with mean <span class="math inline">\(\mu_Y\)</span>. Some situations, e.g., comparing observations before and after a treatment or exposure, necessitate the consideration of paired values.</p>
<p>Consider a random sample of iid pairs
<span class="math display">\[\begin{equation*}
(X_1, Y_1), \dots, (X_n, Y_n)
\end{equation*}\]</span>
with <span class="math inline">\(\E[X_i] = \mu_X\)</span> and <span class="math inline">\(\E[Y_i] = \mu_Y\)</span>. If we are interested in making inferences about the difference <span class="math inline">\(\mu_X - \mu_Y\)</span> then the paired differences
<span class="math display">\[\begin{equation*}
D_i = X_i - Y_i \,,\quad  i=1, \dots, n\,,
\end{equation*}\]</span>
constitute a sample with mean <span class="math inline">\(\mu_D = \mu_X - \mu_Y\)</span> that can be treated using single-sample CIs and tests, e.g., see Section <a href="inference-single-sample.html#mean-normal-var-unknown">3.1.3</a>.</p>
</div>
<div id="compare-proportions" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Comparing proportions</h2>
<p>Consider a population containing a proportion <span class="math inline">\(p_X\)</span> of individuals satisfying a given property. For a sample of size <span class="math inline">\(m\)</span> from this population, we denote the sample proportion by <span class="math inline">\(\widehat{p}_X\)</span>. Likewise, we consider a population containing a proportion <span class="math inline">\(p_Y\)</span> of individuals satisfying the same given property. For a sample of size <span class="math inline">\(n\)</span> from this population, we denote the sample proportion by <span class="math inline">\(\widehat{p}_Y\)</span>. We assume the samples from the <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> populations are independent. The natural estimator for the difference in population proportions <span class="math inline">\(p_X - p_Y\)</span> is the difference in the sample proportions <span class="math inline">\(\widehat{p}_X - \widehat{p}_Y\)</span>.</p>
<p>Provided the samples are much smaller than the population sizes (i.e., the populations are about <span class="math inline">\(20\)</span> times larger than the samples),
<span class="math display">\[\begin{equation*}
 \mu_{(\widehat{p}_X - \widehat{p}_Y)} = \E[\widehat{p}_X - \widehat{p}_Y] = p_X - p_Y\,,
\end{equation*}\]</span>
and
<span class="math display">\[\begin{equation*}
 \sigma_{(\widehat{p}_X - \widehat{p}_Y)}^2 = \Var[\widehat{p}_X - \widehat{p}_Y] 
 = \frac{p_X(1-p_X)}{m} + \frac{p_Y(1-p_Y)}{n}\,,
\end{equation*}\]</span>
by considering the fact that the count of individuals satisfying the given property in each population will be independent draws from <span class="math inline">\(\mathsf{Binom}(m, p_X)\)</span> and <span class="math inline">\(\mathsf{Binom}(n, p_Y)\)</span>, respectively. Further, if <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are large (e.g., <span class="math inline">\(m \geq 30\)</span> and <span class="math inline">\(n \geq 30\)</span>), then <span class="math inline">\(\widehat{p}_X\)</span> and <span class="math inline">\(\widehat{p}_Y\)</span> are (approximately) normally distributed. Standardizing <span class="math inline">\(\widehat{p}_X - \widehat{p}_Y\)</span>,
<span class="math display">\[\begin{equation*}
 Z = \frac{\widehat{p}_X - \widehat{p}_Y - (p_X - p_Y)}{\sqrt{\frac{p_X(1-p_X)}{m} + \frac{p_Y(1-p_Y)}{n}}}
 \quad \sim \mathsf{N}(0,1)\,.
\end{equation*}\]</span>
A CI for <span class="math inline">\(\widehat{p}_X - \widehat{p}_Y\)</span> then follows from the large-sample CI considered in Section <a href="inference-single-sample.html#mean-large-sample">3.1.2</a>.</p>

<div class="proposition">
<span id="prp:ci-compare-proportions" class="proposition"><strong>Proposition 4.9  </strong></span>An approximate <span class="math inline">\(100(1-\alpha)\%\)</span> CI for <span class="math inline">\(p_X - p_Y\)</span> is given by
<span class="math display" id="eq:ci-compare-proportions">\[\begin{equation*}
 \widehat{p}_X - \widehat{p}_Y \pm z_{\alpha/2}\sqrt{\frac{\widehat{p}_X (1 - \widehat{p}_X)}{m} + \frac{\widehat{p}_Y (1 - \widehat{p}_Y)}{n}}\,,
 \tag{4.6}
\end{equation*}\]</span>
and, as a rule of thumb, can be reliably used if <span class="math inline">\(m \widehat{p}_X\)</span>, <span class="math inline">\(m (1 - \widehat{p}_X)\)</span>, <span class="math inline">\(n \widehat{p}_Y\)</span>, and <span class="math inline">\(n (1-\widehat{p}_Y)\)</span> are greater than or equal to <span class="math inline">\(10\)</span>.
</div>
<p>Proposition <a href="inference-two-samples.html#prp:ci-compare-proportions">4.9</a> does not pool the estimators for the population proportions. However, if we are considering a hypothesis test concerning the equality of the population proportions with the null hypothesis
<span class="math display">\[\begin{equation*}
H_0 : p_X - p_Y = 0 \,,
\end{equation*}\]</span>
then we assume <span class="math inline">\(p_X = p_Y\)</span> as our default position. Therefore, as a matter of consistency, we should replace the standard error in <a href="inference-two-samples.html#eq:ci-compare-proportions">(4.6)</a> with a pooled estimator for the standard error of the population proportion,
<span class="math display">\[\begin{equation*}
 \widehat{p} = \frac{m}{m + n} \widehat{p}_X + \frac{n}{m + n} \widehat{p}_Y \,.
\end{equation*}\]</span></p>

<div class="proposition">
<p><span id="prp:htest-compare-proportions" class="proposition"><strong>Proposition 4.10  </strong></span>Assume that <span class="math inline">\(m \widehat{p}_X\)</span>, <span class="math inline">\(m (1-\widehat{p}_X)\)</span>, <span class="math inline">\(n\widehat{p}_Y\)</span>, <span class="math inline">\(n(1-\widehat{p}_Y)\)</span> are all greater than <span class="math inline">\(10\)</span>.</p>
<p>Consider <span class="math inline">\(H_0 : p_X - p_Y = 0\)</span>. The test statistic is
<span class="math display">\[\begin{equation*}
 Z = \frac{\widehat{p}_X - \widehat{p}_Y}{\sqrt{\widehat{p} (1 - \widehat{p}) \left( \frac{1}{m} + \frac{1}{n} \right)}} \,.
\end{equation*}\]</span></p>
<p>For a hypothesis test at level <span class="math inline">\(\alpha\)</span>, we use the following procedure:</p>
<p>If <span class="math inline">\(H_a : p_X - p_Y &gt; 0\)</span>, then <span class="math inline">\(P = 1 - \Phi(z)\)</span>, i.e., upper-tail <span class="math inline">\(R = \{z &gt; z_{\alpha}\}\)</span>.</p>
<p>If <span class="math inline">\(H_a : p_X - p_Y &lt; 0\)</span>, then <span class="math inline">\(P = \Phi(z)\)</span>, i.e., lower-tail <span class="math inline">\(R = \{z &lt; - z_{\alpha}\}\)</span>.</p>
If <span class="math inline">\(H_a : p_X - p_Y \neq 0\)</span>, then <span class="math inline">\(P = 2(1-\Phi(|z|))\)</span>, i.e., two-tailed <span class="math inline">\(R = \{|z| &gt; z_{\alpha/2}\}\)</span>.
</div>
</div>
<div id="compare-variances" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Comparing variances</h2>
<p>For a random sample
<span class="math display">\[\begin{equation*}
X_1, \dots, X_m \sim \mathsf{N}(\mu_X, \sigma_X^2)
\end{equation*}\]</span>
and an independent random sample
<span class="math display">\[\begin{equation*}
Y_1, \dots, Y_n \sim \mathsf{N}(\mu_Y, \sigma_Y^2)\,,
\end{equation*}\]</span>
the rv
<span class="math display" id="eq:F-test-statistic">\[\begin{equation}
 F = \frac{S_X^2 / \sigma_X^2}{S_Y^2 / \sigma_Y^2} \quad \sim \mathsf{F}(m-1, n-1)\,,
 \tag{4.7}
\end{equation}\]</span>
that is, <span class="math inline">\(F\)</span> has an <span class="math inline">\(\mathsf{F}\)</span> distribution with df <span class="math inline">\(\nu_1 = m-1\)</span> and <span class="math inline">\(\nu_2 = n-1\)</span>. The statistic <span class="math inline">\(F\)</span> in <a href="inference-two-samples.html#eq:F-test-statistic">(4.7)</a> comprises the <em>ratio</em> of variances <span class="math inline">\(\sigma_X^2 / \sigma_Y^2\)</span> and not the difference; therefore, the plausibility of <span class="math inline">\(\sigma_X^2 = \sigma_Y^2\)</span> will be based on how much the ratio differs from <span class="math inline">\(1\)</span>.</p>

<div class="proposition">
<span id="prp:htest-compare-variances" class="proposition"><strong>Proposition 4.11  </strong></span>For the null hypothesis <span class="math inline">\(H_0 : \sigma_X^2 = \sigma_Y^2\)</span>, the test statistic to consider is:
<span class="math display">\[\begin{equation*}
f = \frac{s_X^2}{s_Y^2}
\end{equation*}\]</span>
and the <span class="math inline">\(P\)</span>-values are determined by the <span class="math inline">\(\mathsf{F}(m-1, n-1)\)</span> curve where <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> are the respective sample sizes.
</div>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> CI for the ratio <span class="math inline">\(\sigma_X^2 / \sigma_Y^2\)</span> is based on forming the probability,
<span class="math display">\[\begin{equation*}
 P(F_{1-\alpha/2, \nu_1, \nu_2} &lt; F &lt; F_{\alpha/2, \nu_1, \nu_2}) = 1 - \alpha\,,
\end{equation*}\]</span>
where <span class="math inline">\(F_{\alpha/2, \nu_1, \nu_2}\)</span> is the <span class="math inline">\(\alpha/2\)</span> critical value from the <span class="math inline">\(\mathsf{F}(\nu_1 = m-1, \nu_2 = n-1)\)</span> distribution. Substituting <a href="inference-two-samples.html#eq:F-test-statistic">(4.7)</a> with point estimates for <span class="math inline">\(F\)</span> and manipulating the inequalities it is possible to isolate the ratio <span class="math inline">\(\sigma_X^2 / \sigma_Y^2\)</span>,
<span class="math display">\[\begin{equation*}
 P \left( \frac{1}{F_{\alpha/2, \nu_1, \nu_2}} \frac{s_X^2}{s_Y^2} &lt; \frac{\sigma_X^2}{\sigma_Y^2} &lt; \frac{1}{F_{1-\alpha/2, \nu_1, \nu_2}} \frac{s_X^2}{s_Y^2} \right) 
 = 1 - \alpha \,.
\end{equation*}\]</span></p>

<div class="proposition">
<span id="prp:ci-compre-variances" class="proposition"><strong>Proposition 4.12  </strong></span>A <span class="math inline">\(100(1-\alpha)\%\)</span> CI for the ratio of population variances <span class="math inline">\(\sigma_X^2 / \sigma_Y^2\)</span> is given by
<span class="math display">\[\begin{equation*}
 \left(F_{\alpha/2, m-1, n-1}^{-1} s_X^2 / s_Y^2 \,,  F_{1-\alpha/2, m-1, n-1}^{-1} s_X^2 / s_Y^2 \right)\,.
\end{equation*}\]</span>
</div>

<div class="proposition">
<p><span id="prp:htest-compre-variances" class="proposition"><strong>Proposition 4.13  </strong></span>Assume the population distributions are normal and the random samples are both independent of one another.</p>
<p>Consider <span class="math inline">\(H_0 : \sigma_X^2 = \sigma_Y^2\)</span>. The test statistic is
<span class="math display">\[\begin{equation*}
 F = S_X^2 / S_Y^2 \,.
\end{equation*}\]</span></p>
<p>For a hypothesis test at level <span class="math inline">\(\alpha\)</span>, we use the following procedure:</p>
<p>If <span class="math inline">\(H_a : \sigma_X^2 &gt; \sigma_Y^2\)</span>, then <span class="math inline">\(P\)</span>-value is <span class="math inline">\(A_R = {}\)</span> area under the <span class="math inline">\(\mathsf{F}(m-1, n-1)\)</span> curve to the right of <span class="math inline">\(f\)</span>.</p>
<p>If <span class="math inline">\(H_a : \sigma_X^2 &lt; \sigma_Y^2\)</span>, then <span class="math inline">\(P\)</span>-value is <span class="math inline">\(A_L = {}\)</span> area under the <span class="math inline">\(\mathsf{F}(m-1, n-1)\)</span> curve to the left of <span class="math inline">\(f\)</span>.</p>
If <span class="math inline">\(H_a : \sigma_X^2 \neq \sigma_Y^2\)</span>, then <span class="math inline">\(P\)</span>-value is <span class="math inline">\(2 \cdot \min(A_R, A_L)\)</span>.
</div>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="23">
<li id="fn23"><p>If <span class="math inline">\(m \neq n\)</span>, then the estimator with <em>more</em> samples will contain <em>more</em> information about the parameter <span class="math inline">\(\sigma^2\)</span>. Thus, the simple average <span class="math inline">\((S_X^2 + S_Y^2)/2\)</span> wouldn’t really be fair, would it?<a href="inference-two-samples.html#fnref23" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inference-single-sample.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="anova.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dundeemath/MA22004/edit/master/04-infer-two-samples.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ma22004.pdf"],
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true,
"highlight": "pygments",
"number_sections": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
