<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Linear regression | MA22004 - Statistics II</title>
  <meta name="description" content="Module notes. Mathematics Division, School of Science &amp; Engineering, University of Dundee." />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Linear regression | MA22004 - Statistics II" />
  <meta property="og:type" content="book" />
  <meta property="og:image" content="https://dundeemath.github.io/MA22004/assets/images/cover_small.jpg" />
  <meta property="og:description" content="Module notes. Mathematics Division, School of Science &amp; Engineering, University of Dundee." />
  <meta name="github-repo" content="dundeemath/MA22004" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Linear regression | MA22004 - Statistics II" />
  
  <meta name="twitter:description" content="Module notes. Mathematics Division, School of Science &amp; Engineering, University of Dundee." />
  <meta name="twitter:image" content="https://dundeemath.github.io/MA22004/assets/images/cover_small.jpg" />

<meta name="author" content="Dr Eric Hall" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="anova.html"/>
<link rel="next" href="categorical-data.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/font-awesome-6.4.0/css/all.min.css" rel="stylesheet" />
<link href="libs/font-awesome-6.4.0/css/v4-shims.min.css" rel="stylesheet" />
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="assets/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">MA22004</a></li>

<li class="divider"></li>
<li class="part"><span><b>Module Documents</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#about-your-instructor"><i class="fa fa-check"></i>About your instructor</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="labs.html"><a href="labs.html"><i class="fa fa-check"></i>Lab Guide</a></li>
<li class="chapter" data-level="" data-path="writing.html"><a href="writing.html"><i class="fa fa-check"></i>Writing Lab Reports</a>
<ul>
<li class="chapter" data-level="" data-path="writing.html"><a href="writing.html#w-assess"><i class="fa fa-check"></i>Assessment Criteria</a></li>
<li class="chapter" data-level="" data-path="writing.html"><a href="writing.html#w-content"><i class="fa fa-check"></i>Content</a></li>
<li class="chapter" data-level="" data-path="writing.html"><a href="writing.html#w-present"><i class="fa fa-check"></i>Presentation</a>
<ul>
<li class="chapter" data-level="" data-path="writing.html"><a href="writing.html#w-plots"><i class="fa fa-check"></i>Plots</a></li>
<li class="chapter" data-level="" data-path="writing.html"><a href="writing.html#w-math"><i class="fa fa-check"></i>Mathematical formulas</a></li>
<li class="chapter" data-level="" data-path="writing.html"><a href="writing.html#w-structure"><i class="fa fa-check"></i>Structure</a></li>
<li class="chapter" data-level="" data-path="writing.html"><a href="writing.html#w-eng"><i class="fa fa-check"></i>Writing</a></li>
<li class="chapter" data-level="" data-path="writing.html"><a href="writing.html#w-format"><i class="fa fa-check"></i>Formatting</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Lecture Notes</b></span></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i>Preliminaries</a>
<ul>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#notation"><i class="fa fa-check"></i>Notation</a></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#abbreviations"><i class="fa fa-check"></i>Abbreviations</a></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#sample-space-events-probabilities"><i class="fa fa-check"></i>Sample space, events, probabilities</a></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#random-variables"><i class="fa fa-check"></i>Random variables</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="sampling-distributions.html"><a href="sampling-distributions.html"><i class="fa fa-check"></i><b>1</b> Sampling distributions</a>
<ul>
<li class="chapter" data-level="1.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>1.1</b> Uniform distribution</a></li>
<li class="chapter" data-level="1.2" data-path="sampling-distributions.html"><a href="sampling-distributions.html#normal-distribution"><i class="fa fa-check"></i><b>1.2</b> Normal distribution</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#facts-normals"><i class="fa fa-check"></i><b>1.2.1</b> Some useful facts about normal variates</a></li>
<li class="chapter" data-level="1.2.2" data-path="sampling-distributions.html"><a href="sampling-distributions.html#empirical-rule-68-95-99.7-rule"><i class="fa fa-check"></i><b>1.2.2</b> Empirical rule (<span class="math inline">\(68-95-99.7\)</span> rule)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="sampling-distributions.html"><a href="sampling-distributions.html#t-distribution"><i class="fa fa-check"></i><b>1.3</b> <span class="math inline">\(\mathsf{t}\)</span> and Cauchy distribution</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="sampling-distributions.html"><a href="sampling-distributions.html#facts-t"><i class="fa fa-check"></i><b>1.3.1</b> Properties of <span class="math inline">\(\mathsf{t}\)</span> distributions</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="sampling-distributions.html"><a href="sampling-distributions.html#chisq-distribution"><i class="fa fa-check"></i><b>1.4</b> <span class="math inline">\(\chi^2\)</span> distribution</a></li>
<li class="chapter" data-level="1.5" data-path="sampling-distributions.html"><a href="sampling-distributions.html#F-distribution"><i class="fa fa-check"></i><b>1.5</b> <span class="math inline">\(\mathsf{F}\)</span> distribution</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>2</b> Basics of statistical inference</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-inference.html"><a href="statistical-inference.html#point-estimation"><i class="fa fa-check"></i><b>2.1</b> Point estimation</a></li>
<li class="chapter" data-level="2.2" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>2.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="2.3" data-path="statistical-inference.html"><a href="statistical-inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>2.3</b> Hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="inference-single-sample.html"><a href="inference-single-sample.html"><i class="fa fa-check"></i><b>3</b> Inferences based on a single sample</a>
<ul>
<li class="chapter" data-level="3.1" data-path="inference-single-sample.html"><a href="inference-single-sample.html#estimating-means"><i class="fa fa-check"></i><b>3.1</b> Estimating means</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="inference-single-sample.html"><a href="inference-single-sample.html#mean-normal-var-known"><i class="fa fa-check"></i><b>3.1.1</b> Mean of a normal population with known variance</a></li>
<li class="chapter" data-level="3.1.2" data-path="inference-single-sample.html"><a href="inference-single-sample.html#mean-large-sample"><i class="fa fa-check"></i><b>3.1.2</b> Mean of a population with unknown variance (large-sample)</a></li>
<li class="chapter" data-level="3.1.3" data-path="inference-single-sample.html"><a href="inference-single-sample.html#mean-normal-var-unknown"><i class="fa fa-check"></i><b>3.1.3</b> Mean of a normal population with unknown variance</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="inference-single-sample.html"><a href="inference-single-sample.html#estimating-proportions"><i class="fa fa-check"></i><b>3.2</b> Estimating proportions</a></li>
<li class="chapter" data-level="3.3" data-path="inference-single-sample.html"><a href="inference-single-sample.html#estimating-variances"><i class="fa fa-check"></i><b>3.3</b> Estimating variances</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inference-two-samples.html"><a href="inference-two-samples.html"><i class="fa fa-check"></i><b>4</b> Inferences based on two samples</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-means"><i class="fa fa-check"></i><b>4.1</b> Comparing means</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-means-normpops-vars-known"><i class="fa fa-check"></i><b>4.1.1</b> Comparing means of normal populations when variances are known</a></li>
<li class="chapter" data-level="4.1.2" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-means-large-samples"><i class="fa fa-check"></i><b>4.1.2</b> Comparing means when the sample sizes are large</a></li>
<li class="chapter" data-level="4.1.3" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-means-normpops-vars-unknown"><i class="fa fa-check"></i><b>4.1.3</b> Comparing means of normal populations when variances are unknown and the sample size is small</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-paired-samples"><i class="fa fa-check"></i><b>4.2</b> Comparing paired samples</a></li>
<li class="chapter" data-level="4.3" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-proportions"><i class="fa fa-check"></i><b>4.3</b> Comparing proportions</a></li>
<li class="chapter" data-level="4.4" data-path="inference-two-samples.html"><a href="inference-two-samples.html#compare-variances"><i class="fa fa-check"></i><b>4.4</b> Comparing variances</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>5</b> Analysis of variance (ANOVA)</a>
<ul>
<li class="chapter" data-level="5.1" data-path="anova.html"><a href="anova.html#anova-single-factor-test"><i class="fa fa-check"></i><b>5.1</b> Single factor ANOVA test</a></li>
<li class="chapter" data-level="5.2" data-path="anova.html"><a href="anova.html#anova-ci"><i class="fa fa-check"></i><b>5.2</b> Confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>6</b> Linear regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-models.html"><a href="linear-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.1</b> Simple linear regression models</a></li>
<li class="chapter" data-level="6.2" data-path="linear-models.html"><a href="linear-models.html#ls-estimate-var"><i class="fa fa-check"></i><b>6.2</b> Estimating <span class="math inline">\(\sigma^2\)</span> for linear regressions</a></li>
<li class="chapter" data-level="6.3" data-path="linear-models.html"><a href="linear-models.html#inference-ls"><i class="fa fa-check"></i><b>6.3</b> Inferences for least-squares parameters</a></li>
<li class="chapter" data-level="6.4" data-path="linear-models.html"><a href="linear-models.html#correlation"><i class="fa fa-check"></i><b>6.4</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="categorical-data.html"><a href="categorical-data.html"><i class="fa fa-check"></i><b>7</b> Categorical data</a>
<ul>
<li class="chapter" data-level="7.1" data-path="categorical-data.html"><a href="categorical-data.html#multinomial-experiments"><i class="fa fa-check"></i><b>7.1</b> Multinomial experiments</a></li>
<li class="chapter" data-level="7.2" data-path="categorical-data.html"><a href="categorical-data.html#goodness-of-fit-tests"><i class="fa fa-check"></i><b>7.2</b> Goodness-of-fit for a single factor</a></li>
<li class="chapter" data-level="7.3" data-path="categorical-data.html"><a href="categorical-data.html#test-for-the-independence-of-factors"><i class="fa fa-check"></i><b>7.3</b> Test for the independence of factors</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="quality-control.html"><a href="quality-control.html"><i class="fa fa-check"></i><b>8</b> Quality control</a>
<ul>
<li class="chapter" data-level="8.1" data-path="quality-control.html"><a href="quality-control.html#control-charts"><i class="fa fa-check"></i><b>8.1</b> Control charts</a></li>
</ul></li>
<li class="part"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html"><i class="fa fa-check"></i>Curated Content</a>
<ul>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-0"><i class="fa fa-check"></i>Investigation 0</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-1"><i class="fa fa-check"></i>Investigation 1</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-2"><i class="fa fa-check"></i>Investigation 2</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-3"><i class="fa fa-check"></i>Investigation 3</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-4"><i class="fa fa-check"></i>Investigation 4</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-5"><i class="fa fa-check"></i>Investigation 5</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-6"><i class="fa fa-check"></i>Investigation 6</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-7"><i class="fa fa-check"></i>Investigation 7</a></li>
<li class="chapter" data-level="" data-path="curated-content.html"><a href="curated-content.html#investigation-8"><i class="fa fa-check"></i>Investigation 8</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="http://uod.ac.uk/sig-home" rel="nofollow"><img width="73" height="73" src="https://www.dundee.ac.uk/media/dundeewebsite/themes/brandnewhope/img/university-of-dundee-email-favicon.png" alt="University of Dundee shield logo"> </a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MA22004 - Statistics II</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<span class="math inline">\(\DeclareMathOperator{\E}{\mathbf{E}}\)</span>
<span class="math inline">\(\DeclareMathOperator{\Var}{Var}\)</span>
<span class="math inline">\(\DeclareMathOperator{\Cov}{Cov}\)</span>
<span class="math inline">\(\DeclareMathOperator{\corr}{corr}\)</span>
<span class="math inline">\(\newcommand{\se}{\mathsf{se}}\)</span>
<span class="math inline">\(\DeclareMathOperator{\sd}{sd}\)</span>
<div id="linear-models" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Linear regression<a href="linear-models.html#linear-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Regression analysis</strong> allows us to study the relationship among two or more rvs. Typically, we are interested in the relationship between a <strong>response</strong> or <strong>dependent</strong> rv <span class="math inline">\(Y\)</span> and a <strong>covariate</strong> <span class="math inline">\(X\)</span>.<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> The relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> will be explained through a <strong>regression function</strong>,
<span class="math display">\[\begin{equation*}
r(x) = \E[Y \mid X = x] = \int y f(y\mid x) dy \,.
\end{equation*}\]</span>
In particular, we shall assume that <span class="math inline">\(r\)</span> is linear,
<span class="math display" id="eq:linear-regression-function">\[\begin{equation}
r(x) = \beta_0 + \beta_1 x\,,
\tag{6.1}
\end{equation}\]</span>
and estimate the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span> of this linear model from sample data
<span class="math display">\[\begin{equation*}
(Y_1, X_1), \dots, (Y_m, X_m) \sim F_{Y,X}\,.
\end{equation*}\]</span></p>
<div id="simple-linear-regression" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Simple linear regression models<a href="linear-models.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The simplest regression is when <span class="math inline">\(X_i\)</span> is one-dimensional and <span class="math inline">\(r(x)\)</span> is linear as in <a href="linear-models.html#eq:linear-regression-function">(6.1)</a>. A linear regression posits the expected value of <span class="math inline">\(Y_i\)</span> is a linear function of the data <span class="math inline">\(X_i\)</span>, but that <span class="math inline">\(Y\)</span> deviates from its expected value by a random amount for fixed <span class="math inline">\(x_i\)</span>.</p>
<div class="definition">
<p><span id="def:linear-model" class="definition"><strong>Definition 6.1  </strong></span>The <strong>simple linear regression model</strong> relates a random response <span class="math inline">\(Y_i\)</span> to a set of independent variables <span class="math inline">\(X_i\)</span>,
<span class="math display" id="eq:linear-model">\[\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \,,
\tag{6.2}
\end{equation}\]</span>
where the intercept <span class="math inline">\(\beta_0\)</span> and slope <span class="math inline">\(\beta_1\)</span> are unknown parameters and the <strong>random deviation</strong> or <strong>random error</strong> <span class="math inline">\(\epsilon_i\)</span> is a rv assumed to satisfy:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\E[\epsilon_i \mid X_i = x_i] = 0\)</span>,</li>
<li><span class="math inline">\(\Var[\epsilon_i \mid X_i = x_i] = \sigma^2\)</span> does not depend on <span class="math inline">\(x_i\)</span>,</li>
<li><span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(\epsilon_j\)</span> are independent for <span class="math inline">\(i, j = 1, \dots, m\)</span>.</li>
</ol>
</div>
<p>From the assumptions on <span class="math inline">\(\epsilon_i\)</span>, the linear model <a href="linear-models.html#eq:linear-model">(6.2)</a> implies
<span class="math display">\[\begin{equation*}
\E[Y_i \mid X_i = x_i] = \beta_0 + \beta_1 x_i \,.
\end{equation*}\]</span>
Thus, if <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> are estimators of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, then the <strong>fitted line</strong> is
<span class="math display">\[\begin{equation*}
\widehat{r}(x) = \widehat{\beta}_0 + \widehat{\beta}_1 x
\end{equation*}\]</span>
and the <strong>predicted</strong> or <strong>fitted value</strong> <span class="math inline">\(\widehat{Y}_i = \widehat{r}(X_i)\)</span> is an estimator for <span class="math inline">\(\E[Y_i \mid X_i = x_i]\)</span>. The <strong>residuals</strong> are defined to be
<span class="math display" id="eq:ls-residuals">\[\begin{equation}
\widehat{\epsilon}_i = Y_i - \widehat{Y}_i = Y_i - \left( \widehat{\beta}_0 + \widehat{\beta}_1 X_i \right) \,.
\tag{6.3}
\end{equation}\]</span>
The <strong>residual sums of squares</strong>,<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a>
<span class="math display" id="eq:rss">\[\begin{equation}
\mathsf{RSS} = \sum_{i=1}^m \widehat{\epsilon}_i^2\,,
\tag{6.4}
\end{equation}\]</span>
measures how well the regression line <span class="math inline">\(\widehat{r}\)</span> fits the data <span class="math inline">\((Y_1, X_1), \dots, (Y_m, X_m)\)</span>. The <strong>least squares estimates</strong> of <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> are the values that minimize the <span class="math inline">\(\mathsf{RSS}\)</span> in <a href="linear-models.html#eq:rss">(6.4)</a>.</p>
<div class="theorem">
<p><span id="thm:least-squares-estimates" class="theorem"><strong>Theorem 6.1  </strong></span>The <strong>least squares estimates</strong> for <span class="math inline">\(\widehat{\beta}_1\)</span> and <span class="math inline">\(\widehat{\beta}_0\)</span> are given by, respectively,
<span class="math display" id="eq:ls-slope">\[\begin{equation}
\widehat{\beta}_1 = \frac{\sum_{i=1}^m (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^m (X_i - \overline{X})^2} = \frac{S_{xy}}{S_{xx}} \,,
\tag{6.5}
\end{equation}\]</span>
and
<span class="math display" id="eq:ls-intercept">\[\begin{equation}
\widehat{\beta}_0 = \overline{Y} - \widehat{\beta}_1 \overline{X}\,.
\tag{6.6}
\end{equation}\]</span></p>
</div>
<p>Equation <a href="linear-models.html#eq:rss">(6.4)</a> is a function of <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> from the definition of the residuals <a href="linear-models.html#eq:ls-residuals">(6.3)</a>. Then <a href="linear-models.html#eq:ls-slope">(6.5)</a> and <a href="linear-models.html#eq:ls-intercept">(6.6)</a> follow by equating the partial derivatives of <a href="linear-models.html#eq:rss">(6.4)</a> to zero. The <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> are the unique solution to this linear system.</p>
<div class="example">
<p><span id="exm:linear-model-fit-residuals" class="example"><strong>Example 6.1  </strong></span>In Figures <a href="linear-models.html#fig:linear-model-fit">6.1</a> and <a href="linear-models.html#fig:linear-model-residuals">6.2</a>, we consider the <strong>Cherry Tree Data</strong> (see Table <a href="statistical-inference.html#tab:cherry-data">2.1</a> and discussion). We fit a least squares regression of timber volume (response variable) to the tree’s diameter (independent variable). As you would expect, the timber yield increases with diameter.</p>
</div>
<p>The <code>r</code> code below can be used to calculate the least squares regression and residuals.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="linear-models.html#cb13-1" tabindex="-1"></a><span class="fu">data</span>(trees)</span>
<span id="cb13-2"><a href="linear-models.html#cb13-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> trees<span class="sc">$</span>Volume</span>
<span id="cb13-3"><a href="linear-models.html#cb13-3" tabindex="-1"></a>x <span class="ot">&lt;-</span> trees<span class="sc">$</span>Girth <span class="co"># NB: this is the diameter; data mislabeled!</span></span>
<span id="cb13-4"><a href="linear-models.html#cb13-4" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb13-5"><a href="linear-models.html#cb13-5" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">resid</span>(fit)</span>
<span id="cb13-6"><a href="linear-models.html#cb13-6" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit)</span></code></pre></div>
<p>The <code>fit</code> data frame contains the estimates for <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="linear-models.html#cb14-1" tabindex="-1"></a>fit<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>(Intercept)           x 
 -36.943459    5.065856 </code></pre>
<p>Both Figures <a href="linear-models.html#fig:linear-model-fit">6.1</a> and <a href="linear-models.html#fig:linear-model-residuals">6.2</a> are scatter plots of the observed values <span class="math inline">\(y\)</span>. In Figure <a href="linear-models.html#fig:linear-model-fit">6.1</a>, the regression line <span class="math inline">\(\widehat{y}\)</span> is plotted along with the residuals <span class="math inline">\(\widehat{\epsilon}\)</span>. In Figure <a href="linear-models.html#fig:linear-model-residuals">6.2</a>, the sample mean <span class="math inline">\(\overline{y}\)</span> is plotted together with the deviations <span class="math inline">\(y - \overline{y}\)</span>. <span class="math inline">\(\lozenge\)</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linear-model-fit"></span>
<img src="ma22004_files/figure-html/linear-model-fit-1.svg" alt="Linear regression (or least squares fit) of Volume to Diameter from the **Cherry Tree Data**. The vertical bars between the observed data point and the regression line indicate the error in the fit (the least squares residual). The residuals are squared and summed to yield the $\mathsf{RSS}$ (alt: $\mathsf{SSE}$)." width="768" />
<p class="caption">
Figure 6.1: Linear regression (or least squares fit) of Volume to Diameter from the <strong>Cherry Tree Data</strong>. The vertical bars between the observed data point and the regression line indicate the error in the fit (the least squares residual). The residuals are squared and summed to yield the <span class="math inline">\(\mathsf{RSS}\)</span> (alt: <span class="math inline">\(\mathsf{SSE}\)</span>).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linear-model-residuals"></span>
<img src="ma22004_files/figure-html/linear-model-residuals-1.svg" alt="The deviations about the sample mean $\overline{y}$. The sum of the squared deviations or $\mathsf{SST}$ (total sum of squares) is a measure of the total variation in the observations." width="768" />
<p class="caption">
Figure 6.2: The deviations about the sample mean <span class="math inline">\(\overline{y}\)</span>. The sum of the squared deviations or <span class="math inline">\(\mathsf{SST}\)</span> (total sum of squares) is a measure of the total variation in the observations.
</p>
</div>
</div>
<div id="ls-estimate-var" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Estimating <span class="math inline">\(\sigma^2\)</span> for linear regressions<a href="linear-models.html#ls-estimate-var" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The parameter <span class="math inline">\(\sigma^2\)</span> (the variance of the random deviation) determines the variability in the regression model.</p>
<div class="theorem">
<p><span id="thm:least-squares-var-estimate" class="theorem"><strong>Theorem 6.2  </strong></span>An unbiased estimate of <span class="math inline">\(\sigma^2\)</span> is given by
<span class="math display" id="eq:least-squares-var-estimate">\[\begin{equation}
\widehat{\sigma}^2 = s^2 = \frac{\mathsf{RSS}}{m-2} = \frac{1}{m-2} \sum_{i=1}^m (y_i - \widehat{y}_i)^2\,.
\tag{6.7}
\end{equation}\]</span></p>
</div>
<p>In Figure <a href="linear-models.html#fig:linear-model-sigma-large-v-small">6.3</a>, we present a least squares regression of timber volume on both tree diameter and height (for the <strong>Cherry Tree Data</strong>). As expected, the regressions indicate the volume increases with both covariates. Estimates for the variance of the random deviation <a href="linear-models.html#eq:least-squares-var-estimate">(6.7)</a> in both regression models, <span class="math inline">\(\sigma_{D}^2\)</span> and <span class="math inline">\(\sigma_{H}^2\)</span>, respectively, are computed to be <span class="math inline">\(s^2_{D} = 18.08\)</span> and <span class="math inline">\(s^2_{H} = 179.48\)</span>. Thus, we see that small variances lead to observations of <span class="math inline">\((x_i, y_i)\)</span> that sit tightly around the regression line, in contrast to large variances that lead to a large cloud of points.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linear-model-sigma-large-v-small"></span>
<img src="ma22004_files/figure-html/linear-model-sigma-large-v-small-1.svg" alt="For the **Cherry Tree Data**, we estimate the variance to be $s^2_{D} = 18.08$ (for Diameter) and $s^2_{H} = 179.48$ (for Height); small variances lead to observations of $(x_i, y_i)$ that sit tightly around the regression line, in contrast to large variances that lead to a large cloud of points." width="768" />
<p class="caption">
Figure 6.3: For the <strong>Cherry Tree Data</strong>, we estimate the variance to be <span class="math inline">\(s^2_{D} = 18.08\)</span> (for Diameter) and <span class="math inline">\(s^2_{H} = 179.48\)</span> (for Height); small variances lead to observations of <span class="math inline">\((x_i, y_i)\)</span> that sit tightly around the regression line, in contrast to large variances that lead to a large cloud of points.
</p>
</div>
<div class="noteblock">
<p>In Theorem <a href="linear-models.html#thm:least-squares-var-estimate">6.2</a>, the number in the denominator is the df associated with the <span class="math inline">\(\mathsf{RSS}\)</span> and <span class="math inline">\(s^2\)</span>. To calculate <span class="math inline">\(\mathsf{RSS}\)</span>, you must estimate two parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, which results in the loss of two df. Hence the <span class="math inline">\(m-2\)</span>.</p>
</div>
<p>We note to make inferences, the statistic
<span class="math display">\[\begin{equation*}
S^2 = \frac{\mathsf{RSS}}{m-2}
\end{equation*}\]</span>
is an unbiased estimator or <span class="math inline">\(\sigma^2\)</span> and the random variable
<span class="math display">\[\begin{equation*}
\frac{(m-2) S^2}{\sigma^2} \sim \chi^2(m-2)\,.
\end{equation*}\]</span>
Moreover, the statistic <span class="math inline">\(S^2\)</span> is independent of both <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>.</p>
</div>
<div id="inference-ls" class="section level2 hasAnchor" number="6.3">
<h2><span class="header-section-number">6.3</span> Inferences for least-squares parameters<a href="linear-models.html#inference-ls" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>If <span class="math inline">\(\epsilon_i\)</span> in <a href="linear-models.html#eq:linear-model">(6.2)</a> is assumed to be normally distributed, then we can derive the sampling distributions of the estimators <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span>. Hence, we can use these sampling distributions to make inferences about the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</p>
<p>Provided iid <span class="math inline">\(\epsilon_i \mid X_i \sim \mathsf{N}(0, \sigma^2)\)</span>, the least-squares estimators possess the following properties.</p>
<ol style="list-style-type: decimal">
<li>Both <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> are normally distributed.</li>
<li>Both <span class="math inline">\(\widehat{\beta}_0\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> are unbiased, i.e., <span class="math inline">\(\E[\widehat{\beta}_i] = \beta_i\)</span> for <span class="math inline">\(i = 0,1\)</span>.</li>
<li><span class="math inline">\(\Var[\widehat{\beta}_0] = c_{00} \sigma^2\)</span> where <span class="math inline">\(c_{00} = \sum_{i=1}^m x_i^2 / (m S_{xx})\)</span>.</li>
<li><span class="math inline">\(\Var[\widehat{\beta}_1] = c_{11} \sigma^2\)</span> where <span class="math inline">\(c_{11} = 1/S_{xx}\)</span>.</li>
<li><span class="math inline">\(\Cov[\widehat{\beta}_0, \widehat{\beta}_1] = c_{01} \sigma^2\)</span> where <span class="math inline">\(c_{01} = - \overline{x} / S_{xx}\)</span>.</li>
</ol>
<p>These properties can be determined by working directly from <a href="linear-models.html#eq:ls-slope">(6.5)</a> and <a href="linear-models.html#eq:ls-intercept">(6.6)</a>.</p>
<div class="proposition">
<p><span id="prp:htest-ls-betas" class="proposition"><strong>Proposition 6.1  </strong></span>Consider <span class="math inline">\(H_0 : \beta_i = \beta_{i0}\)</span>. The test statistic is
<span class="math display" id="eq:htest-ls-betas-statistic">\[\begin{equation*}
T = \frac{\widehat{\beta}_i - \beta_{i0}}{S\sqrt{c_{ii}}} \,.
\tag{6.8}
\end{equation*}\]</span></p>
<p>For a hypothesis test at level <span class="math inline">\(\alpha\)</span>, we use the following procedure:</p>
<p>If <span class="math inline">\(H_a : \beta_i &gt; \beta_{i0}\)</span>, then <span class="math inline">\(P\)</span>-value is the area under <span class="math inline">\(\mathsf{t}(m-2)\)</span> to the right of <span class="math inline">\(t\)</span>.</p>
<p>If <span class="math inline">\(H_a : \beta_i &lt; \beta_{i0}\)</span>, tthen <span class="math inline">\(P\)</span>-value is the area under <span class="math inline">\(\mathsf{t}(m-2)\)</span> to the left of <span class="math inline">\(t\)</span>.</p>
<p>If <span class="math inline">\(H_a : \beta_i \neq \beta_{i0}\)</span>, then <span class="math inline">\(P\)</span>-value is twice the area under <span class="math inline">\(\mathsf{t}(m-2)\)</span> to the right of <span class="math inline">\(|t|\)</span>.</p>
</div>
<p>A confidence interval for <span class="math inline">\(\beta_i\)</span>, based on the statistic <a href="linear-models.html#eq:htest-ls-betas-statistic">(6.8)</a>, can be given following the procedures in <a href="inference-single-sample.html#inference-single-sample">3</a>.</p>
<div class="proposition">
<p><span id="prp:ci-ls-betas" class="proposition"><strong>Proposition 6.2  </strong></span>A <span class="math inline">\(100(1-\alpha)\%\)</span> CI for <span class="math inline">\(\beta_i\)</span> is given by
<span class="math display">\[\begin{equation*}
\widehat{\beta}_i \pm t_{\alpha/2, m-2} S \sqrt{c_{ii}} \,.
\end{equation*}\]</span></p>
</div>
</div>
<div id="correlation" class="section level2 hasAnchor" number="6.4">
<h2><span class="header-section-number">6.4</span> Correlation<a href="linear-models.html#correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\((X_1, Y_1), \dots, (X_m, Y_m)\)</span> denote a random sample from a bivariate normal distribution with <span class="math inline">\(\E[X_i] = \mu_X\)</span>, <span class="math inline">\(\E[Y_i] = \mu_Y\)</span>, <span class="math inline">\(\Var[X_i] = \sigma_X^2\)</span>, <span class="math inline">\(\Var[Y_i] = \sigma_Y^2\)</span>, and correlation coefficient <span class="math inline">\(\rho\)</span>. The sample correlation coefficient is given by,
<span class="math display" id="eq:sample-correlation-statistic">\[\begin{equation}
r = \frac{\sum_{i=1}^m (X_i - \overline{X})(Y_i - \overline{Y})}{\sqrt{\sum_{i=1}^m (X_i - \overline{X})^2 \sum_{i=1}^m (Y_i - \overline{Y})^2}}\,,
\tag{6.9}
\end{equation}\]</span>
which can be rewritten in terms of <span class="math inline">\(S_{xx}\)</span>, <span class="math inline">\(S_{xy}\)</span>, and <span class="math inline">\(S_{yy}\)</span>:
<span class="math display">\[\begin{equation*}
r = \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}} = \widehat{\beta}_1 \sqrt{\frac{S_{xx}}{S_{yy}}}\,,
\end{equation*}\]</span>
using <a href="linear-models.html#eq:ls-slope">(6.5)</a> and we see that <span class="math inline">\(r\)</span> and <span class="math inline">\(\widehat{\beta}_1\)</span> have the same sign. A <span class="math inline">\(|r|\)</span> close to <span class="math inline">\(1\)</span> means that the regression line is a good fit to the data, and, similarly, an <span class="math inline">\(|r|\)</span> close to <span class="math inline">\(0\)</span> means a poor fit to the data. Note that the correlation coefficient (and the least squares regression) are only suitable for describing <em>linear</em> relationships; a nonlinear relationship can also yield <span class="math inline">\(r\)</span> near zero (see Figure <a href="linear-models.html#fig:linear-model-correlation">6.4</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linear-model-correlation"></span>
<img src="ma22004_files/figure-html/linear-model-correlation-1.svg" alt="Correlations range from $-1$ to $1$ with $|r|=1$ indicating a strong linear relationship and $r$ near zero indicating the absence of a linear relationship." width="768" />
<p class="caption">
Figure 6.4: Correlations range from <span class="math inline">\(-1\)</span> to <span class="math inline">\(1\)</span> with <span class="math inline">\(|r|=1\)</span> indicating a strong linear relationship and <span class="math inline">\(r\)</span> near zero indicating the absence of a linear relationship.
</p>
</div>
<p>Once a model is fit, it can be used to predict a value of <span class="math inline">\(y\)</span> for a given <span class="math inline">\(x\)</span>. However, the model only gives the most likely value of <span class="math inline">\(y\)</span>; a corresponding <strong>prediction interval</strong> is usually more appropriate.</p>
<div class="proposition">
<p><span id="prp:prediction-interval" class="proposition"><strong>Proposition 6.3  </strong></span>A <span class="math inline">\(100(1-\alpha)\%\)</span> <strong>prediction interval</strong> for an actual value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(x = x^*\)</span> is given by
<span class="math display">\[\begin{equation*}
(\widehat{\beta}_0 + \widehat{\beta}_1 x^*) \pm t_{\alpha/2, m-2} S \sqrt{1 + \frac{1}{n} + \frac{(x^* - \overline{x})^2}{S_{xx}}} \,.
\end{equation*}\]</span></p>
</div>
<div class="warningblock">
<p>The prediction interval is different from the confidence interval for expected <span class="math inline">\(Y\)</span>. Note that the length of the <em>confidence interval</em> for <span class="math inline">\(\E[Y]\)</span> when <span class="math inline">\(x=x^*\)</span> is given by
<span class="math display">\[\begin{equation*}
2 \cdot t_{\alpha/2} S  \sqrt{\frac{1}{n} + \frac{(x^* - \overline{x})^2}{S_{xx}}}
\end{equation*}\]</span>
whereas the length for the <em>prediction interval</em> of <span class="math inline">\(Y\)</span> is
<span class="math display">\[\begin{equation*}
2 \cdot t_{\alpha/2} S  \sqrt{1 + \frac{1}{n} + \frac{(x^* - \overline{x})^2}{S_{xx}}} \,.
\end{equation*}\]</span>
Thus the prediction intervals for an actual value of <span class="math inline">\(Y\)</span> are longer than the confidence intervals for <span class="math inline">\(\E[Y]\)</span> if both are determined for the same value <span class="math inline">\(x^*\)</span>.</p>
</div>
<p>The linear model
<span class="math display">\[\begin{equation*}
\E[ Y \mid X = x ] = \beta_0 + \beta_1 x \,,
\end{equation*}\]</span>
assumes that the conditional expectation of <span class="math inline">\(Y\)</span> for a fixed value of <span class="math inline">\(X\)</span> is a linear function of the <span class="math inline">\(x\)</span> value. If we assume that <span class="math inline">\((X,Y)\)</span> has a bivariate normal distribution, then
<span class="math display">\[\begin{equation*}
\beta_1 = \frac{\sigma_Y}{\sigma_X} \rho \,,
\end{equation*}\]</span>
and thus, for the simple hypothesis tests we have considered (Table <a href="statistical-inference.html#tab:htest-null-alt-forms">2.2</a>), statistical tests for <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\rho\)</span> are equivalent.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="20">
<li id="fn20"><p>The covariates <span class="math inline">\(X\)</span> are also called <strong>predictor variables</strong>, <strong>explanatory variables</strong>, <strong>independent variables</strong>, and/or <strong>features</strong> depending on who you are talking to.<a href="linear-models.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>The <span class="math inline">\(\mathsf{RSS}\)</span> is sometimes referred to as the <strong>error sum of squares</strong> and abbreviated <span class="math inline">\(\mathsf{SSE}\)</span> (no, the order is not a typo).<a href="linear-models.html#fnref21" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="anova.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="categorical-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dundeemath/MA22004/edit/master/06-linear-models.Rmd",
"text": "Suggest an edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["ma22004.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed"
},
"info": true,
"highlight": "pygments",
"number_sections": true
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
