# Sampling distributions {#sampling-distributions}

A **statistic** is a quantity that can be calculated from sample data. Before obtaining data, a statistic is an unknown quantity and is, therefore, a rv. We refer to the probability distribution for a statistic as a **sampling distribution** to emphasise how the distribution will vary across all possible sample data. 

## Normal distribution {#normal-distribution}

Normal distributions play an important role in probability and statistics as they describe many natural phenomena. For instance, the Central Limit Theorem tells us that sums of rvs are approximately normal in distribution.  

```{definition, normal-dist}
A continuous rv $X$ has a **normal distribution** with parameters $\mu$ and $\sigma^2$, where $-\infty < \mu < \infty$ and $\sigma > 0$, if $X$ has pdf
\begin{equation*}
 f(x; \mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma}e^{-(x-\mu)^2/(2\sigma^2)}\,, 
 \quad -\infty < x < \infty \,.
\end{equation*}
We write $X \sim \mathsf{N}(\mu, \sigma^2)$. 
```  

For $X\sim \mathsf{N}(\mu,\sigma^2)$, it can be shown that $\E(X) = \mu$ and $\Var(X) = \sigma^2$, that is, $\mu$ is the *mean* and $\sigma^2$ is the *variance* of $X$. The pdf forms a bell-shaped curve that is symmetric about $\mu$. The value $\sigma$ (*standard deviation*) is the distance from $\mu$ to the inflection points of the curve. Thus, the distribution's position (location) and spread depend on $\mu$ and $\sigma$.  

```{r normals-diff-mean, echo = FALSE, fig.cap = "The pdfs of two normal rvs, $X_1 \\sim \\mathsf{N}(-2, 1)$ and $X_2 \\sim \\mathsf{N}(2, 1)$, with *different means* and the same standard deviations."}
norm1.dist <- data.frame(x = seq(-8, 8, by =0.01)) %>% 
 mutate(X1 = dnorm(x = x, mean = -2, sd = 1),
        X2 = dnorm(x = x, mean = 2, sd = 1)) %>%
 gather(key = "X", value = "density", -x)
norm1.dist$X <- factor(norm1.dist$X, levels = c("X1", "X2"), labels = c("-2", "2"))
ggplot(norm1.dist, aes(x = x, y = density, color = X, linetype = X)) +
 geom_line(size = lsz) +
 labs(y = TeX("$f(x;\\mu, \\sigma)$"), x = TeX("$x$"), color = TeX("$\\mu$"), linetype = TeX("$\\mu$")) + theme_ur
```  

```{r normals-diff-sd, echo = FALSE, fig.cap = "The pdfs of two normal rvs, $X_1 \\sim \\mathsf{N}(0, 9)$ and $X_2 \\sim \\mathsf{N}(0, 1)$, with the same means and *different standard deviations*."}
norm2.dist <- data.frame(x = seq(-8, 8, by =0.01)) %>% 
 mutate(X1 = dnorm(x = x, mean = 0, sd = 3),
        X2 = dnorm(x = x, mean = 0, sd = 1)) %>%
 gather(key = "X", value = "density", -x)
norm2.dist$X <- factor(norm2.dist$X, levels = c("X1", "X2"), labels = c("3", "1"))
ggplot(norm2.dist, aes(x = x, y = density, color = X, linetype = X)) +
 geom_line(size = lsz) +
 labs(y = TeX("$f(x;\\mu, \\sigma)$"), x = TeX("$x$"), color = TeX("$\\sigma$"), linetype = TeX("$\\sigma$")) + theme_ur
```  

```{definition, standard-normal}
We say that $X$ has a **standard normal distribution** if $\mu=0$ and $\sigma = 1$ and we will usually denote standard Normal rvs by \[Z \sim \mathsf{N}(0,1)\] (why? tradition!^["Traditions, traditions... Without our traditions, our lives would be as shaky as a fiddler on the roof!" [[https://www.youtube.com/watch?v=gRdfX7ut8gw](https://www.youtube.com/watch?v=gRdfX7ut8gw)].]). We denote the cumulative distribution function of the standard normal by \[\Phi(z) = P(Z \leq z)\] and write $\varphi = \Phi'$ for its density function. 
```  

### Some useful facts about normal variates {#facts-normals}

Here are some useful facts about how to manipulate Normal rvs.  

1. If $X \sim \mathsf{N}(\mu, \sigma^2),$ then $Z = (X - \mu) / \sigma  \sim \mathsf{N}(0,1).$ 
2. If $Z \sim \mathsf{N}(0, 1),$ then $X = \mu + \sigma Z \sim \mathsf{N}(\mu, \sigma^2).$ 
3. If $X_i \sim \mathsf{N}(\mu_i, \sigma_i^2)$ for $i = 1, \dots, n$ are independent rvs, then 
\[\sum_{i=1}^{n} X_i \sim \mathsf{N} \left( \sum_{i=1}^{n} \mu_i, \sum_{i=1}^{n} \sigma_i^2 \right) \,.\]  

In particular, we note that for differences of independent rvs $X_1 \sim \mathsf{N}(\mu_1, \sigma_1^2)$ and $X_2 \sim \mathsf{N}(\mu_2, \sigma_2^2)$ then the variances also add:
\[ X_1 - X_2 \sim \mathsf{N}(\mu_1 - \mu_2, \sigma_1^2 + \sigma_2^2) \,.\]  

Probabilities $P(a \leq X \leq b)$ are found by converting the problem in $X \sim \mathsf{N}(\mu, \sigma^2)$ to the *standard normal* distribution $Z \sim \mathsf{N}(0, 1)$ whose probability values $\Phi(z) = P(Z\leq z)$ can then be looked up in a table. From (1.) above, 
\[\begin{aligned}
   P(a < X < b) &= P\left( \frac{a-\mu}{\sigma} < Z < \frac{b-\mu}{\sigma} \right) \\ 
    &= \Phi \left( \frac{b-\mu}{\sigma}\right) - \Phi\left(\frac{a-\mu}{\sigma}\right) \,.
   \end{aligned}\]
This process is often referred to as *standardising* (the normal rv).  

```{example, eg-norm-rt}
Let $X \sim \mathsf{N}(5, 9)$ and find $P(X \geq 5.5)$. 

\[\begin{aligned}
   P(X \geq 5.5) &= P\left(Z \geq \frac{5.5 - 5}{3}\right) \\
    &= P(Z \geq 0.1667) \\
    &= 1 - P(Z \leq 0.1667) \\
    &= 1 - \Phi(0.1667) \\
    &= 1 - 0.5662 \\
    &= 0.4338\,,
    \end{aligned}\]
where we look up the value of $\Phi(z) = P(Z\leq z)$ in a table of standard normal curve areas.
```  

The probability corresponds to the shaded area under the normal density $\varphi(x) = \Phi'(x)$ corresponding to $x \geq 5.5$.

```{r example-norm-geq, warning = FALSE, echo = FALSE, fig.cap = "The normal density from the exercise with the (one-sided) interval shaded in blue."}
ggplot(NULL, aes(c(-4, 14))) + 
 geom_line(stat = "function", fun = dnorm, args = list(mean = 5, sd = 3), xlim = c(-4, 14), size = lsz) +
 geom_area(stat = "function", fun = dnorm, args = list(mean = 5, sd = 3), fill = "blue", xlim = c(5.5, 14), alpha = 0.3) + 
 geom_segment(aes(x = 5.5, y = 0, xend = 5.5, yend = 0.131), linetype = "dashed") + 
 theme(axis.title.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank()) + 
 annotate("text", x = 5.5, y = -0.005, label = TeX("$x \\geq 5.5$"), size = tsz, color = "blue") + 
 annotate("text", x = 10, y = 0.1, label = TeX("$X \\sim N(5,9)$"), size = tsz)
```

Alternatively, we can use the `r` code: `pnorm(5.5, mean = 5, sd = 3, lower.tail = FALSE)`.  $\lozenge$   


```{example, eg-norm-dt}
Let $X \sim \mathsf{N}(5, 9)$ and find $P(4 \leq X \leq 5.25)$. 

\[\begin{aligned}
   P(4 \leq X \leq 5.25) &= P\left(\frac{4-5}{3} \leq Z \leq \frac{5.25-5}{3}\right) \\
   &= P(-0.3333 \leq Z \leq 0.0833) \\
   &= \Phi(0.0833) - \Phi(-0.3333) \\
   &= 0.5332 - 0.3694 \\
   &= 0.1638\,.
  \end{aligned}\]
where we look up the value of $\Phi(z) = P(Z\leq z)$ in a table of standard normal curve areas. 
```  

The probability corresponds to the shaded area under the normal density $\varphi(x) = \Phi'(x)$ corresponding to $4 \leq x \leq 5.25$.

```{r example-norm-interval, warning = FALSE, echo = FALSE, fig.cap = "The normal density from the exercise with the interval shaded in blue."}
ggplot(NULL, aes(c(-4, 14))) + 
 geom_line(stat = "function", fun = dnorm, args = list(mean = 5, sd = 3), xlim = c(-4, 14), size = lsz) +
 geom_area(stat = "function", fun = dnorm, args = list(mean = 5, sd = 3), fill = "blue", xlim = c(4, 5.25), alpha = 0.3) + 
 geom_segment(aes(x = c(4, 5.25), y = 0, xend = c(4, 5.25), yend = c(0.125, 0.133)), linetype = "dashed") + 
 theme(axis.title.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(), axis.title.x = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank()) + 
 annotate("text", x = 4.625, y = -0.005, label = TeX("$4 \\leq x \\leq 5.25$"), size = tsz, color = "blue") + 
 annotate("text", x = 10, y = 0.1, label = TeX("$X \\sim \\textbf{N}(5,9)$"), size = tsz)
```

Alternatively, we can use the `r` code:
`pnorm(5.25, mean = 5, sd = 3) - pnorm(4, mean = 5, sd = 3)`.  $\lozenge$

### Empirical rule ($68-95-99.7$ rule)

For samples from a normal distribution, the percentage of values that lie within one, two, and three standard deviations of the mean are $68.27\%$, $95.45\%$, and $99.73\%$, respectively. That is, for $X \sim \mathsf{N}(\mu, \sigma^2)$, 
\begin{equation*}
P(\mu - 1 \sigma \leq X \leq \mu + 1 \sigma ) \approx 0.6827\,,
\end{equation*}
\begin{equation*}
P(\mu - 2 \sigma \leq X \leq \mu + 2 \sigma ) \approx 0.9545\,,
\end{equation*}
\begin{equation*}
P(\mu - 3 \sigma \leq X \leq \mu + 3 \sigma ) \approx 0.9973\,.
\end{equation*}
For a normal population, nearly all the values lie within "three sigmas" of the mean.

## $\mathsf{t}$ distribution {#t-distribution}

Student's $\mathsf{t}$ distribution gets its peculiar name as it was first published under the pseudonym  "Student".^[William Sealy Gosset (1876--1937) wrote under the pseudonym "Student" [[https://mathshistory.st-andrews.ac.uk/Biographies/Gosset/](https://mathshistory.st-andrews.ac.uk/Biographies/Gosset/)].] This bit of obfuscation was to protect the identity of his employer,^[Gosset invented the t-test to handle small samples for quality control in brewing, specifically for the Guinness brewery in Dublin [[https://www.wikiwand.com/en/Guinness_Brewery](https://www.wikiwand.com/en/Guinness_Brewery)].] and thereby vital trade secrets, in a highly competitive and lucrative industry.  

```{definition, t-dist}
A continuous rv $X$ has a **$\mathsf{t}$ distribution** with parameter $\nu > 0$, if $X$ has pdf
\begin{equation*}
f(x; \nu) = \frac{\Gamma\left(\tfrac{\nu+1}{2}\right)}{\sqrt{\nu \pi} \Gamma \left(\tfrac{\nu}{2}\right)} \left( 1 + \tfrac{x^2}{\nu} \right)^{- \frac{\nu+1}{2}} \,, \quad -\infty < x < \infty\,.
\end{equation*}
We write $X \sim \mathsf{t}(\nu)$. Note $\Gamma$ is the standard gamma function.^[The gamma function is defined by $\Gamma(z) = \int_0^\infty x^{z-1}e^{-x} dx$ when the real part of $z$ is positive. For any positive integer $n$, $\Gamma(n) = (n-1)!$ and for half-integers $\Gamma(\tfrac{1}{2} + n) = \frac{(2n)!}{4^n n!} \sqrt{\pi}$.]
```  

The density for $\mathsf{t}(\nu)$ for several values of $\nu$ are plotted below. 

```{r exemplar-t-dist, echo = FALSE, fig.cap = "The density for $\\mathsf{t}(\\nu)$ for several values of $\\nu$ (df)."}
t.dist <- data.frame(t = seq(-4, 4, 0.1)) %>% 
 mutate(df1 = dt(x = t, df = 1), 
        df2 = dt(x = t, df = 2), 
        df5 = dt(x = t, df = 5), 
        df1000 = dt(x = t, df = 1000)) %>% 
 gather(key = "df", value = "density", -t)
t.dist$df <- factor(t.dist$df, levels = c("df1", "df2", "df5", "df1000"), labels = c(1, 2, 5, 1000))
ggplot(t.dist, aes(x = t, y = density, color = df)) +
 geom_line(size = lsz) +
 labs(y = TeX("$f(x;\\nu)$"), x = TeX("$x$"), color = TeX("$\\nu$ (df)")) + theme_ur
```

### Properties of $\mathsf{t}$ distributions {#facts-t}

1. The density for $\mathsf{t}(\nu)$ is a bell-shaped curve centred at $0$.
2. The density for $\mathsf{t}(\nu)$ is more spread out than the standard normal density (i.e., it has "fatter tails" than the normal).
3. As $\nu \to \infty$, the spread of the corresponding $\mathsf{t}(\nu)$ density converges to the standard normal density (i.e., the spread of the $\mathsf{t}(\nu)$ density decreases relative to the standard normal).  

If $X \sim \mathsf{t}(\nu)$, then $\E[X] = 0$ for $\nu > 1$ (otherwise the mean is undefined).  


## $\chi^2$ distribution {#chisq-distribution}

The $\chi^2$ distribution arises as the distribution of a sum of the squares of $\nu$ independent standard normal rvs.  

```{definition, chisq-dist}
A continuous rv $X$ has a **$\chi^2$ distribution** with parameter $\nu \in \mathbf{N}_{>}$, if $X$ has pdf
\begin{equation*}
f(x; \nu) = \frac{1}{2^{\nu/2} \Gamma(\nu/2)} x^{(\nu/2)-1} e^{-x/2} \,, 
\end{equation*}
with support $x \in (0, \infty)$ if $\nu=1$, otherwise $x \in [0, \infty)$. We write $X \sim \chi^2(\nu)$. 
```  

The pdf $f(x; \nu)$ of the $\chi^2(\nu)$ distribution depends on a positive integer $\nu$ referred to as the df. The densities for several values of $\nu$ are plotted below. 

```{r exemplar-chisq-dist, echo = FALSE, warning = FALSE, fig.cap = "The density for $\\chi^2(\\nu)$ for several values of $\\nu$ (df)."}
chisq.dist <- data.frame(chisq = 0:7000 / 100) %>% 
 mutate(df1 = dchisq(x = chisq, df = 1), 
        df2 = dchisq(x = chisq, df = 2),
        df3 = dchisq(x = chisq, df = 3),
        df4 = dchisq(x = chisq, df = 4),
        df6 = dchisq(x = chisq, df = 6),
        df9 = dchisq(x = chisq, df = 9)) %>%
 gather(key = "df", value = "density", -chisq)
chisq.dist$df <- factor(chisq.dist$df, levels = c("df1", "df2", "df3", "df4", "df6", "df9"), labels = c(1, 2, 3, 4, 6, 9))
ggplot(chisq.dist, aes(x = chisq, y = density, color = df)) +
 geom_line(size = lsz) + ylim(c(0,.6)) + xlim(c(0,15)) +
 labs(y = TeX("$f(x;\\nu)$"), x = TeX("$x$"), color = TeX("$\\nu$ (df)")) + theme_ur
```

The density $f(x;\nu)$ is positively skewed, i.e., the right tail is longer, so the mass is concentrated to the figure's left. The distribution becomes more symmetric as $\nu$ increases. We denote critical values of the $\chi^2(\nu)$ distribution by $\chi^2_{\alpha, \nu}$.  

:::{.warningblock data-latex=""}
Unlike the normal and $t$ distributions, the $\chi^2$ distribution is not symmetric. This means that the critical values e.g. $\chi^2_{.99, \nu}$ and $\chi^2_{0.01,\nu}$ are **not** equal. Hence, it will be necessary to look up both values for CIs based on $\chi^2$ critical values.  
:::

If $X \sim \chi^2(\nu)$, then $\E[X] = \nu$ and $\Var[X] = 2\nu$.  

## $\mathsf{F}$ distribution {#F-distribution}

The $\mathsf{F}$ distribution arises as a test statistic when comparing population variances and in the analysis of variance (see \@ref(anova)). 

```{definition, F-dist}
A continuous rv $X$ has an **$\mathsf{F}$ distribution** with df parameters $\nu_1$ and $\nu_2$, if $X$ has pdf
\begin{equation*}
 f(x; \nu_1, \nu_2) = 
    \frac{\Gamma\left(\frac{\nu_1+\nu_2}{2}\right) \nu_1^{\nu_1/2} \nu_2^{\nu_2/2}}
 {\Gamma\left(\frac{\nu_1}{2}\right) \Gamma\left(\frac{\nu_2}{2}\right)} 
 \frac{x^{\nu_1/2 - 1}}{(\nu_2+\nu_1 x)^{(\nu_1+\nu_2)/2}} \,.
\end{equation*}
```

The pdf $f(x; \nu_1, \nu_2)$ of the $\mathsf{F}(\nu_1, \nu_2)$ distribution depends on two positive integers $\nu_1$ and $\nu_2$ referred to, respectively, as the numerator and denominator df. The density is plotted below for several combinations of $(\nu_1, \nu_2)$. 

```{r exemplar-F-dist, echo = FALSE, warning = FALSE, fig.cap = "The density for $\\mathsf{F}(\\nu_1, \\nu_2)$ for several combinations of $(\\nu_1, \\nu_2)$."}

f.dist <- data.frame(f = 0:1000 / 100) %>% 
 mutate(df_1_1 = df(x = f, df1 = 1, df2 = 1),
        df_2_1 = df(x = f, df1 = 2, df2 = 1),
        df_5_2 = df(x = f, df1 = 5, df2 = 2),
        df_10_2 = df(x = f, df1 = 10, df2 = 2),
        df_100_100 = df(x = f, df1 = 100, df2 = 100)) %>%
 gather(key = "df", value = "density", -f)
f.dist$df <- factor(f.dist$df, levels = c("df_1_1", "df_2_1", "df_5_2", "df_10_2", "df_100_100"), labels = c("(1, 1)", "(2, 1)", "(5, 2)", "(10, 2)", "(100, 100)"))
ggplot(f.dist, aes(x = f, y = density, color = df)) +
 geom_line(size = lsz) + ylim(c(0,2.1)) + xlim(c(0,3)) +
 labs(y = TeX("$f(x;\\nu_1, \\nu_2)$"), x = TeX("$x$"), color = TeX("$(\\nu_1, \\nu_2)$")) + theme_ur
```

Where do the terms numerator and denominator df come from? The $\mathsf{F}$ distribution is related to ratios of $\chi^2$ rvs.

```{theorem, F-dist-chisq}
If $X_1 \sim \chi^2(\nu_1)$ and $X_2 \sim \chi^2(\nu_2)$ are independent rvs, then the rv 
\begin{equation*}
 F = \frac{X_1 / \nu_1}{X_2 / \nu_2} \quad \sim \mathsf{F}(\nu_1,\nu_2)\,,
\end{equation*}
that comprises the ratio of two $\chi^2$ rvs divided by their respective df has an $\mathsf{F}(\nu_1, \nu_2)$ distribution. 
```