{
  "hash": "b1c48526d7fd502bcf3aa423bf37639a",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n::: {.content-hidden when-format=\"pdf\"}\n\\newcommand{\\E}{\\mathbf{E}} \n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\corr}{corr}\n\\DeclareMathOperator{\\sd}{sd}\n\\newcommand{\\se}{\\mathsf{se}}\n:::\n\n\n\n\n\n# Analysis of variance {#sec-anova}\n\nAnalysis of variance, shortened as ANOVA or AOV, is a collection of statistical models and estimation procedures for analysing the variation among different groups. In particular, a single-factor ANOVA provides a hypothesis test regarding the equality of two or more population means, thereby generalising the one-sample and two-sample $\\mathsf{t}$ tests considered in @sec-mean-normal-var-unknown and @sec-compare-means-normpops-vars-unknown.\n\n## Single factor ANOVA test {#sec-anova-single-factor-test}\n\nSuppose that we have $k$ normally distributed populations with different means $\\mu_1, \\dots, \\mu_k$ and equal variances $\\sigma^2.$ We denote the rv for the $j$th measurement taken from the $i$th population by $X_{ij}$ and the corresponding sample observation by $x_{ij}.$ For samples of size $m_1, \\dots, m_k,$ we denote the sample means\n$$\n \\overline{X}_i = \\frac{1}{m_i} \\sum_{j=1}^{m_i} X_{ij}\\,, \n$$\nand sample variances \n$$\n S_i^2 = \\frac{1}{m_i - 1} \\sum_{j=1}^{m_i} (X_{ij} - \\overline{X}_{i})^2\\,,\n$$\nfor each $i = 1, \\dots, k$; likewise, we denote the associated point estimates for the sample means $\\overline{x}_1, \\dots, \\overline{x}_k$ and the sample variances $s_1^2, \\dots, s_k^2.$ The average over all observations $m = \\sum m_i,$ called the grand mean, is denoted by \n$$\n \\overline{X} = \\frac{1}{m} \\sum_{i=1}^k \\sum_{j=1}^{m_i} X_{ij}\\,.\n$$ \nThe sample variances $s_i^2,$ and hence the sample standard deviations, will generally vary even when the $k$ populations share the same variance; a rule of thumb is that the equality of variances is reasonable if the largest $s_i$ is not much more than two times the smallest. \n\n:::{.callout-tip}\n## Alternative lingo \nIn the context of ANOVA, these $k$ populations are often referred to as *treatment* distributions. \n:::\n\nWe wish to test the equality of the population means, given by the null hypothesis,\n$$\n H_0 : \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\,,\n$$\nversus the alternative hypothesis,\n$$\n H_a : \\text{at least two}\\; \\mu_i \\; \\text{differ}\\,.\n$$\nNote that if $k=3$ then $H_0$ is true only if all three means are the same, i.e., $\\mu_1 = \\mu_1 = \\mu_3,$ but there are a number of ways which the alternative might hold: $\\mu_1 \\neq \\mu_2 = \\mu_3$ or $\\mu_1 = \\mu_2 \\neq \\mu_3$ or $\\mu_1 = \\mu_3 \\neq \\mu_2$ or $\\mu_1 \\neq \\mu_2 \\neq \\mu_3.$ \n\nThe test procedure is based on comparing a measure of the difference in variation among the sample means, i.e., the variation between $x_i$'s, to a measure of variation within each sample.  \n\n:::{#def-mstr-mse}\nThe mean square for treatments is \n$$\n \\mathsf{MSTr} = \\frac{1}{k-1} \\sum_{i=1}^k m_i (\\overline{X}_i - \\overline{X})^2\\,,\n$$\nand the mean square error is \n$$\n \\mathsf{MSE} = \\frac{1}{m-k} \\sum_{i=1}^k (m_i - 1) S_i^2 \\,.\n$$\nThe $\\mathsf{MSTr}$ and $\\mathsf{MSE}$ are statistics that measure the variation among sample means and the variation within samples. We will also use $\\mathsf{MSTr}$ and $\\mathsf{MSE}$ to denote the calculated values of these statistics. \n:::\n\n:::{#prp-htest-anova}\nThe test statistic \n$$\nF = \\frac{\\mathsf{MSTr}}{\\mathsf{MSE}} \n$$\nis the appropriate test statistic for the single-factor ANOVA problem involving $k$ populations (or treatments) with a random sample of size $m_1, \\dots, m_k$ from each. When $H_0$ is true, \n$$\nF \\sim \\mathsf{F}(\\nu_1 = k-1, \\nu_2 = m - k)\\,.\n$$\nIn the present context, a large test statistic value is more contradictory to $H_0$ than a smaller value. Therefore the test is upper-tailed, i.e., consider the area $F_\\alpha$ to the right of the critical value $F_{\\alpha, \\nu_1, \\nu_2}.$ We reject $H_0$ if the value of the test statistic $F > F_\\alpha.$\n:::  \n\n:::{#nte-salary .callout-note collapse=\"true\"}\n## Average Salary Data \n\nThe **Average Salary Data** comprises average salaries reported by $20$ local councils across the four nations of the United Kingdom (England, N Ireland, Scotland and Wales). The sample means and sample standard deviations are summarised in @tbl-anova-samples-stats. \n\n\n::: {#tbl-anova-samples-stats .cell layout-align=\"center\" tbl-cap='**Average Salary Data** reported from $20$ local councils.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Nation </th>\n   <th style=\"text-align:left;\"> Average salaries ('000 Â£) </th>\n   <th style=\"text-align:center;\"> Sample size </th>\n   <th style=\"text-align:center;\"> Sample mean </th>\n   <th style=\"text-align:center;\"> Sample sd </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> England </td>\n   <td style=\"text-align:left;\"> 17, 12, 18, 13, 15, 12 </td>\n   <td style=\"text-align:center;\"> 6 </td>\n   <td style=\"text-align:center;\"> 14.5 </td>\n   <td style=\"text-align:center;\"> 2.588 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> N Ireland </td>\n   <td style=\"text-align:left;\"> 11, 7, 9, 13 </td>\n   <td style=\"text-align:center;\"> 4 </td>\n   <td style=\"text-align:center;\"> 10.0 </td>\n   <td style=\"text-align:center;\"> 2.582 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Scotland </td>\n   <td style=\"text-align:left;\"> 15, 10, 13, 14, 13 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 13.0 </td>\n   <td style=\"text-align:center;\"> 1.871 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Wales </td>\n   <td style=\"text-align:left;\"> 10, 12, 8, 7, 9 </td>\n   <td style=\"text-align:center;\"> 5 </td>\n   <td style=\"text-align:center;\"> 9.2 </td>\n   <td style=\"text-align:center;\"> 1.924 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n:::\n\n:::{#exm-anova}\nConsider the **Average Salary Data** reported in @nte-salary. Is the expected average salary in each nation the same at the $5\\%$ level?\n\nWe begin by exploring the data through the generation and interpretation of some box plots. The box plots in @fig-anova-samples-boxplots indicate that there may be a difference in median average salary by nation. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Box plots of the average mean salary data in Table @tbl-anova-samples-stats indicate five summary statistics: the median, two hinges (first and third quartiles) and two whiskers (extending from the hinge to the most extreme data point within $1.5 \\cdot \\mathsf{IQR}$).](05-anova_files/figure-html/fig-anova-samples-boxplots-1.svg){#fig-anova-samples-boxplots fig-align='center' width=672}\n:::\n:::\n\n\nFor $\\alpha = 0.05,$ we compute the upper-tail area $F_{0.05}$ i.e. to the right of the critical value $F_{0.05, 3, 16}$ by consulting a statistical table or by using `R` to find $F_{0.05} = 3.2388715.$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# alt: qf(.05, df1 = 3, df2 = 16, lower.tail = FALSE)\nqf(1-.05, df1 = 4-1, df2 = 20-4) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.238872\n```\n\n\n:::\n:::\n\n \nThe grand mean is \n$$\n \\overline{x} = \\frac{17 + 12 + 18 + \\cdots + 8 + 7 + 9}{20} = 11.9\\,,\n$$\nand hence the variation among sample means is given by,\n$$\n\\begin{aligned}\n \\mathsf{MSTr} &= \\frac{1}{4-1} \\left(m_1(\\overline{x}_1 - \\overline{x})^2 + \\cdots + m_4 (\\overline{x}_4 - \\overline{x})^2\\right) \\\\\n &= \\left(6 (14.5-11.9)^2 + 4(10.0 - 11.9)^2 + 5(13.0-11.9)^2 + 5 ( 9.2 - 11.9)^2\\right) / 3 \\\\\n &= 32.5 \\,.\n\\end{aligned}\n$$\nThe mean square error is\n$$\n \\begin{aligned}\n \\mathsf{MSE} & = \\frac{1}{20-4} \\left((m_1 - 1)s_1^2 + \\cdots (m_4-1)s_4^2\\right)\\\\ \n  &= \\frac{5(2.588)^2 + 3(2.582)^2 + 4(1.871)^2 + 4(1.924)^2}{16} \\\\\n  &= 5.14366\n \\end{aligned}\n$$\nyielding the test statistic value\n$$\n F = \\frac{\\mathsf{MSTr}}{\\mathsf{MSE}} = \\frac{32.5}{5.14366} \n = 6.3184581 \\,.\n$$\nSince $F > F_\\alpha$ we reject $H_0.$ The data do not support the hypothesis that the mean salaries in each nation are identical at the $5\\%$ level. \n:::\n\n## Confidence intervals {#sec-anova-ci}\n\nIn @sec-compare-means, we gave a CI for comparing population means involving the difference $\\mu_X - \\mu_Y.$ In some settings, we would like to give CIs for more complicated functions of population means $\\mu_i.$ Let\n$$\n \\theta = \\sum_{i=1}^k c_i \\mu_i\\,,\n$$\nfor constants $c_i.$ As we assume the $X_ij$ are normally distributed with $\\E[X_{ij}] = \\mu_i$ and $\\Var[X_{ij}] = \\sigma^2,$ the estimator\n$$\n \\widehat{\\theta} = \\sum_{i=1}^k c_i \\overline{X}_{i}\\,,\n$$\nis normally distributed with\n$$\n \\Var[\\widehat{\\theta}] = \\sum_{i=1}^k c_i^2 \\Var[\\overline{X}_i] = \\sigma^2 \\sum_{i=1}^{k} \\frac{c_i^2}{m_i}\\,.\n$$\nWe estimate $\\sigma^2$ by the $\\mathsf{MSE}$ and standardise the estimator to arrive at a $\\mathsf{t}$ variable\n$$\n \\frac{\\widehat{\\theta} - \\theta}{\\widehat{\\sigma}_{\\widehat{\\theta}}}\\,,\n$$\nwhere $\\widehat{\\sigma}_{\\widehat{\\theta}}$ is the estimated standard error of the estimator. \n\n:::{#prp-ci-anova}\nA $100(1-\\alpha)\\%$ CI for $\\sum c_i \\mu_i$ is given by \n$$\n \\sum_{i=1}^k c_i \\overline{x}_i \\pm t_{\\alpha/2, m-k} \\sqrt{\\mathsf{MSE} \\sum_{i=1}^k \\frac{c_i^2}{m_i}}\\,.\n$$\n:::\n\n:::{#exm-anova-ci}\nDetermine a $90\\%$ CI for the difference in mean average salary for councils in Scotland and England, based on the data available in @tbl-anova-samples-stats\n\nFor $\\alpha = 0.10,$ the critical value $t_{0.05, 16} = 1.7458837$ is found by looking in a table of $\\mathsf{t}$ critical values or by using `R`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# alt: qt(0.1/2, 16, lower.tail = FALSE)\nqt(1-0.1/2, df = 20 - 4) \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.745884\n```\n\n\n:::\n:::\n\n\nThen for the function $\\overline{x}_2 - \\overline{x_1},$ \n$$\n\\begin{aligned}\n(\\overline{x}_{Eng} - \\overline{x}_{Sco}) \\pm& t_{0.05, 16} \\sqrt{\\mathsf{MSE}} \\sqrt{\\frac{1}{m_{Eng}} + \\frac{1}{m_{Sco}}} \\\\\n& = (14.5 - 13.0) \\pm 1.7458837 \\sqrt{5.14366} \\sqrt{\\frac{1}{6} + \\frac{1}{5}} \\\\\n& = 1.5 \\pm 2.3976575\\,.\n\\end{aligned}\n$$\nThus, a $90\\%$ confidence interval for $\\mu_{Eng} - \\mu_{Sco}$ is $(-0.8977\\,, 3.898).$\n:::\n\n:::{.callout-warning}\n## Consider the following\n\nHow does the result in @exm-anova-ci compare to the $\\mathsf{t}$ method in @sec-compare-means-normpops-vars-unknown?\n:::",
    "supporting": [
      "05-anova_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}