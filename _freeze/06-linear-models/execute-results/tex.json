{
  "hash": "2dc414bc38edf13d2ad93be506135e2b",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n::: {.content-hidden when-format=\"pdf\"}\n\\newcommand{\\E}{\\mathbf{E}} \n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\corr}{corr}\n\\DeclareMathOperator{\\sd}{sd}\n\\newcommand{\\se}{\\mathsf{se}}\n:::\n\n\n\n\n\n\n\n# Linear regression {#sec-linear-models}\n\nRegression analysis allows us to study the relationship among two or more rvs. Typically, we are interested in the relationship between a response or dependent rv $Y$ and a covariate $X.$ The relationship between $X$ and $Y$ will be explained through a regression function, \n$$\n r(x) = \\E[Y \\mid X = x] = \\int y f(y\\mid x) dy \\,.\n$$\nIn particular, we shall assume that $r$ is linear,\n$$\n r(x) = \\beta_0 + \\beta_1 x\\,,\n$$ {#eq-linear-regression-function}\nand estimate the intercept $\\beta_0$ and slope $\\beta_1$ of this linear model from sample data \n$$\n (Y_1, X_1), \\dots, (Y_m, X_m) \\sim F_{Y,X}\\,.\n$$\n\n:::{.callout-tip}\n## Alternative lingo\n\nThe covariates $X$ are also called predictor variables, explanatory variables, independent variables, and/or features depending on who you are talking to.\n:::\n\n## Simple linear regression models {#sec-simple-linear-regression}\n\nThe simplest regression is when $X_i$ is one-dimensional and $r(x)$ is linear as in @eq-linear-regression-function. A linear regression posits the expected value of $Y_i$ is a linear function of the data $X_i,$ but that $Y$ deviates from its expected value by a random amount for fixed $x_i.$ \n\n:::{#def-linear-model}\nThe simple linear regression model relates a random response $Y_i$ to a set of independent variables $X_i,$\n$$\n Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i \\,,\n$$ {#eq-linear-model}\nwhere the intercept $\\beta_0$ and slope $\\beta_1$ are unknown parameters and the random deviation or random error $\\epsilon_i$ is a rv assumed to satisfy:\n \n1. $\\E[\\epsilon_i \\mid X_i = x_i] = 0,$\n2. $\\Var[\\epsilon_i \\mid X_i = x_i] = \\sigma^2$ does not depend on $x_i,$\n3. $\\epsilon_i$ and $\\epsilon_j$ are independent for $i, j = 1, \\dots, m.$   \n\n:::\n\nFrom the assumptions on $\\epsilon_i,$ the linear model @eq-linear-model implies\n$$\n \\E[Y_i \\mid X_i = x_i] = \\beta_0 + \\beta_1 x_i \\,.\n$$\nThus, if $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$ are estimators of $\\beta_0$ and $\\beta_1,$ then the fitted line is\n$$\n \\widehat{r}(x) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 x\n$$\nand the predicted or fitted value $\\widehat{Y}_i = \\widehat{r}(X_i)$ is an estimator for $\\E[Y_i \\mid X_i = x_i].$ The residuals are defined to be\n$$\n \\widehat{\\epsilon}_i = Y_i - \\widehat{Y}_i = Y_i - \\left( \\widehat{\\beta}_0 + \\widehat{\\beta}_1 X_i \\right) \\,.\n$$ {#eq-ls-residuals}\nThe residual sums of squares,\n$$\n \\mathsf{RSS} = \\sum_{i=1}^m \\widehat{\\epsilon}_i^2\\,,\n$$ {#eq-rss}\nmeasures how well the regression line $\\widehat{r}$ fits the data $(Y_1, X_1), \\dots, (Y_m, X_m).$ The least squares estimates of $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$ are the values that minimize the $\\mathsf{RSS}$ in @eq-rss.\n\n:::{#thm-least-squares-estimates}\nThe least squares estimates for $\\widehat{\\beta}_1$ and $\\widehat{\\beta}_0$ are given by, respectively,\n$$\n \\widehat{\\beta}_1 = \\frac{\\sum_{i=1}^m (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sum_{i=1}^m (X_i - \\overline{X})^2} = \\frac{S_{xy}}{S_{xx}} \\,,\n$$ {#eq-ls-slope}\nand\n$$\n \\widehat{\\beta}_0 = \\overline{Y} - \\widehat{\\beta}_1 \\overline{X}\\,.\n$$ {#eq-ls-intercept}\n:::\n\n@eq-rss is a function of $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$ from the definition of the residuals @eq-ls-residuals. Then @eq-ls-slope and @eq-ls-intercept follow by equating the partial derivatives of @eq-rss to zero. The $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$ are the unique solution to this linear system.\n\n:::{.callout-tip}\n## Alternative lingo\n\nThe $\\mathsf{RSS}$ is sometimes referred to as the error sum of squares and abbreviated $\\mathsf{SSE}$ (no, the order is not a typo).\n:::\n\n:::{#exm-linear-model-fit-residuals}\nIn @fig-linear-model-fit and @fig-linear-model-residuals, we consider the **Cherry Tree Data** (see @tbl-cherry-data) and discussion). We fit a least squares regression of timber volume (response variable) to the tree's diameter (independent variable). As you would expect, the timber yield increases with diameter.\n\nThe `R` code below can be used to calculate the least squares regression and residuals. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(trees)\ny <- trees$Volume\nx <- trees$Girth # NB: this is the diameter; data mislabeled!\nfit <- lm(y ~ x)\ne <- resid(fit)\nyhat <- predict(fit)\n```\n:::\n\n\nThe `fit` data frame contains the estimates for $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)           x \n -36.943459    5.065856 \n```\n\n\n:::\n:::\n\n\n\nBoth @fig-linear-model-fit and @fig-linear-model-residuals are scatter plots of the observed values $y.$ In @fig-linear-model-fit, the regression line $\\widehat{y}$ is plotted along with the residuals $\\widehat{\\epsilon}.$ In @fig-linear-model-residuals, the sample mean $\\overline{y}$ is plotted together with the deviations $y - \\overline{y}.$\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Linear regression (or least squares fit) of Volume to Diameter from the **Cherry Tree Data**. The vertical bars between the observed data point and the regression line indicate the error in the fit (the least squares residual). The residuals are squared and summed to yield the $\\mathsf{RSS}$ (alt: $\\mathsf{SSE}$).](06-linear-models_files/figure-pdf/fig-linear-model-fit-1.pdf){#fig-linear-model-fit}\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![The deviations about the sample mean $\\overline{y}.$ The sum of the squared deviations or $\\mathsf{SST}$ (total sum of squares) is a measure of the total variation in the observations.](06-linear-models_files/figure-pdf/fig-linear-model-residuals-1.pdf){#fig-linear-model-residuals}\n:::\n:::\n\n\n\n\n## Estimating $\\sigma^2$ for linear regressions {#sec-ls-estimate-var}\n\nThe parameter $\\sigma^2$ (the variance of the random deviation) determines the variability in the regression model.   \n\n:::{#thm-least-squares-var-estimate}\nAn unbiased estimate of $\\sigma^2$ is given by\n$$\n \\widehat{\\sigma}^2 = s^2 = \\frac{\\mathsf{RSS}}{m-2} = \\frac{1}{m-2} \\sum_{i=1}^m (y_i - \\widehat{y}_i)^2\\,.\n$$ {#eq-least-squares-var-estimate}\n:::   \n\nIn @fig-linear-model-sigma-large-v-small, we present a least squares regression of timber volume on both tree diameter and height (for the **Cherry Tree Data**). As expected, the regressions indicate the volume increases with both covariates. Estimates for the variance of the random deviation @eq-least-squares-var-estimate in both regression models, $\\sigma_{D}^2$ and $\\sigma_{H}^2,$ respectively, are computed to be $s^2_{D} = 18.08$ and $s^2_{H} = 179.48.$ Thus, we see that small variances lead to observations of $(x_i, y_i)$ that sit tightly around the regression line, in contrast to large variances that lead to a large cloud of points. \n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![For the **Cherry Tree Data**, we estimate the variance to be $s^2_{D} = 18.08$ (for Diameter) and $s^2_{H} = 179.48$ (for Height); small variances lead to observations of $(x_i, y_i)$ that sit tightly around the regression line, in contrast to large variances that lead to a large cloud of points.](06-linear-models_files/figure-pdf/fig-linear-model-sigma-large-v-small-1.pdf){#fig-linear-model-sigma-large-v-small}\n:::\n:::\n\n\n\n:::{.callout-warning}\n## Why do we lose two degrees of freedom?\n\nIn @thm-least-squares-var-estimate, the number in the denominator is the df associated with the $\\mathsf{RSS}$ and $s^2.$ To calculate $\\mathsf{RSS},$ you must estimate two parameters $\\beta_0$ and $\\beta_1,$ which results in the loss of two df. Hence the $m-2.$   \n:::   \n\nWe note to make inferences, the statistic \n$$\n S^2 = \\frac{\\mathsf{RSS}}{m-2}\n$$\nis an unbiased estimator or $\\sigma^2$ and the random variable \n$$\n \\frac{(m-2) S^2}{\\sigma^2} \\sim \\chi^2(m-2)\\,.\n$$\nMoreover, the statistic $S^2$ is independent of both $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1.$ \n\n\n## Inferences for least-squares parameters {#sec-inference-ls}\n\nIf $\\epsilon_i$ in @eq-linear-model is assumed to be normally distributed, then we can derive the sampling distributions of the estimators $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1.$ Hence, we can use these sampling distributions to make inferences about the parameters $\\beta_0$ and $\\beta_1.$ \n\nProvided iid $\\epsilon_i \\mid X_i \\sim \\mathsf{N}(0, \\sigma^2),$ the least-squares estimators possess the following properties.   \n\n1. Both $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$ are normally distributed. \n2. Both $\\widehat{\\beta}_0$ and $\\widehat{\\beta}_1$ are unbiased, i.e., $\\E[\\widehat{\\beta}_i] = \\beta_i$ for $i = 0,1.$ \n3. $\\Var[\\widehat{\\beta}_0] = c_{00} \\sigma^2$ where $c_{00} = \\sum_{i=1}^m x_i^2 / (m S_{xx}).$\n4. $\\Var[\\widehat{\\beta}_1] = c_{11} \\sigma^2$ where $c_{11} = 1/S_{xx}.$ \n5. $\\Cov[\\widehat{\\beta}_0, \\widehat{\\beta}_1] = c_{01} \\sigma^2$ where $c_{01} = - \\overline{x} / S_{xx}.$   \n\nThese properties can be determined by working directly from @eq-ls-slope and @eq-ls-intercept.   \n\n\n:::{#prp-htest-ls-betas}\nConsider $H_0 : \\beta_i = \\beta_{i0}.$ The test statistic is\n$$\n T = \\frac{\\widehat{\\beta}_i - \\beta_{i0}}{S\\sqrt{c_{ii}}} \\,.\n$$ {#eq-htest-ls-betas-statistic}\n\nFor a hypothesis test at level $\\alpha,$ we use the following procedure:\n\nIf $H_a : \\beta_i > \\beta_{i0},$ then $P$-value is the area under $\\mathsf{t}(m-2)$ to the right of $t.$\n\nIf $H_a : \\beta_i < \\beta_{i0},$ tthen $P$-value is the area under $\\mathsf{t}(m-2)$ to the left of $t.$\n\nIf $H_a : \\beta_i \\neq \\beta_{i0},$ then $P$-value is twice the area under $\\mathsf{t}(m-2)$ to the right of $|t|.$\n::: \n\n\nA confidence interval for $\\beta_i,$ based on the statistic @eq-htest-ls-betas-statistic, can be given following the procedures in @sec-inference-single-sample.\n\n:::{#prp-ci-ls-betas}\nA $100(1-\\alpha)\\%$ CI for $\\beta_i$ is given by \n$$\n \\widehat{\\beta}_i \\pm t_{\\alpha/2, m-2} S \\sqrt{c_{ii}} \\,.\n$$\n:::\n\n\n## Correlation {#sec-correlation}\n\nLet $(X_1, Y_1), \\dots, (X_m, Y_m)$ denote a random sample from a bivariate normal distribution with $\\E[X_i] = \\mu_X,$ $\\E[Y_i] = \\mu_Y,$ $\\Var[X_i] = \\sigma_X^2,$ $\\Var[Y_i] = \\sigma_Y^2,$ and correlation coefficient $\\rho.$ The sample correlation coefficient is given by,\n$$\n r = \\frac{\\sum_{i=1}^m (X_i - \\overline{X})(Y_i - \\overline{Y})}{\\sqrt{\\sum_{i=1}^m (X_i - \\overline{X})^2 \\sum_{i=1}^m (Y_i - \\overline{Y})^2}}\\,,\n$$ {#eq-sample-correlation-statistic}\nwhich can be rewritten in terms of $S_{xx},$ $S_{xy},$ and $S_{yy}$:\n$$\n r = \\frac{S_{xy}}{\\sqrt{S_{xx}S_{yy}}} = \\widehat{\\beta}_1 \\sqrt{\\frac{S_{xx}}{S_{yy}}}\\,,\n$$\nusing @eq-ls-slope and we see that $r$ and $\\widehat{\\beta}_1$ have the same sign. A $|r|$ close to $1$ means that the regression line is a good fit to the data, and, similarly, an $|r|$ close to $0$ means a poor fit to the data. Note that the correlation coefficient (and the least squares regression) are only suitable for describing *linear* relationships; a nonlinear relationship can also yield $r$ near zero (see @fig-linear-model-correlation).\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Correlations range from $-1$ to $1$ with $|r|=1$ indicating a strong linear relationship and $r$ near zero indicating the absence of a linear relationship.](06-linear-models_files/figure-pdf/fig-linear-model-correlation-1.pdf){#fig-linear-model-correlation}\n:::\n:::\n\n\n\n## Prediction using linear models \n\nOnce a model is fit, it can be used to predict a value of $y$ for a given $x.$ However, the model only gives the most likely value of $y$; a corresponding prediction interval is usually more appropriate. \n\n:::{#prp-prediction-interval}\nA $100(1-\\alpha)\\%$ prediction interval for an actual value of $Y$ when $x = x^*$ is given by \n$$\n (\\widehat{\\beta}_0 + \\widehat{\\beta}_1 x^*) \\pm t_{\\alpha/2, m-2} S \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\overline{x})^2}{S_{xx}}} \\,.\n$$\n:::\n\n:::{.callout-warning}\n## Prediction versus confidence intervals\n\nThe prediction interval is different from the confidence interval for expected $Y.$ Note that the length of the *confidence interval* for $\\E[Y]$ when $x=x^*$ is given by \n$$\n 2 \\cdot t_{\\alpha/2} S  \\sqrt{\\frac{1}{n} + \\frac{(x^* - \\overline{x})^2}{S_{xx}}}\n$$\nwhereas the length for the *prediction interval* of $Y$ is \n$$\n 2 \\cdot t_{\\alpha/2} S  \\sqrt{1 + \\frac{1}{n} + \\frac{(x^* - \\overline{x})^2}{S_{xx}}} \\,.\n$$\nThus the prediction intervals for an actual value of $Y$ are longer than the confidence intervals for $\\E[Y]$ if both are determined for the same value $x^*.$   \n:::   \n\nThe linear model \n$$\n \\E[ Y \\mid X = x ] = \\beta_0 + \\beta_1 x \\,, \n$$\nassumes that the conditional expectation of $Y$ for a fixed value of $X$ is a linear function of the $x$ value. If we assume that $(X,Y)$ has a bivariate normal distribution, then \n$$\n \\beta_1 = \\frac{\\sigma_Y}{\\sigma_X} \\rho \\,,\n$$\nand thus, for the simple hypothesis tests we have considered (@tbl-htest-null-alt-forms), statistical tests for $\\beta_1$ and $\\rho$ are equivalent.\n\n",
    "supporting": [
      "06-linear-models_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}