{
  "hash": "85187d4bd8a65a6539e7856ff0be6891",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n\n\n\n# Inferences based on two samples {#inference-two-samples}\n\nWe consider inferences---estimators, confidence intervals, and hypothesis testing---for comparing means, proportions, and variances based on two independent samples from different populations, respectively, in Sections \\@ref(compare-means), \\@ref(compare-proportions), \\@ref(compare-variances). We also consider inferences when the samples are not independent, so-called paired samples, in Section \\@ref(compare-paired-samples). \n\n## Comparing means {#compare-means}\n\nLet us assume that we have two normal populations with iid samples \n\\begin{equation*}\n X_1, \\dots, X_m \\sim \\mathsf{N}(\\mu_X, \\sigma_X^2)\n\\end{equation*}\nand \n\\begin{equation*}\n Y_1, \\dots, Y_n \\sim \\mathsf{N}(\\mu_Y, \\sigma_Y^2)\n\\end{equation*}\nand, moreover, that the $X$ and $Y$ samples are independent of one another. When comparing the means of two populations, the quantity of interest is the difference: $\\mu_X - \\mu_Y$. \n\n\n\n::: {.cell}\n\n```{.proposition .cell-code}\nIf we consider the sample means $\\overline{X}$ and $\\overline{Y}$, then the mean of the variable $\\overline{X} - \\overline{Y}$ is,\n\\begin{equation*}\n \\mu_{\\overline{X} - \\overline{Y}} \n= \\E \\left[ \\overline{X} - \\overline{Y} \\right] = \\mu_X - \\mu_Y\\,,\n\\end{equation*}\nand the variance is,\n\\begin{equation*}\n \\sigma_{\\overline{X} - \\overline{Y}}^2\n= \\Var \\left[ \\overline{X} - \\overline{Y} \\right]\n= \\frac{\\sigma_X^2}{m} + \\frac{\\sigma_Y^2}{n} \\,.\n\\end{equation*}\n```\n:::\n\n\n\nProposition \\@ref(prp:qoi-diff-pop-means) follows directly from the definition of the sample mean in \\@ref(eq:sample-mean) and properties of expectation and variance. If our parameter of interest is \\begin{equation*}\n \\theta = \\mu_1 - \\mu_2\\,,\n\\end{equation*}\nthen its estimator,\n\\begin{equation*}\n \\widehat{\\theta} = \\overline{X} - \\overline{Y}\\,,\n\\end{equation*}\nis normally distributed with mean and variance given by Proposition \\@ref(prp:qoi-diff-pop-means). If the sample sizes $m$ and $n$ are large, then the estimator is approximately normally distributed by the Central Limit Theorem regardless of the population. We now discuss CIs and hypothesis tests for comparing population means $\\theta = \\mu_X - \\mu_Y$. We consider three cases when comparing means:\n\n1. [normal populations when the variances $\\sigma_X^2$ and $\\sigma_Y^2$ are known](#compare-means-normpops-vars-known),\n2. [any populations with unknown variances $\\sigma_X^2$ and $\\sigma_Y^2$, when the sample sizes $m$ and $n$ are large](#compare-means-large-samples),\n3. [normal populations when the variances $\\sigma_X^2$ and $\\sigma_Y^2$ are unknown, when the sample sizes $m$ and $n$ are small](#compare-means-normpops-vars-unknown),\n\nnoting that the development primarily reflects that of Section \\@ref(estimating-means).\n\n\n### Comparing means of normal populations when variances are known {#compare-means-normpops-vars-known}\n\nWhen $\\sigma_X^2$ and $\\sigma_Y^2$ are known, standardizing $\\overline{X} - \\overline{Y}$ yields the standard normal variable:\n\\begin{equation}\n Z = \\frac{\\overline{X} - \\overline{Y} - (\\mu_X - \\mu_Y)}{\\sqrt{\\frac{\\sigma_X^2}{m} + \\frac{\\sigma_Y^2}{n}}}\\quad \\sim \\mathsf{N}(0,1)\\,.\n (\\#eq:compare-means-standard-trans)\n\\end{equation}  \nInferences proceed by treating the parameter of interest $\\theta$ as in the single sample case using the test statistic \\@ref(eq:compare-means-standard-trans).  \n\n\n\n::: {.cell}\n\n```{.proposition .cell-code}\nA $100(1-\\alpha)\\%$ CI for the parameter $\\theta = \\mu_X - \\mu_Y$ based on samples of size $m$ from a normal population $\\mathsf{N}(\\mu_X, \\sigma_X^2)$ and of size $n$ from $\\mathsf{N}(\\mu_Y, \\sigma_Y^2)$  with known variances, is given by\n\\begin{equation*}\n (\\overline{x} - \\overline{y}) \\pm z_{\\alpha/2} \n \\cdot \\sqrt{\\frac{\\sigma_X^2}{m} + \\frac{\\sigma_Y^2}{n}} \\,.\n\\end{equation*}\n```\n:::\n\n::: {.cell}\n\n```{.proposition .cell-code}\nAssume that we sample iid $X_1, \\dots, X_m \\sim \\mathsf{N}(\\mu_X, \\sigma_X^2)$ and iid $Y_1, \\dots, Y_n \\sim \\mathsf{N}(\\mu_Y, \\sigma_Y^2)$ and that the $X$ and $Y$ samples are independent. \n\nConsider $H_0 : \\mu_X - \\mu_Y = \\theta_0$. The test statistic is\n\\begin{equation}\n Z = \\frac{\\overline{X} - \\overline{Y} - \\theta_0}{\\sqrt{\\frac{\\sigma_{X}^2}{m} + \\frac{\\sigma_{Y}^2}{n}}}\\,.\n (\\#eq:htest-compare-means-normpops-vars-known-statistic)\n\\end{equation}\n\nFor a hypothesis test at level $\\alpha$, we use the following procedure:\n\nIf $H_a : \\mu_X - \\mu_Y > \\theta_0$, then $P = 1 - \\Phi(z)$, i.e., upper-tail $R = \\{z > z_{\\alpha}\\}$.\n\nIf $H_a : \\mu_X - \\mu_Y < \\theta_0$, then $P = \\Phi(z)$, i.e., lower-tail $R = \\{z < - z_{\\alpha}\\}$. \n\nIf $H_a : \\mu_X - \\mu_Y \\neq \\theta_0$, then $P = 2(1-\\Phi(|z|))$, i.e., two-tailed $R = \\{|z| > z_{\\alpha/2}\\}$.\n```\n:::\n\n\n\n### Comparing means when the sample sizes are large {#compare-means-large-samples}\n\nWhen the samples are large, the assumptions about the normality of the populations and knowledge of the variances $\\sigma_X^2$ and $\\sigma_Y^2$ can be relaxed. For sufficiently large $m$ and $n$, the difference of the sample means, $\\overline{X} - \\overline{Y}$, has approximately a normal distribution for any underlying population distributions by the Central Limit Theorem. Moreover, if $m$ and $n$ are large enough, replacing the population variances with the sample variances $S_X^2$ and $S_Y^2$ will not increase the variability of the estimator or the test statistic too much. \n\n\n\n::: {.cell}\n\n```{.proposition .cell-code}\nFor $m$ and $n$ sufficiently large, an approximate $100(1-\\alpha)\\%$ CI for $\\mu_X - \\mu_Y$ for two samples from populations with any underlying distribution is given by\n\\begin{equation*}\n (\\overline{x} - \\overline{y}) \\pm z_{\\alpha/2} \n \\cdot \\sqrt{\\frac{s_{X}^2}{m} + \\frac{s_{Y}^2}{n}}\n\\end{equation*}\n```\n:::\n\n::: {.cell}\n\n```{.proposition .cell-code}\nUnder the same assumptions and procedures as in Proposition \\@ref(prp:htest-compare-means-normpops-vars-known), a large-sample, i.e., $m > 40$ and $n > 40$, test statistic,\n\\begin{equation*}\n Z = \\frac{\\overline{X} - \\overline{Y} - \\theta_0}{\\sqrt{\\frac{S_{X}^2}{m} + \\frac{S_{Y}^2}{n}}}\\,,\n\\end{equation*}\ncan be used in place of \\@ref(eq:htest-compare-means-normpops-vars-known-statistic) for hypothesis testing. \n```\n:::\n\n\n\n### Comparing means of normal populations when variances are unknown and the sample size is small {#compare-means-normpops-vars-unknown}\n\nIf $\\sigma_X$ and $\\sigma_Y$ are unknown and either sample is small (e.g., $m < 30$ or $n <30$), but both populations are normally distributed, then we can use Student's $\\mathsf{t}$ distribution to make inferences. We provide the following theorem without proof.  \n\n\n\n::: {.cell}\n\n```{.theorem .cell-code}\nWhen both population distributions are normal, the standardised variable \n\\begin{equation*}\nT = \\frac{\\overline{X}-\\overline{Y} - (\\mu_X - \\mu_Y)}{\\sqrt{\\frac{S_X^2}{m} + \\frac{S_Y^2}{n}}} \n\\quad \\sim \\mathsf{t}(\\nu)\n\\end{equation*}\nwhere the df $\\nu$ is estimated from the data. Namely, $\\nu$ is given by (round $\\nu$ down to the nearest integer):\n\\begin{equation}\n \\nu = \\frac{ \\left( \\frac{s_X^2}{m} + \\frac{s_Y^2}{n} \\right)^2}{\\frac{(s_X^2 / m)^2}{m-1} + \\frac{(s_Y^2/n)^2}{n-1}} \n = \\frac{ \\left( s_{\\overline{X}}^2 + s_{\\overline{Y}}^2 \\right)^2}{\\frac{s_{\\overline{X}}^4}{m-1} + \\frac{s_{\\overline{Y}}^4}{n-1}}\n (\\#eq:dist-t-compare-means-normpops-nu)\n\\end{equation}\nwhere $s_X^2$ and $s_Y^2$ are point estimators of the sample variances; alternatively, we see that the formula \\@ref(eq:dist-t-compare-means-normpops-nu) can also be written in terms of the standard error of the sample means:\n\\begin{equation*}\n s_{\\overline{X}} = \\frac{s_X}{\\sqrt{m}} \n \\quad \\text{and} \\quad \\qquad \n s_{\\overline{Y}} = \\frac{s_Y}{\\sqrt{n}} \\,.\n\\end{equation*}\n```\n:::\n\n\n\nThe formula \\@ref(eq:dist-t-compare-means-normpops-nu) for the data-driven choice of $\\nu$ calls for the computation of the standard error of the sample means. \n\n\n\n::: {.cell}\n\n```{.proposition .cell-code}\nA $100(1-\\alpha)\\%$ CI for $\\mu_X - \\mu_Y$ for two samples of size $m$ and $n$ from normal populations where the variances are unknown is given by \n\\begin{equation*}\n (\\overline{x} - \\overline{y}) \\pm t_{\\alpha/2, \\nu} \\sqrt{ \\frac{s_X^2}{m} + \\frac{s_Y^2}{n}}\\,,\n\\end{equation*}\nwhere we recall that $t_{\\alpha/2, \\nu}$ is the $\\alpha/2$ critical value of $\\mathsf{t}(\\nu)$ with $\\nu$ given by \\@ref(eq:dist-t-compare-means-normpops-nu).\n```\n:::\n\n::: {.cell}\n\n```{.proposition .cell-code}\nAssume that we sample iid $X_1, \\dots, X_m$ and iid $Y_1, \\dots, Y_n$ from normal populations with unknown variances and means $\\mu_X$ and $\\mu_Y$, respectively, and that the $X$ and $Y$ samples are independent. \n\nConsider $H_0 : \\mu_X - \\mu_Y = \\theta_0$. The test statistic is\n\\begin{equation}\n T = \\frac{\\overline{X} - \\overline{Y} - \\theta_0}{\\sqrt{\\frac{S_{X}^2}{m} + \\frac{S_{Y}^2}{n}}}\\,.\n (\\#eq:htest-compare-means-normpops-vars-unknown-statistic)\n\\end{equation}\n\nFor a hypothesis test at level $\\alpha$, we use the following procedure:\n\nIf $H_a : \\mu_X - \\mu_Y > \\theta_0$, then $P$-value is the area under $\\mathsf{t}(\\nu)$ to the right of $t$, i.e., upper-tail $R = \\{t > t_{\\alpha,\\nu}\\}$.\n\nIf $H_a : \\mu_X - \\mu_Y < \\theta_0$, then $P$-value is the area under $\\mathsf{t}(\\nu)$ to the left of $t$, i.e., lower-tail $R = \\{t < - t_{\\alpha,\\nu}\\}$. \n\nIf $H_a : \\mu_X - \\mu_Y \\neq \\theta_0$, then $P$-value is twice the area under $\\mathsf{t}(\\nu)$ to the right of $|t|$, i.e., two-tailed $R = \\{|t| > t_{\\alpha/2, \\nu}\\}$.\n\nHere $\\nu$ is given by \\@ref(eq:dist-t-compare-means-normpops-nu).\n```\n:::\n\n\n\n\nIf the variances of the normal populations are unknown but are the same, $\\sigma_X^2 = \\sigma_Y^2$, then deriving CIs and test statistics for comparing the means can be simplified by considering a combined or pooled estimator for the single parameter $\\sigma^2$. If we have two samples from populations with variance $\\sigma^2$, each sample provides an estimate for $\\sigma^2$. That is, $S_X^2$, based on the $m$ observations of the first sample, is one estimator for $\\sigma^2$ and another is given by $S_Y^2$, based on $n$ observations of the second sample. The correct way to combine these two estimators into a single estimator for the sample variance is to consider the **pooled estimator** of $\\sigma^2$, \n\\begin{equation}\n S_{\\mathsf{p}}^2 = \\frac{m-1}{m+n-2} S_X^2 + \\frac{n-1}{m+n-2} S_Y^2 \\,.\n (\\#eq:pooled-sample-var)\n\\end{equation}\nThe pooled estimator is a weighted average that adjusts for differences between the sample sizes $m$ and $n$.^[If $m \\neq n$, then the estimator with *more* samples will contain *more* information about the parameter $\\sigma^2$. Thus, the simple average $(S_X^2 + S_Y^2)/2$ wouldn't be fair, would it?]\n\n\n\n\n::: {.cell}\n\n```{.proposition .cell-code}\nA $100(1-\\alpha)\\%$ CI for $\\mu_X - \\mu_Y$ for two samples of size $m$ and $n$ from normal populations where the variance $\\sigma^2$ is unknown is given by \n\\begin{equation*}\n (\\overline{x} - \\overline{y}) \\pm t_{\\alpha/2, m  + n - 2} \\cdot \\sqrt{ s_{\\mathsf{p}}^2 \\left( \\frac{1}{m} + \\frac{1}{n} \\right)} \\,,\n\\end{equation*}\nwhere we recall that $t_{\\alpha/2, m+n-2}$ is the $\\alpha/2$ critical value of the $\\mathsf{t}(\\nu)$ with $\\nu = m + n - 2$ df. \n```\n:::\n\n\n\nSimilarly, one can consider a pooled $\\mathsf{t}$ test, i.e., a hypothesis test based on the pooled estimator for the variance as opposed to the two-sample $\\mathsf{t}$ test in Proposition \\@ref(prp:htest-compare-means-normpops-vars-unknown). In the case of a pooled $\\mathsf{t}$ test, the test statistic\n\\begin{equation*}\n T = \\frac{\\overline{X} - \\overline{Y} - \\theta_0}{\\sqrt{S_{\\mathsf{p}}^2 \\left(\\frac{1}{m} + \\frac{1}{n}\\right)}}\\,,\n\\end{equation*}\nwith the pooled estimator of the variance, replaces \\@ref(eq:htest-compare-means-normpops-vars-unknown-statistic) in Proposition \\@ref(prp:htest-compare-means-normpops-vars-unknown) and the same procedures are followed for determining the $P$-value with $\\nu = m+n-2$ in place of \\@ref(eq:dist-t-compare-means-normpops-nu). If you have reasons to believe that $\\sigma_X^2 = \\sigma_Y^2$, these pooled $\\mathsf{t}$ procedures are appealing because $\\nu$ is very easy to compute. \n\n:::{.warningblock data-latex=\"\"}\nPooled $t$ procedures are not robust if the assumption of equalised variance is violated. Theoretically, you could first carry out a statistical test $H_0 : \\sigma_X^2 = \\sigma_Y^2$ on the equality of variances and then use a pooled $\\mathsf{t}$ procedure if the null hypothesis is not rejected. However, there is no free lunch: the typical $\\mathsf{F}$ test for equal variances (see Section \\@ref(compare-variances)) is sensitive to normality assumptions. The two sample $\\mathsf{t}$ procedures, with the data-driven choice of $\\nu$ in \\@ref(eq:dist-t-compare-means-normpops-nu), are therefore recommended unless, of course, you have a very compelling reason to believe $\\sigma_X^2 = \\sigma_Y^2$. \n:::   \n\n\n## Comparing paired samples {#compare-paired-samples}\n\nThe preceding analysis for comparing population means was based on the assumption that a random sample $X_1, \\dots, X_n$ is drawn from a distribution with mean $\\mu_X$ and that a completely independent random sample $Y_1, \\dots, Y_n$ is drawn from a distribution with mean $\\mu_Y$. Some situations, e.g., comparing observations before and after a treatment or exposure, necessitate the consideration of paired values. \n\nConsider a random sample of iid pairs\n\\begin{equation*}\n(X_1, Y_1), \\dots, (X_n, Y_n)\n\\end{equation*}\nwith $\\E[X_i] = \\mu_X$ and $\\E[Y_i] = \\mu_Y$. If we are interested in making inferences about the difference $\\mu_X - \\mu_Y$, then the paired differences\n\\begin{equation*}\nD_i = X_i - Y_i \\,,\\quad  i=1, \\dots, n\\,,\n\\end{equation*}\nconstitute a sample with mean $\\mu_D = \\mu_X - \\mu_Y$ that can be treated using single-sample CIs and tests, e.g., see Section \\@ref(mean-normal-var-unknown).  \n\n\n## Comparing proportions {#compare-proportions}\n\nConsider a population containing a proportion $p_X$ of individuals satisfying a given property. For a sample of size $m$ from this population, we denote the sample proportion by $\\widehat{p}_X$. Likewise, we consider a population containing a proportion $p_Y$ of individuals satisfying the same given property. For a sample of size $n$ from this population, we denote the sample proportion by $\\widehat{p}_Y$. We assume the samples from the $X$ and $Y$ populations are independent. The natural estimator for the difference in population proportions $p_X - p_Y$ is the difference in the sample proportions $\\widehat{p}_X - \\widehat{p}_Y$. \n\nProvided the samples are much smaller than the population sizes (i.e., the populations are about $20$ times larger than the samples), \n\\begin{equation*}\n \\mu_{(\\widehat{p}_X - \\widehat{p}_Y)} = \\E[\\widehat{p}_X - \\widehat{p}_Y] = p_X - p_Y\\,,\n\\end{equation*}\nand \n\\begin{equation*}\n \\sigma_{(\\widehat{p}_X - \\widehat{p}_Y)}^2 = \\Var[\\widehat{p}_X - \\widehat{p}_Y] \n = \\frac{p_X(1-p_X)}{m} + \\frac{p_Y(1-p_Y)}{n}\\,,\n\\end{equation*}\nbecause the count of individuals satisfying the given property in each population will be independent draws from $\\mathsf{Binom}(m, p_X)$ and $\\mathsf{Binom}(n, p_Y)$, respectively. Further, if $m$ and $n$ are large (e.g., $m \\geq 30$ and $n \\geq 30$), then $\\widehat{p}_X$ and $\\widehat{p}_Y$ are (approximately) normally distributed. Standardizing $\\widehat{p}_X - \\widehat{p}_Y$,\n\\begin{equation*}\n Z = \\frac{\\widehat{p}_X - \\widehat{p}_Y - (p_X - p_Y)}{\\sqrt{\\frac{p_X(1-p_X)}{m} + \\frac{p_Y(1-p_Y)}{n}}}\n \\quad \\sim \\mathsf{N}(0,1)\\,.\n\\end{equation*}\nA CI for $\\widehat{p}_X - \\widehat{p}_Y$ then follows from the large-sample CI considered in Section \\@ref(mean-large-sample).  \n\n\n\n::: {.cell}\n\n```{.proposition .cell-code}\nAn approximate $100(1-\\alpha)\\%$ CI for $p_X - p_Y$ is given by\n\\begin{equation*}\n \\widehat{p}_X - \\widehat{p}_Y \\pm z_{\\alpha/2}\\sqrt{\\frac{\\widehat{p}_X (1 - \\widehat{p}_X)}{m} + \\frac{\\widehat{p}_Y (1 - \\widehat{p}_Y)}{n}}\\,,\n (\\#eq:ci-compare-proportions)\n\\end{equation*}\nand, as a rule of thumb, can be reliably used if $m \\widehat{p}_X$, $m (1 - \\widehat{p}_X)$, $n \\widehat{p}_Y$, and $n (1-\\widehat{p}_Y)$ are greater than or equal to $10$. \n```\n:::\n\n\n\nProposition \\@ref(prp:ci-compare-proportions) does not pool the estimators for the population proportions. However, if we are considering a hypothesis test concerning the equality of the population proportions with the null hypothesis \n\\begin{equation*}\nH_0 : p_X - p_Y = 0 \\,,\n\\end{equation*}\nthen we assume $p_X = p_Y$ as our default position. Therefore, as a matter of consistency, we should replace the standard error in \\@ref(eq:ci-compare-proportions) with a pooled estimator for the standard error of the population proportion,\n\\begin{equation*}\n \\widehat{p} = \\frac{m}{m + n} \\widehat{p}_X + \\frac{n}{m + n} \\widehat{p}_Y \\,.\n\\end{equation*}\n\n\n\n\n::: {.cell}\n\n```{.proposition .cell-code}\nAssume that $m \\widehat{p}_X$, $m (1-\\widehat{p}_X)$, $n\\widehat{p}_Y$, $n(1-\\widehat{p}_Y)$ are all greater than $10$. \n\nConsider $H_0 : p_X - p_Y = 0$. The test statistic is\n\\begin{equation*}\n Z = \\frac{\\widehat{p}_X - \\widehat{p}_Y}{\\sqrt{\\widehat{p} (1 - \\widehat{p}) \\left( \\frac{1}{m} + \\frac{1}{n} \\right)}} \\,.\n\\end{equation*}\n\nFor a hypothesis test at level $\\alpha$, we use the following procedure:\n\nIf $H_a : p_X - p_Y > 0$, then $P = 1 - \\Phi(z)$, i.e., upper-tail $R = \\{z > z_{\\alpha}\\}$.\n\nIf $H_a : p_X - p_Y < 0$, then $P = \\Phi(z)$, i.e., lower-tail $R = \\{z < - z_{\\alpha}\\}$.\n\nIf $H_a : p_X - p_Y \\neq 0$, then $P = 2(1-\\Phi(|z|))$, i.e., two-tailed $R = \\{|z| > z_{\\alpha/2}\\}$.\n```\n:::\n\n\n\n## Comparing variances {#compare-variances}\n\nFor a random sample \n\\begin{equation*}\nX_1, \\dots, X_m \\sim \\mathsf{N}(\\mu_X, \\sigma_X^2)\n\\end{equation*}\nand an independent random sample \n\\begin{equation*}\nY_1, \\dots, Y_n \\sim \\mathsf{N}(\\mu_Y, \\sigma_Y^2)\\,,\n\\end{equation*}\nthe rv\n\\begin{equation}\n F = \\frac{S_X^2 / \\sigma_X^2}{S_Y^2 / \\sigma_Y^2} \\quad \\sim \\mathsf{F}(m-1, n-1)\\,,\n (\\#eq:F-test-statistic)\n\\end{equation}\nthat is, $F$ has an $\\mathsf{F}$ distribution with df $\\nu_1 = m-1$ and $\\nu_2 = n-1$. The statistic $F$ in \\@ref(eq:F-test-statistic) comprises the *ratio* of variances $\\sigma_X^2 / \\sigma_Y^2$ and not the difference; therefore, the plausibility of $\\sigma_X^2 = \\sigma_Y^2$ will be based on how much the ratio differs from $1$. \n\n\n\n::: {.cell}\n\n```{.proposition .cell-code}\nFor the null hypothesis $H_0 : \\sigma_X^2 = \\sigma_Y^2$, the test statistic to consider is:\n\\begin{equation*}\nf = \\frac{s_X^2}{s_Y^2}\n\\end{equation*}\nand the $P$-values are determined by the $\\mathsf{F}(m-1, n-1)$ curve where $m$ and $n$ are the respective sample sizes.\n```\n:::\n\n\n\nA $100(1-\\alpha)\\%$ CI for the ratio $\\sigma_X^2 / \\sigma_Y^2$ is based on forming the probability,\n\\begin{equation*}\n P(F_{1-\\alpha/2, \\nu_1, \\nu_2} < F < F_{\\alpha/2, \\nu_1, \\nu_2}) = 1 - \\alpha\\,,\n\\end{equation*}\nwhere $F_{\\alpha/2, \\nu_1, \\nu_2}$ is the $\\alpha/2$ critical value from the $\\mathsf{F}(\\nu_1 = m-1, \\nu_2 = n-1)$ distribution. Substituting \\@ref(eq:F-test-statistic) with point estimates for $F$ and manipulating the inequalities it is possible to isolate the ratio $\\sigma_X^2 / \\sigma_Y^2$, \n\\begin{equation*}\n P \\left( \\frac{1}{F_{\\alpha/2, \\nu_1, \\nu_2}} \\frac{s_X^2}{s_Y^2} < \\frac{\\sigma_X^2}{\\sigma_Y^2} < \\frac{1}{F_{1-\\alpha/2, \\nu_1, \\nu_2}} \\frac{s_X^2}{s_Y^2} \\right) \n = 1 - \\alpha \\,.\n\\end{equation*}\n\n\n\n::: {.cell}\n\n```{.proposition .cell-code}\nA $100(1-\\alpha)\\%$ CI for the ratio of population variances $\\sigma_X^2 / \\sigma_Y^2$ is given by\n\\begin{equation*}\n \\left(F_{\\alpha/2, m-1, n-1}^{-1} s_X^2 / s_Y^2 \\,,  F_{1-\\alpha/2, m-1, n-1}^{-1} s_X^2 / s_Y^2 \\right)\\,.\n\\end{equation*}\n```\n:::\n\n::: {.cell}\n\n```{.proposition .cell-code}\nAssume the population distributions are normal and the random samples are independent of one another.\n\nConsider $H_0 : \\sigma_X^2 = \\sigma_Y^2$. The test statistic is\n\\begin{equation*}\n F = S_X^2 / S_Y^2 \\,.\n\\end{equation*}\n\nFor a hypothesis test at level $\\alpha$, we use the following procedure:\n\nIf $H_a : \\sigma_X^2 > \\sigma_Y^2$, then $P$-value is $A_R = {}$ area under the $\\mathsf{F}(m-1, n-1)$ curve to the right of $f$.\n\nIf $H_a : \\sigma_X^2 < \\sigma_Y^2$, then $P$-value is $A_L = {}$ area under the $\\mathsf{F}(m-1, n-1)$ curve to the left of $f$.\n\nIf $H_a : \\sigma_X^2 \\neq \\sigma_Y^2$, then $P$-value is $2 \\cdot \\min(A_R, A_L)$.\n```\n:::",
    "supporting": [
      "04-infer-two-samples_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{\"knit_meta_id\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]}},\"value\":[{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"booktabs\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"longtable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"array\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"multirow\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"wrapfig\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"float\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"colortbl\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"pdflscape\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"tabu\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttable\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"threeparttablex\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"ulem\"]},{\"type\":\"character\",\"attributes\":{},\"value\":[\"normalem\"]},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"makecell\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]},{\"type\":\"list\",\"attributes\":{\"names\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"name\",\"options\",\"extra_lines\"]},\"class\":{\"type\":\"character\",\"attributes\":{},\"value\":[\"latex_dependency\"]}},\"value\":[{\"type\":\"character\",\"attributes\":{},\"value\":[\"xcolor\"]},{\"type\":\"NULL\"},{\"type\":\"NULL\"}]}]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}