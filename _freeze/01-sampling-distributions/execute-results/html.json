{
  "hash": "08fe7cb34e5200be3138e5f295197e06",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n::: {.content-hidden when-format=\"pdf\"}\n\\newcommand{\\E}{\\mathbf{E}} \n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\corr}{corr}\n\\DeclareMathOperator{\\sd}{sd}\n\\newcommand{\\se}{\\mathsf{se}}\n:::\n\n\n\n# Sampling distributions {#sec-sampling-distributions}\n\nA *statistic* is a quantity that can be calculated from sample data. Before observing data, a statistic is an unknown quantity and is, therefore, a rv. \n\n::: {#def-statistic}\n## Statistic \n\nLet $X_1, \\dots, X_n$ be observable rvs and let $g$ be an arbitrary real-valued function of $n$ random variables. The rv $$T = g(X_1, \\dots, X_n)$$ is a statistic. \n:::\n\nWe refer to the probability distribution for a statistic as a sampling distribution. The sampling distribution illustrates how the statistic will vary across possible sample data. The sampling distribution contains information about the values a statistic is likely to assume and how likely it is to assume those values prior to observing data. \n\n::: {#def-sampling-dist}\n## Sampling distribution\n\nSuppose rvs $X_1, \\dots, X_n$ are a random sample from $F(\\theta),$ a distribution depending a parameter $\\theta$ whose value is uknown. Let the rv $$T = g(X_1, \\dots, X_n, \\theta)$$ be a function of $X_1, \\dots, X_n$ and (possibly) $\\theta$. The distribution of $T$ (given $\\theta$) is the sampling distribution of $T$. \n:::\n\nThe sampling distribution of $T$ is derived from the distribution of the random sample. Often we will be interested in a statistic $T$ that is an estimator for a parameter $\\theta$ (that is, $T$ will not depend on $\\theta$). \n\nIn what follows, we review several special families of distributions that are widely used in probability and statistics. These special families of distributions will be indexed by one or parameters. \n\n## Uniform distribution {#sec-uniform-distribution}\n\nThe uniform distribution places equal on uniform weight on the items being sampled.\n\n::: {#def-uniform-dist}\n## Uniform distribution\n\nA continuous rv $X$ has a uniform distribution on $[a,b]$ with $a<b,$ if $X$ has pdf\n$$\n f(x; a,b) = \\frac{1}{b-a}\\,, \n \\quad a < x < b\\,,\n$$\n or zero otherwise. \nWe write $X \\sim \\mathsf{Unif}(a,b)$. \n:::  \n\n::: {.callout-warning}\n## Parameters\n\nNote that $a$ and $b$ are parameters in @def-uniform-dist.\n:::\n\n::: {#exr-uniform}\nAs an exercise, derive the cdf using the definition. Derive a formula for the mean and variance in terms of the parameters $a$ and $b$.\n:::\n\n## Normal distribution {#sec-normal-distribution}\n\nNormal distributions play an important role in probability and statistics as they describe many natural phenomena. For instance, the Central Limit Theorem tells us that the sample mean of a large random sample (size $m$) of rvs with mean $\\mu$ and variance $\\sigma^2$ is approximately normal in distribution with mean $\\mu$ and variance $\\sigma^2/m$.  \n\n::: {#def-normal-dist}\n## Normal or Gaussian distribution\n\nA continuous rv $X$ has a normal distribution with parameters $\\mu$ and $\\sigma^2,$ where $-\\infty < \\mu < \\infty$ and $\\sigma > 0,$ if $X$ has pdf\n$$\n f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma}e^{-(x-\\mu)^2/(2\\sigma^2)}\\,, \n \\quad -\\infty < x < \\infty \\,.\n$$\nWe write $X \\sim \\mathsf{N}(\\mu, \\sigma^2)$. \n::: \n\nFor $X\\sim \\mathsf{N}(\\mu,\\sigma^2),$ it can be shown that $\\E(X) = \\mu$ and $\\Var(X) = \\sigma^2,$ that is, $\\mu$ is the *mean* and $\\sigma^2$ is the *variance* of $X$. The pdf forms a bell-shaped curve that is symmetric about $\\mu,$ as illustrated in @fig-normals-diff-mean. The value $\\sigma$ (*standard deviation*) is the distance from $\\mu$ to the inflection points of the curve. As $\\sigma$ increases, the dispersion in the density increases, as illustrated in @fig-normals-diff-sd. Thus, the distribution's position (location) and spread depend on $\\mu$ and $\\sigma$.  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The pdfs of two normal rvs, $X_1 \\sim \\mathsf{N}(-2, 1)$ and $X_2 \\sim \\mathsf{N}(2, 1),$ with *different means* and the same standard deviations.](01-sampling-distributions_files/figure-html/fig-normals-diff-mean-1.svg){#fig-normals-diff-mean fig-align='center' width=672}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The pdfs of two normal rvs, $X_1 \\sim \\mathsf{N}(0, 9)$ and $X_2 \\sim \\mathsf{N}(0, 1),$ with the same means and *different standard deviations*.](01-sampling-distributions_files/figure-html/fig-normals-diff-sd-1.svg){#fig-normals-diff-sd fig-align='center' width=672}\n:::\n:::\n\n\n::: {#def-standard-normal}\n## Standard normal distribution\n\nWe say that $X$ has a standard normal distribution if $\\mu=0$ and $\\sigma = 1$ and we will usually denote standard normal rvs by $$Z \\sim \\mathsf{N}(0,1)$$ (why $Z$? tradition!^[\"Traditions, traditions... Without our traditions, our lives would be as shaky as a fiddler on the roof!\" [[https://www.youtube.com/watch?v=gRdfX7ut8gw](https://www.youtube.com/watch?v=gRdfX7ut8gw)].]). We denote the cdf of the standard normal by $$\\Phi(z) = P(Z \\leq z)$$ and write $\\varphi = \\Phi'$ for its density function. \n:::\n\n::: {.callout-important}\n## Useful facts about normal variates\n\n1. If $X \\sim \\mathsf{N}(\\mu, \\sigma^2),$ then $$Z = (X - \\mu) / \\sigma  \\sim \\mathsf{N}(0,1).$$\n2. If $Z \\sim \\mathsf{N}(0, 1),$ then $$X = \\mu + \\sigma Z \\sim \\mathsf{N}(\\mu, \\sigma^2).$$\n3. If $X_i \\sim \\mathsf{N}(\\mu_i, \\sigma_i^2)$ for $i = 1, \\dots, n$ are independent rvs, then \n$$\\sum_{i=1}^{n} X_i \\sim \\mathsf{N} \\left( \\sum_{i=1}^{n} \\mu_i, \\sum_{i=1}^{n} \\sigma_i^2 \\right) \\,.$$  \n::: \n\n::: {.callout-warning}\n## Variances add\n\nIn particular, for differences of independent rvs $X_1 \\sim \\mathsf{N}(\\mu_1, \\sigma_1^2)$ and $X_2 \\sim \\mathsf{N}(\\mu_2, \\sigma_2^2)$ then the variances add:\n$$ X_1 - X_2 \\sim \\mathsf{N}(\\mu_1 - \\mu_2, \\sigma_1^2 + \\sigma_2^2) \\,.$$  \n:::\n\nProbabilities $P(a \\leq X \\leq b)$ are found by converting the problem in $X \\sim \\mathsf{N}(\\mu, \\sigma^2)$ to the *standard normal* distribution $Z \\sim \\mathsf{N}(0, 1)$ whose probability values $\\Phi(z) = P(Z\\leq z)$ can then be looked up in a table. From (1.) above, \n$$\n\\begin{aligned}\n   P(a < X < b) &= P\\left( \\frac{a-\\mu}{\\sigma} < Z < \\frac{b-\\mu}{\\sigma} \\right) \\\\ \n    &= \\Phi \\left( \\frac{b-\\mu}{\\sigma}\\right) - \\Phi\\left(\\frac{a-\\mu}{\\sigma}\\right) \\,.\n\\end{aligned}\n$$\nThis process is often referred to as *standardising* (the normal rv).  \n\n::: {#exm-norm-rt}\n\nLet $X \\sim \\mathsf{N}(5, 9)$ and find $P(X \\geq 5.5)$. \n\n$$\n\\begin{aligned}\n   P(X \\geq 5.5) &= P\\left(Z \\geq \\frac{5.5 - 5}{3}\\right) \\\\\n    &= P(Z \\geq 0.1667) \\\\\n    &= 1 - P(Z \\leq 0.1667) \\\\\n    &= 1 - \\Phi(0.1667) \\\\\n    &= 1 - 0.5662 \\\\\n    &= 0.4338\\,,\n\\end{aligned}\n$$\nwhere we look up the value of $\\Phi(z) = P(Z\\leq z)$ in a table of standard normal curve areas.\n\nThe probability corresponds to the shaded area under the normal density $\\varphi(x) = \\Phi'(x)$ corresponding to $x \\geq 5.5$ (see @fig-example-norm-geq). To calculate this area, we can also use the `R` code: `pnorm(5.5, mean = 5, sd = 3, lower.tail = FALSE)`.  \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The normal density $\\mathsf{N}(5,9)$ with the (one-sided) interval shaded in blue that corresponds to the probability $P(X \\geq 5.5)$.](01-sampling-distributions_files/figure-html/fig-example-norm-geq-1.svg){#fig-example-norm-geq fig-align='center' width=672}\n:::\n:::\n\n:::\n\n\n::: {#exm-norm-dt}\n\nLet $X \\sim \\mathsf{N}(5, 9)$ and find $P(4 \\leq X \\leq 5.25)$. \n\n$$\n\\begin{aligned}\n   P(4 \\leq X \\leq 5.25) &= P\\left(\\frac{4-5}{3} \\leq Z \\leq \\frac{5.25-5}{3}\\right) \\\\\n   &= P(-0.3333 \\leq Z \\leq 0.0833) \\\\\n   &= \\Phi(0.0833) - \\Phi(-0.3333) \\\\\n   &= 0.5332 - 0.3694 \\\\\n   &= 0.1638\\,.\n  \\end{aligned}\n$$\nwhere we look up the value of $\\Phi(z) = P(Z\\leq z)$ in a table of standard normal curve areas. \n\nThe probability corresponds to the shaded area under the normal density $\\varphi(x) = \\Phi'(x)$ corresponding to $4 \\leq x \\leq 5.25$ (see @fig-example-norm-interval). To calculate this area, we can use the `R` code: `pnorm(5.25, mean = 5, sd = 3) - pnorm(4, mean = 5, sd = 3)`. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The normal density $\\mathsf{N}(5,9)$ with the (two-sided) interval shaded in blue that corresponds to the probability $P(4 \\leq X \\leq 5.25)$.](01-sampling-distributions_files/figure-html/fig-example-norm-interval-1.svg){#fig-example-norm-interval fig-align='center' width=672}\n:::\n:::\n\n:::\n\n::: {.callout-important}\n## Empirical rule ($68-95-99.7$ rule)\n\nFor samples from a normal distribution, the percentage of values that lie within one, two, and three standard deviations of the mean are $68.27\\%,$ $95.45\\%,$ and $99.73\\%,$ respectively. That is, for $X \\sim \\mathsf{N}(\\mu, \\sigma^2),$ \n$$\nP(\\mu - 1 \\sigma \\leq X \\leq \\mu + 1 \\sigma ) \\approx 0.6827\\,,\n$$\n$$\nP(\\mu - 2 \\sigma \\leq X \\leq \\mu + 2 \\sigma ) \\approx 0.9545\\,,\n$$\n$$\nP(\\mu - 3 \\sigma \\leq X \\leq \\mu + 3 \\sigma ) \\approx 0.9973\\,.\n$$\nFor a normal population, nearly all the values lie within \"three sigmas\" of the mean.\n:::\n\n## Student's $\\mathsf{t}$ distribution {#sec-t-distribution}\n\nStudent's $\\mathsf{t}$ distribution gets its peculiar name as it was first published under the pseudonym  \"Student\".^[William Sealy Gosset (1876--1937) wrote under the pseudonym \"Student\" [[https://mathshistory.st-andrews.ac.uk/Biographies/Gosset/](https://mathshistory.st-andrews.ac.uk/Biographies/Gosset/)].] This bit of obfuscation was to protect the identity of his employer,^[Gosset invented the t-test to handle small samples for quality control in brewing, specifically for the Guinness brewery in Dublin [[https://www.wikiwand.com/en/Guinness_Brewery](https://www.wikiwand.com/en/Guinness_Brewery)].] and thereby vital trade secrets, in a highly competitive and lucrative industry.  \n\n::: {#def-t-dist}\n## Student's $\\mathsf{t}$ distribution\n\nA continuous rv $X$ has a $\\mathsf{t}$ distribution with parameter $\\nu > 0,$ if $X$ has pdf\n$$\nf(x; \\nu) = \\frac{\\Gamma\\left(\\tfrac{\\nu+1}{2}\\right)}{\\sqrt{\\nu \\pi} \\Gamma \\left(\\tfrac{\\nu}{2}\\right)} \\left( 1 + \\tfrac{x^2}{\\nu} \\right)^{- \\frac{\\nu+1}{2}} \\,, \\quad -\\infty < x < \\infty\\,.\n$$\nWe write $X \\sim \\mathsf{t}(\\nu)$. Note $\\Gamma$ is the standard gamma function.^[The gamma function is defined by $\\Gamma(z) = \\int_0^\\infty x^{z-1}e^{-x} dx$ when the real part of $z$ is positive. For any positive integer $n,$ $\\Gamma(n) = (n-1)!$ and for half-integers $\\Gamma(\\tfrac{1}{2} + n) = \\frac{(2n)!}{4^n n!} \\sqrt{\\pi}$.]\n:::\n\nThe density for $\\mathsf{t}(\\nu)$ for several values of $\\nu$ are plotted below in @fig-example-t-dist. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The density for $\\mathsf{t}(\\nu)$ for several values of $\\nu$ (df).](01-sampling-distributions_files/figure-html/fig-example-t-dist-1.svg){#fig-example-t-dist fig-align='center' width=672}\n:::\n:::\n\n\n::: {.callout-important}\n## Properties of $\\mathsf{t}$ distributions {#facts-t}\n\n1. The density for $\\mathsf{t}(\\nu)$ is a bell-shaped curve centred at $0$.\n2. The density for $\\mathsf{t}(\\nu)$ is more spread out than the standard normal density (i.e., it has \"fatter tails\" than the normal).\n3. As $\\nu \\to \\infty,$ the spread of the corresponding $\\mathsf{t}(\\nu)$ density converges to the standard normal density (i.e., the spread of the $\\mathsf{t}(\\nu)$ density decreases relative to the standard normal).  \n\nIf $X \\sim \\mathsf{t}(\\nu),$ then $\\E[X] = 0$ for $\\nu > 1$ (otherwise the mean is undefined).  \n:::\n\n::: {.callout-note}\n## Cauchy distribution\n\nA $\\mathsf{t}$ distributions with $\\nu = 1$ has pdf \n$$f(x) = \\frac{1}{\\pi (1 + x^2)}\\,,$$\nand we call this the Cauchy distribution. \n:::\n\n\n## $\\chi^2$ distribution {#sec-chisq-distribution}\n\nThe $\\chi^2$ distribution arises as the distribution of a sum of the squares of $\\nu$ independent standard normal rvs.  \n\n::: {#def-chisq-dist}\n## $\\chi^2$ distribution\n\nA continuous rv $X$ has a $\\chi^2$ distribution with parameter $\\nu \\in \\mathbf{N}_{>},$ if $X$ has pdf\n\\begin{equation*}\nf(x; \\nu) = \\frac{1}{2^{\\nu/2} \\Gamma(\\nu/2)} x^{(\\nu/2)-1} e^{-x/2} \\,, \n\\end{equation*}\nwith support $x \\in (0, \\infty)$ if $\\nu=1,$ otherwise $x \\in [0, \\infty)$. We write $X \\sim \\chi^2(\\nu)$. \n:::  \n\nThe pdf $f(x; \\nu)$ of the $\\chi^2(\\nu)$ distribution depends on a positive integer $\\nu$ referred to as the df. The densities for several values of $\\nu$ are plotted below in @fig-example-chisq-dist. The density $f(x;\\nu)$ is positively skewed, i.e., the right tail is longer, so the mass is concentrated to the figure's left in @fig-example-chisq-dist. The distribution becomes more symmetric as $\\nu$ increases. We denote critical values of the $\\chi^2(\\nu)$ distribution by $\\chi^2_{\\alpha, \\nu}$.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The density for $\\chi^2(\\nu)$ for several values of $\\nu$ (df).](01-sampling-distributions_files/figure-html/fig-example-chisq-dist-1.svg){#fig-example-chisq-dist fig-align='center' width=672}\n:::\n:::\n\n\n:::{.callout-warning} \n## Skew\n\nUnlike the normal and $t$ distributions, the $\\chi^2$ distribution is not symmetric! This means that critical values, e.g., $$\\chi^2_{.99, \\nu} \\quad \\text{and}\\quad \\chi^2_{0.01,\\nu}\\,,$$ are **not** equal. Hence, it will be necessary to look up both values for CIs based on $\\chi^2$ critical values.  \n:::\n\nIf $X \\sim \\chi^2(\\nu),$ then $\\E[X] = \\nu$ and $\\Var[X] = 2\\nu$.  \n\n## $\\mathsf{F}$ distribution {#sec-F-distribution}\n\nThe $\\mathsf{F}$ distribution (\"F\" for Fisher) arises as a test statistic when comparing population variances and in the analysis of variance (see \\@sec-anova). \n\n::: {#def-F-dist}\n## $\\mathsf{F}$ distribution\nA continuous rv $X$ has an $\\mathsf{F}$ distribution with df parameters $\\nu_1$ and $\\nu_2,$ if $X$ has pdf\n$$\n f(x; \\nu_1, \\nu_2) = \n    \\frac{\\Gamma\\left(\\frac{\\nu_1+\\nu_2}{2}\\right) \\nu_1^{\\nu_1/2} \\nu_2^{\\nu_2/2}}\n {\\Gamma\\left(\\frac{\\nu_1}{2}\\right) \\Gamma\\left(\\frac{\\nu_2}{2}\\right)} \n \\frac{x^{\\nu_1/2 - 1}}{(\\nu_2+\\nu_1 x)^{(\\nu_1+\\nu_2)/2}} \\,.\n$$\n:::\n\nThe pdf $f(x; \\nu_1, \\nu_2)$ of the $\\mathsf{F}(\\nu_1, \\nu_2)$ distribution depends on two positive integers $\\nu_1$ and $\\nu_2$ referred to, respectively, as the numerator and denominator df. The density is plotted below for several combinations of $(\\nu_1, \\nu_2)$ in @fig-example-F-dist.\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The density for $\\mathsf{F}(\\nu_1, \\nu_2)$ for several combinations of $(\\nu_1, \\nu_2)$.](01-sampling-distributions_files/figure-html/fig-example-F-dist-1.svg){#fig-example-F-dist fig-align='center' width=672}\n:::\n:::\n\n\n:::{.callout-tip}\n## Where do the terms numerator and denominator df come from?\n\nThe $\\mathsf{F}$ distribution is related to ratios of $\\chi^2$ rvs, as captured in @thm-F-dist-chisq.\n:::\n\n\n\n::: {#thm-F-dist-chisq}\n## Ratio of $\\chi^2$ rvs\n\nIf $X_1 \\sim \\chi^2(\\nu_1)$ and $X_2 \\sim \\chi^2(\\nu_2)$ are independent rvs, then the rv \n$$\n F = \\frac{X_1 / \\nu_1}{X_2 / \\nu_2} \\quad \\sim \\mathsf{F}(\\nu_1,\\nu_2)\\,,\n$$\nthat comprises the ratio of two $\\chi^2$ rvs divided by their respective df has an $\\mathsf{F}(\\nu_1, \\nu_2)$ distribution. \n:::\n \n",
    "supporting": [
      "01-sampling-distributions_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}