{
  "hash": "1bfc3ee265b6e6f09ceb3f992bba57e9",
  "result": {
    "engine": "knitr",
    "markdown": "\n\n::: {.content-hidden when-format=\"pdf\"}\n\\newcommand{\\E}{\\mathbf{E}} \n\\DeclareMathOperator{\\Var}{Var}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\corr}{corr}\n\\DeclareMathOperator{\\sd}{sd}\n\\newcommand{\\se}{\\mathsf{se}}\n:::\n\n\n\n# Basics of statistical inference {#sec-statistical-inference}\n\nWe discuss point estimation, confidence intervals, and hypothesis testing in Sections @sec-point-estimation, @sec-confidence-intervals, and @sec-hypothesis-testing, respectively. These three tools will form the basis for making inferences about a population.\n\n## Point estimation {#sec-point-estimation}\n\nStatistical inference seeks to draw conclusions about the characteristics of a population from data. For example, suppose we are botanists interested in the taxonomic classification of iris flowers. Let $\\mu$ denote the true average petal length (in cm) of the *Iris setosa*^[More about the *Iris setosa* here [[https://www.wikiwand.com/en/Iris_setosa](https://www.wikiwand.com/en/Iris_setosa)].] (AKA the bristle-pointed iris). The parameter $\\mu$ is a characteristic of the whole population of the *setosa* species. Before we collect data, the petal lengths of $m$ independent *setosa* flowers are denoted by rvs $X_1, X_2, \\dots, X_m.$ Any function of the $X_i$'s, such as the sample mean,\n$$\n  \\overline{X} = \\frac{1}{m} \\sum_{i=1}^m X_i\\,, \n$$ {#eq-sample-mean}\nor the sample variance,\n$$\n  S^2 = \\frac{1}{m-1} \\sum_{i=1}^m (X_i - \\overline{X})^2 \\,, \n$$ {#eq-sample-var}\nis also a rv. \n\nSuppose we actually find and measure the petal length of $50$ independent *setosa* flowers resulting in observations $x_1, x_2, \\dots, x_{50}$; the distribution (counts) of $50$ such petal length measurements are displayed in @fig-setosa-petal-lengths. The sample mean $\\overline{x}$ for petal length can then be used to draw a conclusion about the (true) value of the population mean $\\mu.$ Based on the data in  @fig-setosa-petal-lengths and using @eq-sample-mean, the value of the sample mean is $\\overline{x} = 1.462.$ The value $\\overline{x}$ provides a \"best guess\" or point estimate for the true value of $\\mu$ based on the $m=50$ samples. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![The distribution (counts) of $m = 50$ *setosa* petal length measurments.](02-basics-stat-infer_files/figure-html/fig-setosa-petal-lengths-1.svg){#fig-setosa-petal-lengths fig-align='center' width=672}\n:::\n:::\n\n\n:::{.callout-tip}\n## Loading datasets \n\nThe `datasets` package has a variety of datasets that you can play with. Once installed, data sets can be accessed in `R` by loading `library(datasets)` and then calling, e.g., `data(iris)` to see the `iris` data set. For a full list of available data sets, call `library(help = \"datasets\")` from the console.\n:::\n\n:::{#nte-iris .callout-note collapse=\"true\"}\n## Iris Data\n\nThe botanist Edgar Anderson's **Iris Data** contains 50 obs. of four features (sepal length [cm], sepal width [cm], petal length [cm], and petal width [cm]) for each of three plant species (*setosa*, *virginica*, *versicolor*) for 150 obs. total. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\niris |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8, 4.8, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, set…\n```\n\n\n:::\n:::\n\n:::\n\n::: {#def-point-estimate}\n## Point estimate\n\nA point estimate of a parameter $\\theta$ (recall: a parameter is a fixed, unknown quantity) is a single number that we consider a reasonable value for $\\theta.$ Consider $$\\text{iid}\\; X_1, X_2, \\dots, X_m \\sim F(\\theta)\\,.$$ A point estimator $\\widehat{\\theta}_m$ of $\\theta$ is obtained by selecting a suitable statistic $g,$\n$$\n  \\widehat{\\theta}_m = g(X_1, \\dots, X_m) \\,.\n$$\nA point estimate $\\widehat{\\theta}_m$ can then be computed from the estimator using sample data.\n:::\n\n:::{.callout-warning}\n## Overloaded notation\n\nThe symbol $\\widehat{\\theta}_m$ (or simply $\\widehat{\\theta}$ when the sample size $m$ is clear from context) is typically used to denote both the estimator and the point estimate resulting from a given sample.\n:::\n\n:::{.callout-tip}\n## Best practice for reporting \n\nWriting, e.g., $\\widehat{\\theta} = 42$ does not indicate how the point estimate was obtained. Therefore, it is essential to report both the estimator and the resulting point estimate. \n:::   \n\n@def-point-estimate does not say how to select an appropriate statistic. For the *setosa* example, the sample mean $\\overline{X}$ is suggested as a good estimator of the population mean $\\mu.$ That is, $\\widehat{\\mu} = \\overline{X}$ or: \n\n> \"the point estimator of $\\mu$ is the sample mean $\\overline{X}$\". \n\nHere, while $\\mu$ and $\\sigma^2$ are fixed quantities representing population characteristics, $\\overline{X}$ and $S^2$ are rvs with sampling distributions. If the population is *normally distributed* or if the *sample is large* then the sampling distribution for $\\overline{X}$ has a known form:\n$$\n  \\overline{X} \\sim \\mathsf{N}(\\mu, \\sigma^{2} / m) \\,,\n$$\nthat is, $\\overline{X}$ is normal with mean $\\mu_{\\overline{X}} = \\mu$ and variance $\\sigma_{\\overline{X}}^2 = \\sigma^{2} / m$ where $m$ is the sample size and $\\mu$ and $\\sigma$ are the (typically unknown) population parameters.\n\n:::{#nte-tree .callout-note collapse=\"true\"}\n## Cherry Tree Data \n\nThe **Cherry Tree Data** contains 31 obs. of three features (diameter, height, and volume).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrees |> glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 31\nColumns: 3\n$ Girth  <dbl> 8.3, 8.6, 8.8, 10.5, 10.7, 10.8, 11.0, 11.0, 11.1, 11.2, 11.3, 11.4, 11.4…\n$ Height <dbl> 70, 65, 63, 72, 81, 83, 66, 75, 80, 75, 79, 76, 76, 69, 75, 74, 85, 86, 7…\n$ Volume <dbl> 10.3, 10.3, 10.2, 16.4, 18.8, 19.7, 15.6, 18.2, 22.6, 19.9, 24.2, 21.0, 2…\n```\n\n\n:::\n:::\n\n::: \n\n::: {#exm-estimators}\nLet us consider the heights (measured in inches) of $31$ black cherry trees (sorted, for your enjoyment) in @tbl-cherry-data.\n\n\n::: {#tbl-cherry-data .cell layout-align=\"center\" tbl-cap='Observations of $m = 31$ felled black cherry trees.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Height [in] </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;width: 150mm; \"> 63, 64, 65, 66, 69, 70, 71, 72, 72, 74, 74, 75, 75, 75, 76, 76, 77, 78, 79, 80, 80, 80, 80, 80, 81, 81, 82, 83, 85, 86, 87 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Normal quantile-quantile plot for the `Height` variable (feature) in the Cherry Tree Data.](02-basics-stat-infer_files/figure-html/fig-qq-plot-cherry-1.svg){#fig-qq-plot-cherry fig-align='center' width=672}\n:::\n:::\n\n\nThe quantile-quantile plot in @fig-qq-plot-cherry, which compares the quantiles of this data to the quantiles of a normal distribution, is fairly straight. Therefore, we assume that the distribution of black cherry tree heights is (at least approximately) normal with a mean value $\\mu$; i.e., that the population of heights is distributed $\\mathsf{N}(\\mu, \\sigma^2),$ where $\\mu$ is a parameter to be estimated and $\\sigma^2$ is unknown. The observations $X_1, \\dots, X_{31}$ are then assumed to be a random sample from this normal distribution, \n$$\n\\text{iid} \\quad X_1, \\dots, X_{31} \\sim \\mathsf{N}(\\mu, \\sigma^2) \\,.\n$$\n\nConsider the following three different estimators and the resulting point estimates for $\\mu$ based on the $31$ samples in @tbl-cherry-data.\n\na. Estimator (sample mean) $\\overline{X}$ as in @eq-sample-mean and estimate $\\overline{x} = \\sum x_i / n = 2356 / 31 = 76.$\n\nb. Estimator (average of extreme heights) $\\widetilde{X} = [\\min(X_i) + \\max(X_i)]/2$ and estimate $\\widetilde{x} = (63 + 87)/2 = 75.$ \n\nc. Estimator ($10\\%$ trimmed mean -- i.e., in this instance exclude the smallest and largest three values) $\\overline{X}_{\\text{tr}(10)}$ and estimate $\\overline{x}_{\\text{tr}(10)} = (2356 - 63 - 64 - 65 - 87 - 86 - 85) / 25 = 76.24.$ \n\nEach estimator above uses a different notion of \"centre\" for the sample data, i.e., represents a different statistic. An interesting question is: which estimator will tend to produce estimates closest to the true parameter value? Will the estimators work universally well for all distributions? \n::: \n\n:::{.callout-tip}\n## How do we tell whether a population is normal? \n\nConstructing a normal quantile-quantile plot (or QQ plot) is one way of assessing whether a normality assumption is reasonable. A QQ plot compares the quantiles of the sample data $x_i$ against the theoretical standard normal quantiles, see @fig-qq-plot-cherry. If the sample data is consistent with a sample from a normal distribution, the points will lie in a straight line (more or less). The QQ plot in @fig-qq-plot-cherry compares quantiles of cherry tree heights from @tbl-cherry-data to normal quantiles. It is produced using the following code. \n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrees |> ggplot(aes(sample = Height)) + stat_qq() + stat_qq_line()\n```\n:::\n\n\nThe data `trees` is piped to the command `ggplot`. For a QQ plot the key aesthetic element is `sample`; in this particular instance we set this to `Height`. The geometry `stat_qq()` adds the data quantiles plotted versus the normal quantiles. The geometry `stat_qq_line()` simply adds the fit line. \n:::\n\n::: {#exm-infer-point-estimation}\nAlthough probably overkill for this problem, the `infer` package can be used for point estimation using the `specify` and `calculate` commands as follows:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntrees |>\n specify(response = Height) |>\n calculate(stat = \"mean\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResponse: Height (numeric)\n# A tibble: 1 × 1\n   stat\n  <dbl>\n1    76\n```\n\n\n:::\n:::\n\n\nThe `response` option specifies the variable of interest, and the `stat` option can be changed to several quantities of interest.\n:::\n\n \nIn addition to reporting a point estimate and its estimator, some indication of its precision should be given. One measure of the precision of an estimate is its standard error.  \n\n::: {#def-standard-error}\n## Standard error \n\nThe standard error of an estimator $\\widehat{\\theta}$ is the standard deviation \n$$\n\\sigma_{\\widehat{\\theta}} = \\sqrt{\\Var(\\widehat{\\theta})}\\,.\n$$\nOften, the standard error depends on unknown parameters and must also be estimated. The estimated standard error is denoted by $\\widehat{\\sigma}_{\\widehat{\\theta}}$ or simply $s_{\\widehat{\\theta}}.$\n:::  \n\n:::{.callout-tip}\n## Alternative notation\n\nThe standard error is sometimes denoted $\\se = \\se(\\widehat{\\theta})$ and the estimated standard error by $\\widehat{\\se}.$\n:::\n\n## Confidence intervals {#sec-confidence-intervals}\n\nAn alternative to reporting a point estimate for a parameter is to report an interval estimate suggesting an entire range of plausible values for the parameter of interest. A confidence interval is an estimate that makes a probability statement about the interval's degree of reliability. The first step in computing a confidence interval is to select the confidence level $\\alpha.$ A popular choice is a $95\\%$ confidence interval which corresponds to level $\\alpha = 0.05.$  \n\n::: {#def-confidence-interval-gen}\n## Confidence interval\n\nA $100(1-\\alpha)\\%$ confidence interval for a parameter $\\theta$ is a *random* interval $$C_m = (L_m , U_m)\\,,$$ where $L_m = \\ell(X_1, \\dots, X_m)$ and $U_m = u(X_1, \\dots, X_m)$ are functions of the data, such that \n$$\nP_{\\theta}(L_m < \\theta < U_m ) = 1 - \\alpha\\,, \n$$\nfor all $\\theta \\in \\Theta.$ \n:::\n\nMy favourite interpretation of a confidence interval is due to [@Wasserman:2013as, p 92]:  \n\n> *On day 1, you collect data and construct a 95 percent confidence interval for a parameter $\\theta_1.$ On day 2, you collect new data and construct a 95 percent confidence interval for an unrelated parameter $\\theta_2.$ On day 3, you collect new data and construct a 95 percent confidence interval for an unrelated parameter $\\theta_3.$ You continue this way constructing confidence intervals for a sequence of unrelated parameters $\\theta_1,$ $\\theta_2,$ $\\dots$ Then 95 percent of your intervals will trap the true parameter value. There is no need to introduce the idea of repeating the same experiment over and over.*\n\nThis interpretation clarifies that a confidence interval is not a probability statement about the parameter $\\theta.$ In @def-confidence-interval-gen, note that $\\theta$ is fixed ($\\theta$ is not a rv) and the interval $C_m$ is random. After data has been collected and a point estimator has been calculated, the resulting CIs either contain the true parameter value or do not, as illustrated in @fig-fifty-cis. \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![Fifty $95\\%$ CIs for a population mean $\\mu.$ After a sample is taken, the computed interval estimate either contains $\\mu$ or does not (asterisks identify intervals that do not include $\\mu$). When drawing such a large number of $95\\%$ CIs, we would anticipate that approximately $5\\%$ (ca. 2 or 3) would fail to cover the true parameter $\\mu.$](02-basics-stat-infer_files/figure-html/fig-fifty-cis-1.svg){#fig-fifty-cis fig-align='center' width=288}\n:::\n:::\n\n\n\n## Hypothesis testing {#sec-hypothesis-testing}\n\n@sec-point-estimation and @sec-confidence-intervals reviewed how to estimate a parameter by a single number (point estimate) or range of plausible values (confidence interval), respectively. Next, we discuss methods for determining which of two contradictory claims, or hypotheses, about a parameter is correct. \n\n::: {#def-null-alt-hypothesis}\n## Null and alternative\n\nThe null hypothesis, denoted by $H_0,$ is a claim we initially assume to be true by default. The alternative hypothesis, denoted by $H_a,$ is an assertion contradictory to $H_0.$ \n:::\n\nTypically, we shall consider a hypothesis test concerning a parameter $\\theta \\in \\Theta,$ i.e., taking values in a parameter space $\\Theta.$ The statistical hypotheses are contradictory in that $H_0$ and $H_a$ divide $\\Theta$ into two disjoint sets. For example, for a statistical inference regarding the *equality* of a parameter $\\theta$ with a fixed quantity $\\theta_0,$ the null and alternative hypotheses will usually take one of the following forms in @tbl-htest-null-alt-forms.\n\n::: {.striped .hover #tbl-htest-null-alt-forms}\n| Null hypothesis | Alternative hypothesis | Test form |\n| ---             | ---                    | ---       |\n|$H_0 : \\theta = \\theta_0$ | $H_a : \\theta \\neq \\theta_0$ | two-sided test |\n| $H_0 : \\theta \\leq \\theta_0$ | $H_a : \\theta > \\theta_0$ | one-sided test |\n| $H_0 : \\theta \\geq \\theta_0$ | $H_a : \\theta < \\theta_0$ | one-sided test |\n\n: Typical null hypothesis and corresponding alternative hypothesis. \n:::\n\nThese hypothesis pairs are associated with either a one-sided or two-sided test; what this means will become apparent in the sequel. The value $\\theta_0,$ called the null value, separates the alternative from the null. \n\n:::{#def-hypothesis-test}\n## Hypothesis test\n\nA hypothesis test asks if the available data provides sufficient evidence to reject $H_0.$ If the observations disagree with $H_0,$ we reject the null hypothesis. If the sample evidence does not strongly contradict $H_0,$ then we continue to believe $H_0.$ The two possible conclusions of a hypothesis test are: *reject $H_0$* or *fail to reject $H_0$*.  \n:::\n\n\n::: {.callout-important}\n## \"Fail to reject\" versus \"accept\"\n\nWe comment that *fail to reject $H_0$* is sometimes phrased as *retain $H_0$* or (perhaps less accurately) *accept $H_0$*. \n\nWhy not just *accept* the null and move on with our lives? \n\nWell, if I search the Highlands for the Scottish wildcat (endangered) and fail to find any, does that prove they do not exist?\n:::\n\nA procedure for carrying out a hypothesis test is based on specifying two additional items: a test statistic and a corresponding rejection region. A test statistic $T$ is a function of the sample data (like an estimator). The decision to reject or fail to reject $H_0$ will involve computing the test statistic. The rejection region $R$ is the collection of values of the test statistic for which $H_0$ is to be rejected in favour of the alternative, e.g.,\n$$\nR = \\left\\{ x : T(x) > c \\right\\}\\,,\n$$\nwhere $c$ is referred to as a critical value. If a given sample falls in the rejection region, we reject $H_0.$ If $X \\in R$ (e.g., the calculated test statistic exceeds some critical value), we reject $H_0.$ The alternative is that $X \\not\\in R$ and we fail to reject the null in this case. \n\nTwo types of errors can be made when carrying out a hypothesis test. The basis for choosing a rejection region involves considering these errors. \n\n::: {#def-error-types}\n## Error types\nA type I error occurs if $H_0$ is rejected when $H_0$ is actually true. A type II error is made if we fail to reject $H_0$ when $H_0$ is actually false.\n:::   \n\nIf a test's maximal type I error is fixed at an acceptably small value, then the type II error decreases as the sample size increases. In particular, a conclusion is reached in a hypothesis test by selecting a significance level $\\alpha$ for the test linked to the maximal type I error rate. Typically, $\\alpha = 0.10,$ $0.05,$ $0.01,$ or $0.001$ is selected for the significance level.\n\n:::{#def-P-value}\n## $P$-value\nA $P$-value is the probability, calculated assuming $H_0$ is true, of obtaining a value of the test statistic at least as contradictory to $H_0$ as the value calculated from the sample data. \n:::    \n\nSmaller $P$-values indicate stronger evidence against $H_0$ in favor of $H_a.$ If $P \\leq \\alpha$ then we reject $H_0$ at significance level $\\alpha.$ If $P \\geq \\alpha$ we fail to reject $H_0$ at significance level $\\alpha.$   \n\n:::{.callout-warning}\n## What a $P$-value isn't...\n\nThe $P$-value is a probability calculated assuming that $H_0$ is true. However, the $P$-value is **not** the probability that:\n\n1. $H_0$ is TRUE, \n2. $H_0$ is FALSE, or \n3. a wrong conclusion is reached.\n:::   \n\n:::{#prp-signifiance-type1}\nThe hypothesis test procedure that \n$$\n\\begin{cases} \n\\text{rejects}\\; H_0 & \\text{if}\\; P \\leq \\alpha,\\\\\n\\text{fails to reject}\\; H_0 & \\text{otherwise},\n\\end{cases}\n$$\nhas $P(\\text{type I error}) = \\alpha.$ \n:::   \n\n:::{#exm-htest-setup}\nChurchill claims that he will receive half the votes for the House of Commons seat for the constituency of Dundee.^[Sir Winston Churchill was Member of Parliament for Dundee from 1908--1922 [[https://www.wikiwand.com/en/Winston_Churchill](https://www.wikiwand.com/en/Winston_Churchill)].] If we do not believe Churchill's claim and are doubtful of his popularity, we would seek to test an alternative hypothesis. How should we write down our research hypotheses?\n\nIf we let $p$ be the fraction of the population voting for Churchill, then we have the null hypothesis,\n$$\n H_0 : p = 0.5 \\,,\n$$\nand the alternative hypothesis (we believe Churchill is less popular than he claims),\n$$\n H_a : p < 0.5 \\,.\n$$\nSupport for the alternative hypothesis is obtained by showing a lack of support for its converse hypothesis (the null hypothesis).\n:::\n\n::: {#exm-htest-alpha}\nSuppose that $m = 15$ voters are selected from Dundee and $X,$ the number favouring Churchill, is recorded. Based on observing $X,$ we construct a rejection region $R = \\{x : x \\leq k \\}.$ If $k$ is small compared to $m,$ then the rejection region would provide strong evidence to reject $H_0.$ How should one choose the rejection region?\n\nAssume now that $m = 15$ voters are polled and that we select $k = 2$ to have a rejection region $R = \\{ x \\leq 2 \\}.$ For this choice of $k,$ the rejection region $R$ provides strong support to reject $H_0.$ Assuming the null hypothesis is true, we expect approximately half of the $15$ voters (ca. 7) to vote for Churchill. Observing $x = 0,$ $x = 1$ or $x = 2$ (the values that would place us in the rejection region) would provide strong evidence *against* $H_0.$ \n\nWe can calculate the probability of a type I error. From the definition of type I error,\n$$\n \\begin{aligned}\n \\alpha &= P(\\text{type I error})\\\\\n  &= P(\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is true})\\\\\n  &= P(X \\in R \\text{ when } H_0 \\text{ is true})\\\\\n  &= P(X \\leq 2 \\text{ when } p = 0.5) \\,.\n \\end{aligned}\n$$\nSince $X \\sim \\mathsf{Binom}(15, 0.50),$ we calculate that $\\alpha = 0.00369.$ Thus, for this particular choice of rejection region $R,$ the risk of concluding that Churchill will lose if, in fact, he is the winner is tiny.\n\nFor this rejection region, how good is the test at protecting us from type II errors, i.e., concluding that Churchill is the winner if, in fact, he will lose? Suppose that Churchill receives $25%$ of the votes ($p=0.25$). The probability of type II error $\\beta$ is,\n$$\n  \\begin{aligned}\n  \\beta &= P(\\text{type II error})\\\\\n  &= P(\\text{fail to reject } H_0 \\text{ when } H_0 \\text{ false})\\\\\n  &= P(X \\not\\in R \\text{ when } H_0 \\text{ false})\\\\\n  &= P(X > 2 \\text{ when } p = 0.3)\\,.\n \\end{aligned}\n$$\nFor $X \\sim \\mathsf{Binom}(15, 0.25),$ we calculate $\\beta = 0.764.$ If we use $R = \\{ x \\leq 2\\},$ then our test will lead us to conclude that Churchill is the winner with a probability of $0.764$ even if $p$ is as low as $0.25$!\n\nIf we repeat these calculations for $R^* = \\{x \\leq 5\\},$ we find $\\alpha = 0.151$ versus $\\beta = 0.148,$ even if $p$ is as low as $0.25,$ which is a much better balance between type I and type II errors.\n:::\n\n:::{.callout-warning}\n## What if the sample size is close to the population size?\n\nIn @exm-htest-alpha, $X$ is a binomial random variable because it can be modelled as $m$ independent Bernoulli trails each with probability $p$ of success (i.e., votes for Churchill) as long as the sample size $m$ is much smaller than the population of Dundee. If we had the means to canvas nearly the whole population, what goes wrong conceptually?\n:::\n\n::: {#fig-churchill-ballot}\n![](assets/images/churchill_ballot.jpg){fig-alt=\"Ballot card listing Churchill.\"}\n\nBallot listing Churchill from the collection of the McManus, Dundee. When you take a break from studying, go and see if you can find it! For more information on visiting the McManus visit [https://www.mcmanus.co.uk/](https://www.mcmanus.co.uk/).\n:::\n\n:::{.callout-important}\n## Elements of a statistical test  \n\nA statistical test is based on a null hypothesis ($H_0$) and an alternative hypothesis ($H_a$).\n\nAn appropriate test statistic $T$ is computed. Then either:\n\n- $T$ is compared to a rejection region (based on significance level $\\alpha$) \n\nOR\n\n- $P$-value (based on $T$) is compare to the significance level $\\alpha.$ \n\n:::   \n",
    "supporting": [
      "02-basics-stat-infer_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}